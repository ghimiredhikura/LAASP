save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/40.resnet20.3.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/40.resnet20.3.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 40, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 8641, 'cuda': True, 'use_cuda': True}
Random Seed: 8641
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/40.resnet20.3.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=40, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=8641, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=43.630, Acc@5=87.310 | Loss= 1.96011

==>>[2022-08-15 14:55:15] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=43.63, Error=56.37]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=61.360, Acc@5=96.300 | Loss= 1.11161

==>>[2022-08-15 14:55:58] [Epoch=001/160] [Need: 02:02:57] [learning_rate=0.1000] [Best : Acc@1=61.36, Error=38.64]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=61.460, Acc@5=95.530 | Loss= 1.31276

==>>[2022-08-15 14:56:42] [Epoch=002/160] [Need: 01:57:55] [learning_rate=0.1000] [Best : Acc@1=61.46, Error=38.54]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=70.480, Acc@5=97.770 | Loss= 0.85294

==>>[2022-08-15 14:57:25] [Epoch=003/160] [Need: 01:55:47] [learning_rate=0.1000] [Best : Acc@1=70.48, Error=29.52]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=75.670, Acc@5=98.530 | Loss= 0.72731

==>>[2022-08-15 14:58:08] [Epoch=004/160] [Need: 01:54:21] [learning_rate=0.1000] [Best : Acc@1=75.67, Error=24.33]
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=76.160, Acc@5=98.700 | Loss= 0.69808

==>>[2022-08-15 14:58:51] [Epoch=005/160] [Need: 01:53:13] [learning_rate=0.1000] [Best : Acc@1=76.16, Error=23.84]
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=76.030, Acc@5=98.510 | Loss= 0.70061
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=76.800, Acc@5=98.760 | Loss= 0.71378

==>>[2022-08-15 15:00:17] [Epoch=007/160] [Need: 01:51:08] [learning_rate=0.1000] [Best : Acc@1=76.80, Error=23.20]
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=79.590, Acc@5=98.860 | Loss= 0.60255

==>>[2022-08-15 15:01:00] [Epoch=008/160] [Need: 01:50:17] [learning_rate=0.1000] [Best : Acc@1=79.59, Error=20.41]
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=81.010, Acc@5=99.140 | Loss= 0.56051

==>>[2022-08-15 15:01:43] [Epoch=009/160] [Need: 01:49:25] [learning_rate=0.1000] [Best : Acc@1=81.01, Error=18.99]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=75.920, Acc@5=98.720 | Loss= 0.71998
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=75.400, Acc@5=98.620 | Loss= 0.76238
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=73.130, Acc@5=98.600 | Loss= 0.94462
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=75.300, Acc@5=98.150 | Loss= 0.78093
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=76.180, Acc@5=98.590 | Loss= 0.74211
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=76.410, Acc@5=98.660 | Loss= 0.71853
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=72.140, Acc@5=97.980 | Loss= 0.88058
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=79.270, Acc@5=98.660 | Loss= 0.65113
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=79.510, Acc@5=99.010 | Loss= 0.61864
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=77.980, Acc@5=97.960 | Loss= 0.69993
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=81.450, Acc@5=99.040 | Loss= 0.54247

==>>[2022-08-15 15:09:37] [Epoch=020/160] [Need: 01:40:56] [learning_rate=0.1000] [Best : Acc@1=81.45, Error=18.55]
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=80.840, Acc@5=99.180 | Loss= 0.59434
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=75.260, Acc@5=98.950 | Loss= 0.73646
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=74.080, Acc@5=98.560 | Loss= 0.90291
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=82.200, Acc@5=99.050 | Loss= 0.53899

==>>[2022-08-15 15:12:34] [Epoch=024/160] [Need: 01:38:02] [learning_rate=0.1000] [Best : Acc@1=82.20, Error=17.80]
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=81.570, Acc@5=99.010 | Loss= 0.56325
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=77.170, Acc@5=98.960 | Loss= 0.70794
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=81.200, Acc@5=99.180 | Loss= 0.56762
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=78.850, Acc@5=98.800 | Loss= 0.64755
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=80.180, Acc@5=98.380 | Loss= 0.62488
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.640, Acc@5=99.180 | Loss= 0.63613
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=78.410, Acc@5=98.690 | Loss= 0.67100
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=77.840, Acc@5=98.870 | Loss= 0.66465
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=74.370, Acc@5=98.670 | Loss= 0.82237
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=79.250, Acc@5=99.120 | Loss= 0.63487
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=78.230, Acc@5=98.290 | Loss= 0.70809
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=81.690, Acc@5=99.230 | Loss= 0.57902
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=82.840, Acc@5=99.120 | Loss= 0.53577

==>>[2022-08-15 15:21:37] [Epoch=037/160] [Need: 01:27:56] [learning_rate=0.1000] [Best : Acc@1=82.84, Error=17.16]
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=80.330, Acc@5=98.900 | Loss= 0.61581
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=78.910, Acc@5=98.720 | Loss= 0.69407
Val Acc@1: 78.910, Acc@5: 98.720,  Loss: 0.69407
[Pruning Method: eucl] Flop Reduction Rate: 0.010136/0.300000 [Pruned 1 filters from 36]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.920, Acc@5=99.710 | Loss= 0.32232

==>>[2022-08-15 15:24:23] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.92, Error=11.08]
[Pruning Method: l1norm] Flop Reduction Rate: 0.020636/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=90.190, Acc@5=99.720 | Loss= 0.28945

==>>[2022-08-15 15:25:11] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.19, Error=9.81]
[Pruning Method: l2norm] Flop Reduction Rate: 0.027862/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.950, Acc@5=99.700 | Loss= 0.30858

==>>[2022-08-15 15:25:59] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.95, Error=10.05]
[Pruning Method: l1norm] Flop Reduction Rate: 0.035088/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=90.050, Acc@5=99.740 | Loss= 0.29696

==>>[2022-08-15 15:26:47] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.05, Error=9.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.042314/0.300000 [Pruned 1 filters from 5]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.500, Acc@5=99.650 | Loss= 0.31665

==>>[2022-08-15 15:27:34] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.50, Error=10.50]
[Pruning Method: l1norm] Flop Reduction Rate: 0.049540/0.300000 [Pruned 1 filters from 15]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.590, Acc@5=99.710 | Loss= 0.30799

==>>[2022-08-15 15:28:22] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.59, Error=10.41]
[Pruning Method: cos] Flop Reduction Rate: 0.060040/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.340, Acc@5=99.760 | Loss= 0.32312

==>>[2022-08-15 15:29:09] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.34, Error=10.66]
[Pruning Method: l1norm] Flop Reduction Rate: 0.070540/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.280, Acc@5=99.700 | Loss= 0.32061

==>>[2022-08-15 15:29:56] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.28, Error=10.72]
[Pruning Method: l1norm] Flop Reduction Rate: 0.077766/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.400, Acc@5=99.630 | Loss= 0.33763

==>>[2022-08-15 15:30:44] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.40, Error=10.60]
[Pruning Method: l1norm] Flop Reduction Rate: 0.088266/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.710 | Loss= 0.34290

==>>[2022-08-15 15:31:32] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.05, Error=10.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.095492/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.680, Acc@5=99.670 | Loss= 0.31868

==>>[2022-08-15 15:32:20] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.68, Error=10.32]
[Pruning Method: l2norm] Flop Reduction Rate: 0.105992/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.460, Acc@5=99.650 | Loss= 0.32967

==>>[2022-08-15 15:33:07] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.46, Error=10.54]
[Pruning Method: l1norm] Flop Reduction Rate: 0.113218/0.300000 [Pruned 1 filters from 15]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.230, Acc@5=99.670 | Loss= 0.37550

==>>[2022-08-15 15:33:54] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.23, Error=11.77]
[Pruning Method: l1norm] Flop Reduction Rate: 0.123718/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.510, Acc@5=99.670 | Loss= 0.36544

==>>[2022-08-15 15:34:42] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.51, Error=11.49]
[Pruning Method: l1norm] Flop Reduction Rate: 0.130944/0.300000 [Pruned 1 filters from 15]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.850, Acc@5=99.600 | Loss= 0.41786

==>>[2022-08-15 15:35:29] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.85, Error=12.15]
[Pruning Method: l1norm] Flop Reduction Rate: 0.141444/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.550, Acc@5=99.680 | Loss= 0.39367

==>>[2022-08-15 15:36:17] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.55, Error=12.45]
[Pruning Method: eucl] Flop Reduction Rate: 0.148670/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.070, Acc@5=99.640 | Loss= 0.34536

==>>[2022-08-15 15:37:04] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.07, Error=10.93]
[Pruning Method: cos] Flop Reduction Rate: 0.155896/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.630, Acc@5=99.600 | Loss= 0.34663

==>>[2022-08-15 15:37:52] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.63, Error=11.37]
[Pruning Method: l1norm] Flop Reduction Rate: 0.163122/0.300000 [Pruned 1 filters from 15]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.170, Acc@5=99.600 | Loss= 0.34460

==>>[2022-08-15 15:38:40] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.17, Error=10.83]
[Pruning Method: l1norm] Flop Reduction Rate: 0.170348/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.170, Acc@5=99.400 | Loss= 0.40536

==>>[2022-08-15 15:39:27] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.17, Error=12.83]
[Pruning Method: l1norm] Flop Reduction Rate: 0.177574/0.300000 [Pruned 1 filters from 15]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=86.570, Acc@5=99.540 | Loss= 0.44922

==>>[2022-08-15 15:40:15] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.57, Error=13.43]
[Pruning Method: l1norm] Flop Reduction Rate: 0.184800/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.960, Acc@5=99.560 | Loss= 0.35317

==>>[2022-08-15 15:41:03] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.96, Error=11.04]
[Pruning Method: eucl] Flop Reduction Rate: 0.195300/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.060, Acc@5=99.500 | Loss= 0.38796

==>>[2022-08-15 15:41:50] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.06, Error=11.94]
[Pruning Method: cos] Flop Reduction Rate: 0.205800/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.710, Acc@5=99.620 | Loss= 0.36329

==>>[2022-08-15 15:42:38] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.71, Error=11.29]
[Pruning Method: cos] Flop Reduction Rate: 0.213026/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.720, Acc@5=99.580 | Loss= 0.39135

==>>[2022-08-15 15:43:25] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.72, Error=12.28]
[Pruning Method: l1norm] Flop Reduction Rate: 0.223526/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.560, Acc@5=99.630 | Loss= 0.39299

==>>[2022-08-15 15:44:13] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.56, Error=12.44]
[Pruning Method: l1norm] Flop Reduction Rate: 0.234026/0.300000 [Pruned 3 filters from 34]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.270, Acc@5=99.600 | Loss= 0.34994

==>>[2022-08-15 15:45:00] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.27, Error=10.73]
[Pruning Method: eucl] Flop Reduction Rate: 0.241252/0.300000 [Pruned 1 filters from 10]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=88.670, Acc@5=99.630 | Loss= 0.37516

==>>[2022-08-15 15:45:48] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.67, Error=11.33]
[Pruning Method: eucl] Flop Reduction Rate: 0.251752/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.060, Acc@5=99.600 | Loss= 0.41929

==>>[2022-08-15 15:46:35] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.06, Error=12.94]
[Pruning Method: cos] Flop Reduction Rate: 0.262591/0.300000 [Pruned 6 filters from 53]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.500, Acc@5=99.520 | Loss= 0.41912

==>>[2022-08-15 15:47:23] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.50, Error=12.50]
[Pruning Method: l1norm] Flop Reduction Rate: 0.273091/0.300000 [Pruned 3 filters from 29]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=87.580, Acc@5=99.610 | Loss= 0.39398

==>>[2022-08-15 15:48:10] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.58, Error=12.42]
[Pruning Method: cos] Flop Reduction Rate: 0.282477/0.300000 [Pruned 7 filters from 40]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=84.780, Acc@5=99.260 | Loss= 0.51344

==>>[2022-08-15 15:48:57] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=84.78, Error=15.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.291070/0.300000 [Pruned 2 filters from 42]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=82.030, Acc@5=99.220 | Loss= 0.59196

==>>[2022-08-15 15:49:44] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=82.03, Error=17.97]
[Pruning Method: l1norm] Flop Reduction Rate: 0.296701/0.300000 [Pruned 1 filters from 26]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=86.810, Acc@5=99.510 | Loss= 0.42471

==>>[2022-08-15 15:50:32] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.81, Error=13.19]
[Pruning Method: l1norm] Flop Reduction Rate: 0.305292/0.300000 [Pruned 2 filters from 55]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=86.410, Acc@5=99.520 | Loss= 0.43454

==>>[2022-08-15 15:51:19] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.41, Error=13.59]
Prune Stats: {'l1norm': 40, 'l2norm': 4, 'eucl': 9, 'cos': 21}
Final Flop Reduction Rate: 0.3053
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 15, 7: 16, 10: 6, 12: 16, 15: 11, 17: 16, 21: 32, 23: 30, 26: 30, 29: 11, 31: 30, 34: 14, 36: 30, 40: 57, 42: 60, 45: 60, 48: 64, 50: 60, 53: 58, 55: 60}
Layerwise Pruning Rate: {1: 0.0, 5: 0.0625, 7: 0.0, 10: 0.625, 12: 0.0, 15: 0.3125, 17: 0.0, 21: 0.0, 23: 0.0625, 26: 0.0625, 29: 0.65625, 31: 0.0625, 34: 0.5625, 36: 0.0625, 40: 0.109375, 42: 0.0625, 45: 0.0625, 48: 0.0, 50: 0.0625, 53: 0.09375, 55: 0.0625}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 30, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(30, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(30, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(30, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(57, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(30, 60, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(60, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=60, out_features=10, bias=True)
)
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=85.240, Acc@5=99.530 | Loss= 0.46505

==>>[2022-08-15 15:52:01] [Epoch=040/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.24, Error=14.76]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=87.310, Acc@5=99.550 | Loss= 0.40486

==>>[2022-08-15 15:52:41] [Epoch=041/160] [Need: 01:21:28] [learning_rate=0.0200] [Best : Acc@1=87.31, Error=12.69]
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=87.580, Acc@5=99.500 | Loss= 0.41262

==>>[2022-08-15 15:53:23] [Epoch=042/160] [Need: 01:20:30] [learning_rate=0.0200] [Best : Acc@1=87.58, Error=12.42]
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=87.340, Acc@5=99.570 | Loss= 0.39649
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=88.530, Acc@5=99.620 | Loss= 0.37038

==>>[2022-08-15 15:54:44] [Epoch=044/160] [Need: 01:19:28] [learning_rate=0.0200] [Best : Acc@1=88.53, Error=11.47]
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=83.590, Acc@5=99.430 | Loss= 0.55167
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=86.720, Acc@5=99.500 | Loss= 0.42759
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=86.660, Acc@5=99.480 | Loss= 0.43965
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=87.620, Acc@5=99.530 | Loss= 0.39924
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=87.820, Acc@5=99.530 | Loss= 0.36956
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.580, Acc@5=99.590 | Loss= 0.40456
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=82.650, Acc@5=98.800 | Loss= 0.62779
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=86.930, Acc@5=99.530 | Loss= 0.43747
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=86.180, Acc@5=99.510 | Loss= 0.45448
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=87.050, Acc@5=99.620 | Loss= 0.41047
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=85.850, Acc@5=99.560 | Loss= 0.47680
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=87.880, Acc@5=99.400 | Loss= 0.39831
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=88.250, Acc@5=99.370 | Loss= 0.36622
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=88.000, Acc@5=99.580 | Loss= 0.37580
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=88.320, Acc@5=99.490 | Loss= 0.37398
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=86.850, Acc@5=99.450 | Loss= 0.42477
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=87.600, Acc@5=99.410 | Loss= 0.41233
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=87.230, Acc@5=99.480 | Loss= 0.41689
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=87.760, Acc@5=99.510 | Loss= 0.39418
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=87.530, Acc@5=99.590 | Loss= 0.41812
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.560 | Loss= 0.34629

==>>[2022-08-15 16:08:59] [Epoch=065/160] [Need: 01:04:29] [learning_rate=0.0200] [Best : Acc@1=89.13, Error=10.87]
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=88.180, Acc@5=99.680 | Loss= 0.40594
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=84.840, Acc@5=99.440 | Loss= 0.50462
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=86.630, Acc@5=99.550 | Loss= 0.43067
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=87.300, Acc@5=99.420 | Loss= 0.41969
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.570, Acc@5=99.480 | Loss= 0.43570
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=89.070, Acc@5=99.540 | Loss= 0.36707
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=87.650, Acc@5=99.530 | Loss= 0.41677
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=86.550, Acc@5=99.280 | Loss= 0.44216
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=86.080, Acc@5=99.530 | Loss= 0.45204
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.640 | Loss= 0.36394
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=87.230, Acc@5=99.480 | Loss= 0.41861
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=87.420, Acc@5=99.600 | Loss= 0.39240
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=87.430, Acc@5=99.450 | Loss= 0.41166
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=86.850, Acc@5=99.490 | Loss= 0.44444
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.000, Acc@5=99.720 | Loss= 0.28924

==>>[2022-08-15 16:19:12] [Epoch=080/160] [Need: 00:54:23] [learning_rate=0.0040] [Best : Acc@1=91.00, Error=9.00]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.060, Acc@5=99.790 | Loss= 0.28720

==>>[2022-08-15 16:19:53] [Epoch=081/160] [Need: 00:53:42] [learning_rate=0.0040] [Best : Acc@1=91.06, Error=8.94]
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.750 | Loss= 0.27721

==>>[2022-08-15 16:20:34] [Epoch=082/160] [Need: 00:53:01] [learning_rate=0.0040] [Best : Acc@1=91.17, Error=8.83]
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.520, Acc@5=99.760 | Loss= 0.28477

==>>[2022-08-15 16:21:14] [Epoch=083/160] [Need: 00:52:21] [learning_rate=0.0040] [Best : Acc@1=91.52, Error=8.48]
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.410, Acc@5=99.800 | Loss= 0.27890
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.330, Acc@5=99.750 | Loss= 0.27762
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.450, Acc@5=99.730 | Loss= 0.28536
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.350, Acc@5=99.750 | Loss= 0.29099
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.420, Acc@5=99.690 | Loss= 0.29163
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.650 | Loss= 0.29542
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.720 | Loss= 0.29665
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.280, Acc@5=99.740 | Loss= 0.29782
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.160, Acc@5=99.740 | Loss= 0.29295
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.660 | Loss= 0.29672
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.730 | Loss= 0.29777
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.680 | Loss= 0.29939
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=91.070, Acc@5=99.660 | Loss= 0.30105
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.440, Acc@5=99.670 | Loss= 0.29997
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.330, Acc@5=99.690 | Loss= 0.30953
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=91.270, Acc@5=99.720 | Loss= 0.30493
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.220, Acc@5=99.610 | Loss= 0.30295
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=91.120, Acc@5=99.690 | Loss= 0.31364
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=90.990, Acc@5=99.690 | Loss= 0.31471
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=91.360, Acc@5=99.710 | Loss= 0.30602
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=91.010, Acc@5=99.750 | Loss= 0.31856
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=91.270, Acc@5=99.660 | Loss= 0.31121
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=91.000, Acc@5=99.650 | Loss= 0.32830
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.690 | Loss= 0.32376
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.690 | Loss= 0.31075
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=91.140, Acc@5=99.620 | Loss= 0.31689
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=91.120, Acc@5=99.690 | Loss= 0.32456
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=91.390, Acc@5=99.650 | Loss= 0.30752
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=91.440, Acc@5=99.630 | Loss= 0.31432
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=91.240, Acc@5=99.620 | Loss= 0.32553
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=91.200, Acc@5=99.650 | Loss= 0.31386
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=91.100, Acc@5=99.710 | Loss= 0.32152
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.650 | Loss= 0.31925
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=90.940, Acc@5=99.630 | Loss= 0.33089
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=91.400, Acc@5=99.630 | Loss= 0.32451
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=91.230, Acc@5=99.640 | Loss= 0.32063
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.350, Acc@5=99.640 | Loss= 0.30791
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.650 | Loss= 0.30933

==>>[2022-08-15 16:47:05] [Epoch=121/160] [Need: 00:26:31] [learning_rate=0.0008] [Best : Acc@1=91.55, Error=8.45]
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.600 | Loss= 0.30744
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.600 | Loss= 0.31017
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.660 | Loss= 0.31093
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.460, Acc@5=99.600 | Loss= 0.30843
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.430, Acc@5=99.600 | Loss= 0.30988
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.630 | Loss= 0.31245
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.630 | Loss= 0.30862
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.440, Acc@5=99.610 | Loss= 0.31078
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.620 | Loss= 0.30927
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.710, Acc@5=99.660 | Loss= 0.30910

==>>[2022-08-15 16:53:52] [Epoch=131/160] [Need: 00:19:42] [learning_rate=0.0008] [Best : Acc@1=91.71, Error=8.29]
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.670, Acc@5=99.660 | Loss= 0.31018
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.580, Acc@5=99.630 | Loss= 0.30910
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.670, Acc@5=99.650 | Loss= 0.30788
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.640 | Loss= 0.31121
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.660 | Loss= 0.31421
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.570, Acc@5=99.630 | Loss= 0.31114
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.470, Acc@5=99.620 | Loss= 0.31208
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.650 | Loss= 0.31410
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.650 | Loss= 0.31215
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.630 | Loss= 0.31234
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.620 | Loss= 0.31620
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.500, Acc@5=99.620 | Loss= 0.31270
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.640 | Loss= 0.31283
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.600 | Loss= 0.31401
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.600 | Loss= 0.31505
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.620 | Loss= 0.31459
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.460, Acc@5=99.640 | Loss= 0.31415
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.590 | Loss= 0.31406
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.630 | Loss= 0.31385
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.640 | Loss= 0.31909
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.670 | Loss= 0.31614
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.650 | Loss= 0.31611
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.520, Acc@5=99.610 | Loss= 0.31598
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.660 | Loss= 0.31420
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.640 | Loss= 0.32102
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.690 | Loss= 0.31491
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.630 | Loss= 0.31532
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.470, Acc@5=99.630 | Loss= 0.31599
