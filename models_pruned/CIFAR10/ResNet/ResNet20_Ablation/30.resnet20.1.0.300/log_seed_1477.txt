save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/30.resnet20.1.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/30.resnet20.1.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 30, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 1477, 'cuda': True, 'use_cuda': True}
Random Seed: 1477
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/30.resnet20.1.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=30, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=1477, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=37.480, Acc@5=91.140 | Loss= 2.04053

==>>[2022-08-12 20:57:44] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=37.48, Error=62.52]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=54.670, Acc@5=95.100 | Loss= 1.46280

==>>[2022-08-12 20:58:27] [Epoch=001/160] [Need: 02:03:07] [learning_rate=0.1000] [Best : Acc@1=54.67, Error=45.33]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=67.270, Acc@5=97.690 | Loss= 0.97793

==>>[2022-08-12 20:59:11] [Epoch=002/160] [Need: 01:58:18] [learning_rate=0.1000] [Best : Acc@1=67.27, Error=32.73]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=70.070, Acc@5=97.400 | Loss= 0.93124

==>>[2022-08-12 20:59:54] [Epoch=003/160] [Need: 01:56:22] [learning_rate=0.1000] [Best : Acc@1=70.07, Error=29.93]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=68.540, Acc@5=97.170 | Loss= 0.98797
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=75.050, Acc@5=97.620 | Loss= 0.77187

==>>[2022-08-12 21:01:20] [Epoch=005/160] [Need: 01:53:27] [learning_rate=0.1000] [Best : Acc@1=75.05, Error=24.95]
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=71.930, Acc@5=98.420 | Loss= 0.85295
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=72.040, Acc@5=98.110 | Loss= 0.87177
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=79.910, Acc@5=98.910 | Loss= 0.60555

==>>[2022-08-12 21:03:30] [Epoch=008/160] [Need: 01:50:37] [learning_rate=0.1000] [Best : Acc@1=79.91, Error=20.09]
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=71.360, Acc@5=96.740 | Loss= 0.91569
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=72.160, Acc@5=97.640 | Loss= 0.94131
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=79.190, Acc@5=98.950 | Loss= 0.62186
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=76.060, Acc@5=98.440 | Loss= 0.74445
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=74.780, Acc@5=97.910 | Loss= 0.85132
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=79.960, Acc@5=99.120 | Loss= 0.59084

==>>[2022-08-12 21:07:50] [Epoch=014/160] [Need: 01:45:53] [learning_rate=0.1000] [Best : Acc@1=79.96, Error=20.04]
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=78.610, Acc@5=99.010 | Loss= 0.64634
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=75.250, Acc@5=98.270 | Loss= 0.80581
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=74.720, Acc@5=98.870 | Loss= 0.81953
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=81.050, Acc@5=98.820 | Loss= 0.58804

==>>[2022-08-12 21:10:43] [Epoch=018/160] [Need: 01:42:49] [learning_rate=0.1000] [Best : Acc@1=81.05, Error=18.95]
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=82.490, Acc@5=99.180 | Loss= 0.51942

==>>[2022-08-12 21:11:26] [Epoch=019/160] [Need: 01:42:03] [learning_rate=0.1000] [Best : Acc@1=82.49, Error=17.51]
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=76.750, Acc@5=98.790 | Loss= 0.73379
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=76.800, Acc@5=98.580 | Loss= 0.73061
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=80.490, Acc@5=99.320 | Loss= 0.60615
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=75.110, Acc@5=98.240 | Loss= 0.81628
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=76.000, Acc@5=97.680 | Loss= 0.80660
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=69.470, Acc@5=96.970 | Loss= 1.07710
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=78.550, Acc@5=98.450 | Loss= 0.66990
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=77.970, Acc@5=98.820 | Loss= 0.70064
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=81.860, Acc@5=99.360 | Loss= 0.55741
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=80.910, Acc@5=99.140 | Loss= 0.61245
Val Acc@1: 80.910, Acc@5: 99.140,  Loss: 0.61245
[Pruning Method: l1norm] Flop Reduction Rate: 0.024362/0.300000 [Pruned 1 filters from 1]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=81.730, Acc@5=99.130 | Loss= 0.54458

==>>[2022-08-12 21:20:14] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.73, Error=18.27]
[Pruning Method: l1norm] Flop Reduction Rate: 0.033496/0.300000 [Pruned 2 filters from 50]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=79.120, Acc@5=98.720 | Loss= 0.66550

==>>[2022-08-12 21:21:09] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.12, Error=20.88]
[Pruning Method: l1norm] Flop Reduction Rate: 0.042629/0.300000 [Pruned 2 filters from 50]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=70.380, Acc@5=98.470 | Loss= 1.12214

==>>[2022-08-12 21:22:04] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=70.38, Error=29.62]
[Pruning Method: eucl] Flop Reduction Rate: 0.051762/0.300000 [Pruned 2 filters from 45]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=80.030, Acc@5=98.670 | Loss= 0.61568

==>>[2022-08-12 21:22:59] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.03, Error=19.97]
[Pruning Method: cos] Flop Reduction Rate: 0.062601/0.300000 [Pruned 3 filters from 34]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=80.280, Acc@5=98.890 | Loss= 0.61458

==>>[2022-08-12 21:23:55] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.28, Error=19.72]
[Pruning Method: l1norm] Flop Reduction Rate: 0.073214/0.300000 [Pruned 4 filters from 21]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=75.520, Acc@5=98.810 | Loss= 0.82239

==>>[2022-08-12 21:24:50] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=75.52, Error=24.48]
[Pruning Method: cos] Flop Reduction Rate: 0.082770/0.300000 [Pruned 1 filters from 26]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=79.540, Acc@5=99.160 | Loss= 0.63410

==>>[2022-08-12 21:25:45] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.54, Error=20.46]
[Pruning Method: cos] Flop Reduction Rate: 0.092326/0.300000 [Pruned 1 filters from 36]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=81.470, Acc@5=98.740 | Loss= 0.58678

==>>[2022-08-12 21:26:40] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.47, Error=18.53]
[Pruning Method: l2norm] Flop Reduction Rate: 0.102149/0.300000 [Pruned 6 filters from 53]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=76.970, Acc@5=98.410 | Loss= 0.74513

==>>[2022-08-12 21:27:35] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=76.97, Error=23.03]
[Pruning Method: l1norm] Flop Reduction Rate: 0.110937/0.300000 [Pruned 2 filters from 50]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=79.730, Acc@5=98.890 | Loss= 0.60595

==>>[2022-08-12 21:28:30] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.73, Error=20.27]
[Pruning Method: l1norm] Flop Reduction Rate: 0.121099/0.300000 [Pruned 4 filters from 21]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=81.430, Acc@5=98.950 | Loss= 0.58154

==>>[2022-08-12 21:29:25] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.43, Error=18.57]
[Pruning Method: l1norm] Flop Reduction Rate: 0.130583/0.300000 [Pruned 6 filters from 53]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=77.500, Acc@5=98.780 | Loss= 0.69830

==>>[2022-08-12 21:30:20] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=77.50, Error=22.50]
[Pruning Method: cos] Flop Reduction Rate: 0.140744/0.300000 [Pruned 3 filters from 34]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.870, Acc@5=98.830 | Loss= 0.67097

==>>[2022-08-12 21:31:15] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.87, Error=21.13]
[Pruning Method: cos] Flop Reduction Rate: 0.149732/0.300000 [Pruned 1 filters from 23]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.700, Acc@5=98.910 | Loss= 0.66878

==>>[2022-08-12 21:32:10] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.70, Error=21.30]
[Pruning Method: l1norm] Flop Reduction Rate: 0.159216/0.300000 [Pruned 6 filters from 53]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=83.180, Acc@5=99.150 | Loss= 0.51416

==>>[2022-08-12 21:33:05] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=83.18, Error=16.82]
[Pruning Method: l2norm] Flop Reduction Rate: 0.169039/0.300000 [Pruned 3 filters from 29]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=72.220, Acc@5=98.070 | Loss= 0.93376

==>>[2022-08-12 21:34:00] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=72.22, Error=27.78]
[Pruning Method: eucl] Flop Reduction Rate: 0.178523/0.300000 [Pruned 6 filters from 53]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=82.160, Acc@5=98.890 | Loss= 0.53509

==>>[2022-08-12 21:34:54] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=82.16, Error=17.84]
[Pruning Method: l1norm] Flop Reduction Rate: 0.187173/0.300000 [Pruned 1 filters from 26]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=79.420, Acc@5=98.720 | Loss= 0.66080

==>>[2022-08-12 21:35:49] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.42, Error=20.58]
[Pruning Method: cos] Flop Reduction Rate: 0.196657/0.300000 [Pruned 6 filters from 53]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=64.740, Acc@5=96.190 | Loss= 1.40148

==>>[2022-08-12 21:36:44] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=64.74, Error=35.26]
[Pruning Method: cos] Flop Reduction Rate: 0.204084/0.300000 [Pruned 2 filters from 42]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=75.970, Acc@5=98.840 | Loss= 0.77796

==>>[2022-08-12 21:37:38] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=75.97, Error=24.03]
[Pruning Method: l1norm] Flop Reduction Rate: 0.211511/0.300000 [Pruned 2 filters from 45]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=75.060, Acc@5=98.590 | Loss= 0.79262

==>>[2022-08-12 21:38:33] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=75.06, Error=24.94]
[Pruning Method: l1norm] Flop Reduction Rate: 0.220318/0.300000 [Pruned 6 filters from 48]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=81.740, Acc@5=99.150 | Loss= 0.53472

==>>[2022-08-12 21:39:28] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.74, Error=18.26]
[Pruning Method: l1norm] Flop Reduction Rate: 0.228961/0.300000 [Pruned 1 filters from 26]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=80.120, Acc@5=98.720 | Loss= 0.63161

==>>[2022-08-12 21:40:22] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.12, Error=19.88]
[Pruning Method: cos] Flop Reduction Rate: 0.235735/0.300000 [Pruned 1 filters from 15]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=83.500, Acc@5=99.150 | Loss= 0.49770

==>>[2022-08-12 21:41:17] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=83.50, Error=16.50]
[Pruning Method: cos] Flop Reduction Rate: 0.244881/0.300000 [Pruned 3 filters from 34]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=77.890, Acc@5=98.140 | Loss= 0.73456

==>>[2022-08-12 21:42:12] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=77.89, Error=22.11]
[Pruning Method: cos] Flop Reduction Rate: 0.254026/0.300000 [Pruned 3 filters from 29]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=80.990, Acc@5=98.850 | Loss= 0.58169

==>>[2022-08-12 21:43:06] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.99, Error=19.01]
[Pruning Method: cos] Flop Reduction Rate: 0.260800/0.300000 [Pruned 1 filters from 5]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=77.340, Acc@5=98.770 | Loss= 0.73881

==>>[2022-08-12 21:44:00] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=77.34, Error=22.66]
[Pruning Method: eucl] Flop Reduction Rate: 0.269946/0.300000 [Pruned 3 filters from 29]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=81.760, Acc@5=99.230 | Loss= 0.54506

==>>[2022-08-12 21:44:54] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.76, Error=18.24]
[Pruning Method: eucl] Flop Reduction Rate: 0.279091/0.300000 [Pruned 3 filters from 34]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.480, Acc@5=98.300 | Loss= 0.70849

==>>[2022-08-12 21:45:48] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.48, Error=21.52]
[Pruning Method: l2norm] Flop Reduction Rate: 0.288575/0.300000 [Pruned 4 filters from 21]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=74.290, Acc@5=97.020 | Loss= 0.90985

==>>[2022-08-12 21:46:42] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=74.29, Error=25.71]
[Pruning Method: cos] Flop Reduction Rate: 0.311325/0.300000 [Pruned 1 filters from 7]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.990, Acc@5=98.920 | Loss= 0.69282

==>>[2022-08-12 21:47:37] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.99, Error=21.01]
Prune Stats: {'l1norm': 37, 'l2norm': 13, 'eucl': 14, 'cos': 26}
Final Flop Reduction Rate: 0.3113
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 14, 5: 15, 7: 14, 10: 16, 12: 14, 15: 15, 17: 14, 21: 20, 23: 27, 26: 27, 29: 23, 31: 27, 34: 20, 36: 27, 40: 64, 42: 52, 45: 52, 48: 58, 50: 52, 53: 34, 55: 52}
Layerwise Pruning Rate: {1: 0.125, 5: 0.0625, 7: 0.125, 10: 0.0, 12: 0.125, 15: 0.0625, 17: 0.125, 21: 0.375, 23: 0.15625, 26: 0.15625, 29: 0.28125, 31: 0.15625, 34: 0.375, 36: 0.15625, 40: 0.0, 42: 0.1875, 45: 0.1875, 48: 0.09375, 50: 0.1875, 53: 0.46875, 55: 0.1875}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(14, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(20, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(14, 27, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(27, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(23, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(27, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(20, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(27, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(27, 52, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(52, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(52, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(34, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=52, out_features=10, bias=True)
)
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=80.290, Acc@5=98.880 | Loss= 0.58795

==>>[2022-08-12 21:48:20] [Epoch=030/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.29, Error=19.71]
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=81.810, Acc@5=98.980 | Loss= 0.55132

==>>[2022-08-12 21:49:03] [Epoch=031/160] [Need: 01:31:42] [learning_rate=0.1000] [Best : Acc@1=81.81, Error=18.19]
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=79.440, Acc@5=98.670 | Loss= 0.62573
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=77.750, Acc@5=98.210 | Loss= 0.73473
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=79.400, Acc@5=98.900 | Loss= 0.66593
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=82.260, Acc@5=99.130 | Loss= 0.53437

==>>[2022-08-12 21:51:54] [Epoch=035/160] [Need: 01:29:10] [learning_rate=0.1000] [Best : Acc@1=82.26, Error=17.74]
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=80.920, Acc@5=99.130 | Loss= 0.57612
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=82.830, Acc@5=98.920 | Loss= 0.54256

==>>[2022-08-12 21:53:20] [Epoch=037/160] [Need: 01:27:51] [learning_rate=0.1000] [Best : Acc@1=82.83, Error=17.17]
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=75.580, Acc@5=98.650 | Loss= 0.81674
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=68.270, Acc@5=95.790 | Loss= 1.16902
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.910, Acc@5=99.670 | Loss= 0.30412

==>>[2022-08-12 21:55:29] [Epoch=040/160] [Need: 01:25:42] [learning_rate=0.0200] [Best : Acc@1=89.91, Error=10.09]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=89.670, Acc@5=99.700 | Loss= 0.30810
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=89.630, Acc@5=99.670 | Loss= 0.31928
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=89.440, Acc@5=99.720 | Loss= 0.31687
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=89.890, Acc@5=99.650 | Loss= 0.30465
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=89.910, Acc@5=99.620 | Loss= 0.31737
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=90.070, Acc@5=99.640 | Loss= 0.30225

==>>[2022-08-12 21:59:46] [Epoch=046/160] [Need: 01:21:29] [learning_rate=0.0200] [Best : Acc@1=90.07, Error=9.93]
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=89.970, Acc@5=99.690 | Loss= 0.31489
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.610 | Loss= 0.34263
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=90.020, Acc@5=99.630 | Loss= 0.31187
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.570 | Loss= 0.36536
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.210, Acc@5=99.620 | Loss= 0.34031
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=89.640, Acc@5=99.660 | Loss= 0.32650
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=88.420, Acc@5=99.620 | Loss= 0.36633
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.630 | Loss= 0.33953
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=89.370, Acc@5=99.600 | Loss= 0.34447
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=86.350, Acc@5=99.520 | Loss= 0.47478
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=89.840, Acc@5=99.670 | Loss= 0.32756
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=88.600, Acc@5=99.550 | Loss= 0.36325
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=89.480, Acc@5=99.690 | Loss= 0.34530
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=87.600, Acc@5=99.490 | Loss= 0.40283
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=88.150, Acc@5=99.570 | Loss= 0.39122
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=87.370, Acc@5=99.430 | Loss= 0.42312
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=88.350, Acc@5=99.690 | Loss= 0.36901
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=89.270, Acc@5=99.540 | Loss= 0.33401
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=87.300, Acc@5=99.430 | Loss= 0.42314
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=89.160, Acc@5=99.670 | Loss= 0.34818
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=89.300, Acc@5=99.570 | Loss= 0.34236
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=87.880, Acc@5=99.540 | Loss= 0.39573
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=86.900, Acc@5=99.430 | Loss= 0.43681
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.290, Acc@5=99.610 | Loss= 0.37135
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=87.060, Acc@5=99.360 | Loss= 0.42393
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=88.840, Acc@5=99.670 | Loss= 0.35234
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=87.640, Acc@5=99.600 | Loss= 0.40556
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=87.380, Acc@5=99.560 | Loss= 0.41219
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=89.240, Acc@5=99.610 | Loss= 0.34121
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=87.570, Acc@5=99.520 | Loss= 0.40684
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=87.540, Acc@5=99.500 | Loss= 0.42353
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=87.370, Acc@5=99.470 | Loss= 0.40387
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=87.240, Acc@5=99.380 | Loss= 0.42678
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.370, Acc@5=99.650 | Loss= 0.27830

==>>[2022-08-12 22:24:02] [Epoch=080/160] [Need: 00:57:07] [learning_rate=0.0040] [Best : Acc@1=91.37, Error=8.63]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.290, Acc@5=99.680 | Loss= 0.27899
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.300, Acc@5=99.670 | Loss= 0.28743
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.330, Acc@5=99.650 | Loss= 0.27702
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.620, Acc@5=99.660 | Loss= 0.27877

==>>[2022-08-12 22:26:53] [Epoch=084/160] [Need: 00:54:15] [learning_rate=0.0040] [Best : Acc@1=91.62, Error=8.38]
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.460, Acc@5=99.670 | Loss= 0.28486
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.240, Acc@5=99.650 | Loss= 0.28655
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.410, Acc@5=99.700 | Loss= 0.29212
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.680 | Loss= 0.29219
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.250, Acc@5=99.670 | Loss= 0.29432
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.660 | Loss= 0.30267
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.400, Acc@5=99.670 | Loss= 0.30044
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.670 | Loss= 0.29160
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.470, Acc@5=99.660 | Loss= 0.28973
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.520, Acc@5=99.680 | Loss= 0.29921
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.700 | Loss= 0.31017
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=91.190, Acc@5=99.680 | Loss= 0.30852
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.670 | Loss= 0.29504
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.280, Acc@5=99.640 | Loss= 0.30968
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.700 | Loss= 0.31293
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.680 | Loss= 0.31589
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=91.460, Acc@5=99.680 | Loss= 0.30597
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=91.370, Acc@5=99.690 | Loss= 0.31318
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=91.240, Acc@5=99.720 | Loss= 0.31387
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.700 | Loss= 0.31156
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.700 | Loss= 0.31666
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.640 | Loss= 0.32321
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.220, Acc@5=99.680 | Loss= 0.31632
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=90.900, Acc@5=99.700 | Loss= 0.33204
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=91.400, Acc@5=99.670 | Loss= 0.31954
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=90.860, Acc@5=99.710 | Loss= 0.31834
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.680 | Loss= 0.32150
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=91.200, Acc@5=99.700 | Loss= 0.31955
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=90.870, Acc@5=99.680 | Loss= 0.33978
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=91.360, Acc@5=99.690 | Loss= 0.32138
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=91.250, Acc@5=99.700 | Loss= 0.32313
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=90.930, Acc@5=99.680 | Loss= 0.32922
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.750 | Loss= 0.33943
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=91.010, Acc@5=99.620 | Loss= 0.33070
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=91.010, Acc@5=99.700 | Loss= 0.32591
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.610, Acc@5=99.720 | Loss= 0.31189
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.600, Acc@5=99.730 | Loss= 0.30878
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.560, Acc@5=99.700 | Loss= 0.31185
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.650, Acc@5=99.690 | Loss= 0.30944

==>>[2022-08-12 22:54:43] [Epoch=123/160] [Need: 00:26:24] [learning_rate=0.0008] [Best : Acc@1=91.65, Error=8.35]
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.590, Acc@5=99.710 | Loss= 0.31309
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.690 | Loss= 0.31610
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.590, Acc@5=99.690 | Loss= 0.31399
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.540, Acc@5=99.650 | Loss= 0.31366
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.590, Acc@5=99.690 | Loss= 0.31490
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.630, Acc@5=99.700 | Loss= 0.31431
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.600, Acc@5=99.710 | Loss= 0.31578
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.570, Acc@5=99.720 | Loss= 0.31578
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.540, Acc@5=99.720 | Loss= 0.31998
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.610, Acc@5=99.680 | Loss= 0.31789
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.660 | Loss= 0.31784
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.740, Acc@5=99.700 | Loss= 0.31643

==>>[2022-08-12 23:03:19] [Epoch=135/160] [Need: 00:17:51] [learning_rate=0.0008] [Best : Acc@1=91.74, Error=8.26]
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.690 | Loss= 0.32131
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.750, Acc@5=99.690 | Loss= 0.31669

==>>[2022-08-12 23:04:45] [Epoch=137/160] [Need: 00:16:25] [learning_rate=0.0008] [Best : Acc@1=91.75, Error=8.25]
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.700 | Loss= 0.31889
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.660 | Loss= 0.31584
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.560, Acc@5=99.730 | Loss= 0.31989
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.720 | Loss= 0.32178
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.720 | Loss= 0.32029
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.570, Acc@5=99.730 | Loss= 0.31947
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.650, Acc@5=99.710 | Loss= 0.31884
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.540, Acc@5=99.710 | Loss= 0.32261
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.640, Acc@5=99.690 | Loss= 0.32082
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.440, Acc@5=99.660 | Loss= 0.32364
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.570, Acc@5=99.680 | Loss= 0.32349
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.610, Acc@5=99.690 | Loss= 0.32236
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.600, Acc@5=99.710 | Loss= 0.32206
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.620, Acc@5=99.680 | Loss= 0.31893
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.580, Acc@5=99.720 | Loss= 0.32505
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.670 | Loss= 0.32331
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.640, Acc@5=99.670 | Loss= 0.32532
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.690, Acc@5=99.680 | Loss= 0.32577
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.560, Acc@5=99.690 | Loss= 0.32394
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.740 | Loss= 0.32529
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.680, Acc@5=99.680 | Loss= 0.32225
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.440, Acc@5=99.700 | Loss= 0.32572
