save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/130.resnet20.3.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/130.resnet20.3.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 130, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 1886, 'cuda': True, 'use_cuda': True}
Random Seed: 1886
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/130.resnet20.3.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=130, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=1886, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=49.840, Acc@5=94.450 | Loss= 1.46481

==>>[2022-08-16 20:35:41] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=49.84, Error=50.16]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=61.820, Acc@5=95.920 | Loss= 1.10930

==>>[2022-08-16 20:36:25] [Epoch=001/160] [Need: 02:05:18] [learning_rate=0.1000] [Best : Acc@1=61.82, Error=38.18]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=68.320, Acc@5=97.920 | Loss= 0.91785

==>>[2022-08-16 20:37:08] [Epoch=002/160] [Need: 01:59:54] [learning_rate=0.1000] [Best : Acc@1=68.32, Error=31.68]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=71.050, Acc@5=98.230 | Loss= 0.80323

==>>[2022-08-16 20:37:52] [Epoch=003/160] [Need: 01:57:36] [learning_rate=0.1000] [Best : Acc@1=71.05, Error=28.95]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=66.190, Acc@5=96.600 | Loss= 1.07033
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=70.920, Acc@5=98.490 | Loss= 0.89439
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=78.160, Acc@5=98.700 | Loss= 0.64564

==>>[2022-08-16 20:40:04] [Epoch=006/160] [Need: 01:53:57] [learning_rate=0.1000] [Best : Acc@1=78.16, Error=21.84]
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=77.060, Acc@5=98.530 | Loss= 0.66846
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=77.520, Acc@5=98.600 | Loss= 0.67353
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=74.830, Acc@5=98.110 | Loss= 0.77024
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=75.620, Acc@5=97.980 | Loss= 0.76501
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=71.120, Acc@5=97.100 | Loss= 1.00147
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=78.970, Acc@5=99.200 | Loss= 0.59811

==>>[2022-08-16 20:44:26] [Epoch=012/160] [Need: 01:48:48] [learning_rate=0.1000] [Best : Acc@1=78.97, Error=21.03]
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=67.460, Acc@5=97.350 | Loss= 1.13742
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=75.730, Acc@5=98.300 | Loss= 0.80100
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=72.710, Acc@5=98.030 | Loss= 0.85689
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=82.340, Acc@5=99.270 | Loss= 0.51720

==>>[2022-08-16 20:47:22] [Epoch=016/160] [Need: 01:45:41] [learning_rate=0.1000] [Best : Acc@1=82.34, Error=17.66]
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=75.460, Acc@5=97.950 | Loss= 0.79704
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=80.640, Acc@5=98.930 | Loss= 0.56696
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=76.580, Acc@5=98.780 | Loss= 0.75476
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=73.380, Acc@5=97.340 | Loss= 0.88588
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=73.260, Acc@5=98.020 | Loss= 0.90333
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=74.380, Acc@5=97.440 | Loss= 0.82320
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=72.250, Acc@5=97.170 | Loss= 0.92189
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=81.250, Acc@5=98.780 | Loss= 0.58638
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=80.690, Acc@5=98.790 | Loss= 0.60350
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=73.170, Acc@5=98.120 | Loss= 0.85500
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=75.610, Acc@5=98.860 | Loss= 0.81588
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=79.310, Acc@5=98.540 | Loss= 0.63609
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=77.410, Acc@5=98.860 | Loss= 0.73170
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=75.570, Acc@5=98.770 | Loss= 0.77816
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=72.850, Acc@5=98.340 | Loss= 0.86230
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=75.220, Acc@5=98.690 | Loss= 0.78583
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=76.010, Acc@5=97.970 | Loss= 0.74556
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=78.580, Acc@5=98.420 | Loss= 0.68823
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=77.790, Acc@5=99.000 | Loss= 0.72584
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=76.110, Acc@5=98.950 | Loss= 0.76050
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=84.250, Acc@5=98.970 | Loss= 0.48497

==>>[2022-08-16 21:02:43] [Epoch=037/160] [Need: 01:30:04] [learning_rate=0.1000] [Best : Acc@1=84.25, Error=15.75]
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=81.160, Acc@5=99.000 | Loss= 0.57522
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=78.680, Acc@5=98.900 | Loss= 0.68229
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.860, Acc@5=99.750 | Loss= 0.29643

==>>[2022-08-16 21:04:55] [Epoch=040/160] [Need: 01:27:52] [learning_rate=0.0200] [Best : Acc@1=89.86, Error=10.14]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=90.140, Acc@5=99.700 | Loss= 0.30127

==>>[2022-08-16 21:05:39] [Epoch=041/160] [Need: 01:27:08] [learning_rate=0.0200] [Best : Acc@1=90.14, Error=9.86]
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=90.050, Acc@5=99.640 | Loss= 0.30642
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=90.150, Acc@5=99.700 | Loss= 0.29684

==>>[2022-08-16 21:07:07] [Epoch=043/160] [Need: 01:25:41] [learning_rate=0.0200] [Best : Acc@1=90.15, Error=9.85]
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=90.210, Acc@5=99.730 | Loss= 0.29795

==>>[2022-08-16 21:07:51] [Epoch=044/160] [Need: 01:24:57] [learning_rate=0.0200] [Best : Acc@1=90.21, Error=9.79]
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=90.230, Acc@5=99.730 | Loss= 0.29665

==>>[2022-08-16 21:08:39] [Epoch=045/160] [Need: 01:24:13] [learning_rate=0.0200] [Best : Acc@1=90.23, Error=9.77]
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=90.040, Acc@5=99.690 | Loss= 0.30833
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=89.590, Acc@5=99.750 | Loss= 0.32325
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.790, Acc@5=99.670 | Loss= 0.32648
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=89.900, Acc@5=99.710 | Loss= 0.32207
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.010, Acc@5=99.660 | Loss= 0.36569
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.690, Acc@5=99.720 | Loss= 0.33158
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=89.880, Acc@5=99.700 | Loss= 0.33888
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=88.980, Acc@5=99.620 | Loss= 0.36939
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=88.430, Acc@5=99.560 | Loss= 0.37340
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=88.920, Acc@5=99.690 | Loss= 0.35288
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=89.640, Acc@5=99.590 | Loss= 0.33591
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.530 | Loss= 0.38440
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=89.270, Acc@5=99.660 | Loss= 0.33696
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=89.440, Acc@5=99.750 | Loss= 0.34721
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=89.660, Acc@5=99.630 | Loss= 0.33101
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=88.120, Acc@5=99.460 | Loss= 0.39215
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=88.010, Acc@5=99.630 | Loss= 0.39421
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=88.290, Acc@5=99.570 | Loss= 0.39565
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=89.350, Acc@5=99.660 | Loss= 0.33101
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.620 | Loss= 0.36973
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=88.340, Acc@5=99.610 | Loss= 0.39459
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=88.270, Acc@5=99.580 | Loss= 0.38681
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.650 | Loss= 0.35370
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=88.790, Acc@5=99.670 | Loss= 0.35967
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.100, Acc@5=99.520 | Loss= 0.39535
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=86.150, Acc@5=99.160 | Loss= 0.48539
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=88.560, Acc@5=99.540 | Loss= 0.37682
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=89.040, Acc@5=99.540 | Loss= 0.35909
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=88.130, Acc@5=99.530 | Loss= 0.40362
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=88.880, Acc@5=99.610 | Loss= 0.36712
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=86.180, Acc@5=99.570 | Loss= 0.45275
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=86.950, Acc@5=99.480 | Loss= 0.43396
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=87.230, Acc@5=99.460 | Loss= 0.43209
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=88.300, Acc@5=99.550 | Loss= 0.37970
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.320, Acc@5=99.680 | Loss= 0.27406

==>>[2022-08-16 21:34:09] [Epoch=080/160] [Need: 00:58:32] [learning_rate=0.0040] [Best : Acc@1=91.32, Error=8.68]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.590, Acc@5=99.700 | Loss= 0.27810

==>>[2022-08-16 21:34:53] [Epoch=081/160] [Need: 00:57:47] [learning_rate=0.0040] [Best : Acc@1=91.59, Error=8.41]
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.490, Acc@5=99.740 | Loss= 0.28134
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.540, Acc@5=99.730 | Loss= 0.27419
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.470, Acc@5=99.750 | Loss= 0.28161
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.740, Acc@5=99.740 | Loss= 0.27913

==>>[2022-08-16 21:37:49] [Epoch=085/160] [Need: 00:54:52] [learning_rate=0.0040] [Best : Acc@1=91.74, Error=8.26]
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.520, Acc@5=99.690 | Loss= 0.28521
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.410, Acc@5=99.720 | Loss= 0.28787
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.730, Acc@5=99.650 | Loss= 0.28896
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.760, Acc@5=99.670 | Loss= 0.29284

==>>[2022-08-16 21:40:44] [Epoch=089/160] [Need: 00:51:56] [learning_rate=0.0040] [Best : Acc@1=91.76, Error=8.24]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.680 | Loss= 0.30047
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.630, Acc@5=99.700 | Loss= 0.29279
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.530, Acc@5=99.620 | Loss= 0.29347
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.610 | Loss= 0.29751
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.440, Acc@5=99.710 | Loss= 0.30540
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.650 | Loss= 0.30516
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=91.570, Acc@5=99.710 | Loss= 0.29779
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.400, Acc@5=99.630 | Loss= 0.30280
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.410, Acc@5=99.640 | Loss= 0.30810
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=91.370, Acc@5=99.610 | Loss= 0.30828
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.330, Acc@5=99.660 | Loss= 0.31210
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=91.280, Acc@5=99.690 | Loss= 0.31716
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=91.320, Acc@5=99.690 | Loss= 0.30713
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=91.570, Acc@5=99.670 | Loss= 0.30989
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.580 | Loss= 0.33145
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=91.650, Acc@5=99.650 | Loss= 0.30679
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=90.970, Acc@5=99.680 | Loss= 0.32380
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.790, Acc@5=99.660 | Loss= 0.31396

==>>[2022-08-16 21:53:55] [Epoch=107/160] [Need: 00:38:46] [learning_rate=0.0040] [Best : Acc@1=91.79, Error=8.21]
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=91.040, Acc@5=99.660 | Loss= 0.32496
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=91.490, Acc@5=99.670 | Loss= 0.31643
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=91.380, Acc@5=99.690 | Loss= 0.32118
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=91.600, Acc@5=99.680 | Loss= 0.32460
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=91.580, Acc@5=99.610 | Loss= 0.31962
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.620 | Loss= 0.33000
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.630 | Loss= 0.33983
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=91.420, Acc@5=99.640 | Loss= 0.33252
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=91.030, Acc@5=99.660 | Loss= 0.33529
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=91.000, Acc@5=99.650 | Loss= 0.33717
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=91.510, Acc@5=99.660 | Loss= 0.32061
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=91.330, Acc@5=99.680 | Loss= 0.32744
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.780, Acc@5=99.700 | Loss= 0.30870
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.750, Acc@5=99.720 | Loss= 0.30974
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.730, Acc@5=99.690 | Loss= 0.30748
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.670, Acc@5=99.720 | Loss= 0.30637
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.720, Acc@5=99.720 | Loss= 0.30916
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.730, Acc@5=99.730 | Loss= 0.31097
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.820, Acc@5=99.690 | Loss= 0.30871

==>>[2022-08-16 22:07:50] [Epoch=126/160] [Need: 00:24:52] [learning_rate=0.0008] [Best : Acc@1=91.82, Error=8.18]
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.560, Acc@5=99.700 | Loss= 0.31271
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.690, Acc@5=99.710 | Loss= 0.30965
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.730, Acc@5=99.710 | Loss= 0.31474
Val Acc@1: 91.730, Acc@5: 99.710,  Loss: 0.31474
[Pruning Method: l1norm] Flop Reduction Rate: 0.007226/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.590, Acc@5=99.680 | Loss= 0.31252

==>>[2022-08-16 22:11:37] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.59, Error=8.41]
[Pruning Method: l2norm] Flop Reduction Rate: 0.014452/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.630, Acc@5=99.670 | Loss= 0.30999

==>>[2022-08-16 22:12:35] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.63, Error=8.37]
[Pruning Method: l2norm] Flop Reduction Rate: 0.021678/0.300000 [Pruned 1 filters from 15]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.680, Acc@5=99.670 | Loss= 0.30989

==>>[2022-08-16 22:13:32] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.68, Error=8.32]
[Pruning Method: l1norm] Flop Reduction Rate: 0.028904/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.660, Acc@5=99.650 | Loss= 0.31385

==>>[2022-08-16 22:14:29] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.66, Error=8.34]
[Pruning Method: l1norm] Flop Reduction Rate: 0.036130/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.660 | Loss= 0.31513

==>>[2022-08-16 22:15:26] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.49, Error=8.51]
[Pruning Method: l1norm] Flop Reduction Rate: 0.043355/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.520, Acc@5=99.670 | Loss= 0.32215

==>>[2022-08-16 22:16:23] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.52, Error=8.48]
[Pruning Method: l1norm] Flop Reduction Rate: 0.050581/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.690 | Loss= 0.31712

==>>[2022-08-16 22:17:20] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.25, Error=8.75]
[Pruning Method: l1norm] Flop Reduction Rate: 0.057807/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.650 | Loss= 0.31851

==>>[2022-08-16 22:18:17] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.42, Error=8.58]
[Pruning Method: l2norm] Flop Reduction Rate: 0.065033/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.660 | Loss= 0.32131

==>>[2022-08-16 22:19:13] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.38, Error=8.62]
[Pruning Method: l2norm] Flop Reduction Rate: 0.072259/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.660 | Loss= 0.32138

==>>[2022-08-16 22:20:10] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.31, Error=8.69]
[Pruning Method: eucl] Flop Reduction Rate: 0.079485/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.410, Acc@5=99.710 | Loss= 0.32281

==>>[2022-08-16 22:21:07] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.41, Error=8.59]
[Pruning Method: eucl] Flop Reduction Rate: 0.090324/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.680 | Loss= 0.32449

==>>[2022-08-16 22:22:03] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.24, Error=8.76]
[Pruning Method: eucl] Flop Reduction Rate: 0.097550/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.670 | Loss= 0.32644

==>>[2022-08-16 22:22:59] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.53, Error=8.47]
[Pruning Method: l1norm] Flop Reduction Rate: 0.108389/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.710 | Loss= 0.32354

==>>[2022-08-16 22:23:55] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.39, Error=8.61]
[Pruning Method: eucl] Flop Reduction Rate: 0.115614/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.260, Acc@5=99.700 | Loss= 0.32510

==>>[2022-08-16 22:24:51] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.26, Error=8.74]
[Pruning Method: l1norm] Flop Reduction Rate: 0.126453/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.650 | Loss= 0.32569

==>>[2022-08-16 22:25:47] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.16, Error=8.84]
[Pruning Method: cos] Flop Reduction Rate: 0.133679/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.670 | Loss= 0.32468

==>>[2022-08-16 22:26:43] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.27, Error=8.73]
[Pruning Method: l1norm] Flop Reduction Rate: 0.144518/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.690 | Loss= 0.32688

==>>[2022-08-16 22:27:38] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.16, Error=8.84]
[Pruning Method: cos] Flop Reduction Rate: 0.151744/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.180, Acc@5=99.630 | Loss= 0.33209

==>>[2022-08-16 22:28:33] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.18, Error=8.82]
[Pruning Method: l1norm] Flop Reduction Rate: 0.162583/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.010, Acc@5=99.620 | Loss= 0.33769

==>>[2022-08-16 22:29:29] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.01, Error=8.99]
[Pruning Method: l1norm] Flop Reduction Rate: 0.173422/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.000, Acc@5=99.690 | Loss= 0.33910

==>>[2022-08-16 22:30:24] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.00, Error=9.00]
[Pruning Method: eucl] Flop Reduction Rate: 0.184260/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.740, Acc@5=99.650 | Loss= 0.34017

==>>[2022-08-16 22:31:19] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.74, Error=9.26]
[Pruning Method: eucl] Flop Reduction Rate: 0.195099/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.860, Acc@5=99.640 | Loss= 0.34029

==>>[2022-08-16 22:32:15] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.86, Error=9.14]
[Pruning Method: eucl] Flop Reduction Rate: 0.202325/0.300000 [Pruned 1 filters from 5]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.780, Acc@5=99.670 | Loss= 0.34410

==>>[2022-08-16 22:33:10] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.78, Error=9.22]
[Pruning Method: l2norm] Flop Reduction Rate: 0.209551/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.670, Acc@5=99.650 | Loss= 0.34281

==>>[2022-08-16 22:34:05] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.67, Error=9.33]
[Pruning Method: l1norm] Flop Reduction Rate: 0.216777/0.300000 [Pruned 1 filters from 10]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.880, Acc@5=99.650 | Loss= 0.33972

==>>[2022-08-16 22:35:01] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.88, Error=9.12]
[Pruning Method: l1norm] Flop Reduction Rate: 0.224003/0.300000 [Pruned 1 filters from 15]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.890, Acc@5=99.650 | Loss= 0.34291

==>>[2022-08-16 22:35:55] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.89, Error=9.11]
[Pruning Method: l1norm] Flop Reduction Rate: 0.231229/0.300000 [Pruned 1 filters from 15]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.660 | Loss= 0.33735

==>>[2022-08-16 22:36:50] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=91.25, Error=8.75]
[Pruning Method: eucl] Flop Reduction Rate: 0.242068/0.300000 [Pruned 3 filters from 29]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.890, Acc@5=99.620 | Loss= 0.34349

==>>[2022-08-16 22:37:45] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.89, Error=9.11]
[Pruning Method: l1norm] Flop Reduction Rate: 0.252907/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.750, Acc@5=99.630 | Loss= 0.35004

==>>[2022-08-16 22:38:39] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.75, Error=9.25]
[Pruning Method: cos] Flop Reduction Rate: 0.263745/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.500, Acc@5=99.540 | Loss= 0.34799

==>>[2022-08-16 22:39:34] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.50, Error=9.50]
[Pruning Method: eucl] Flop Reduction Rate: 0.274584/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.630, Acc@5=99.610 | Loss= 0.35423

==>>[2022-08-16 22:40:28] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.63, Error=9.37]
[Pruning Method: l1norm] Flop Reduction Rate: 0.283717/0.300000 [Pruned 2 filters from 55]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.370, Acc@5=99.670 | Loss= 0.34811

==>>[2022-08-16 22:41:23] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.37, Error=9.63]
[Pruning Method: l1norm] Flop Reduction Rate: 0.294556/0.300000 [Pruned 3 filters from 34]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.420, Acc@5=99.610 | Loss= 0.35403

==>>[2022-08-16 22:42:17] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.42, Error=9.58]
[Pruning Method: l1norm] Flop Reduction Rate: 0.305056/0.300000 [Pruned 6 filters from 48]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.280, Acc@5=99.550 | Loss= 0.35862

==>>[2022-08-16 22:43:11] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.28, Error=9.72]
Prune Stats: {'l1norm': 38, 'l2norm': 5, 'eucl': 19, 'cos': 5}
Final Flop Reduction Rate: 0.3051
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 9, 7: 16, 10: 6, 12: 16, 15: 13, 17: 16, 21: 32, 23: 32, 26: 32, 29: 14, 31: 32, 34: 11, 36: 32, 40: 64, 42: 62, 45: 62, 48: 58, 50: 62, 53: 64, 55: 62}
Layerwise Pruning Rate: {1: 0.0, 5: 0.4375, 7: 0.0, 10: 0.625, 12: 0.0, 15: 0.1875, 17: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.5625, 31: 0.0, 34: 0.65625, 36: 0.0, 40: 0.0, 42: 0.03125, 45: 0.03125, 48: 0.09375, 50: 0.03125, 53: 0.0, 55: 0.03125}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=62, out_features=10, bias=True)
)
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=90.310, Acc@5=99.580 | Loss= 0.35227

==>>[2022-08-16 22:43:54] [Epoch=130/160] [Need: 00:00:00] [learning_rate=0.0008] [Best : Acc@1=90.31, Error=9.69]
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=90.430, Acc@5=99.590 | Loss= 0.35029

==>>[2022-08-16 22:44:38] [Epoch=131/160] [Need: 00:20:48] [learning_rate=0.0008] [Best : Acc@1=90.43, Error=9.57]
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=90.350, Acc@5=99.580 | Loss= 0.34957
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=90.180, Acc@5=99.570 | Loss= 0.34656
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=90.410, Acc@5=99.600 | Loss= 0.34550
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=90.380, Acc@5=99.530 | Loss= 0.34856
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=90.150, Acc@5=99.560 | Loss= 0.35148
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=90.470, Acc@5=99.650 | Loss= 0.34847

==>>[2022-08-16 22:48:58] [Epoch=137/160] [Need: 00:16:38] [learning_rate=0.0008] [Best : Acc@1=90.47, Error=9.53]
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=90.380, Acc@5=99.590 | Loss= 0.34877
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=90.400, Acc@5=99.600 | Loss= 0.34701
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=90.430, Acc@5=99.580 | Loss= 0.34437
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=90.530, Acc@5=99.580 | Loss= 0.34404

==>>[2022-08-16 22:51:52] [Epoch=141/160] [Need: 00:13:45] [learning_rate=0.0008] [Best : Acc@1=90.53, Error=9.47]
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=90.710, Acc@5=99.600 | Loss= 0.33759

==>>[2022-08-16 22:52:36] [Epoch=142/160] [Need: 00:13:02] [learning_rate=0.0008] [Best : Acc@1=90.71, Error=9.29]
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=90.430, Acc@5=99.600 | Loss= 0.34470
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=90.510, Acc@5=99.650 | Loss= 0.34319
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=90.770, Acc@5=99.640 | Loss= 0.34030

==>>[2022-08-16 22:54:46] [Epoch=145/160] [Need: 00:10:51] [learning_rate=0.0008] [Best : Acc@1=90.77, Error=9.23]
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=90.540, Acc@5=99.610 | Loss= 0.34074
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=90.490, Acc@5=99.620 | Loss= 0.34046
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=90.670, Acc@5=99.640 | Loss= 0.34366
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=90.670, Acc@5=99.610 | Loss= 0.34192
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=90.610, Acc@5=99.640 | Loss= 0.34246
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=90.470, Acc@5=99.650 | Loss= 0.34575
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=90.590, Acc@5=99.620 | Loss= 0.34609
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=90.790, Acc@5=99.620 | Loss= 0.34208

==>>[2022-08-16 23:00:34] [Epoch=153/160] [Need: 00:05:04] [learning_rate=0.0008] [Best : Acc@1=90.79, Error=9.21]
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=90.530, Acc@5=99.610 | Loss= 0.34543
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=90.690, Acc@5=99.590 | Loss= 0.34738
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=90.450, Acc@5=99.620 | Loss= 0.35415
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=90.490, Acc@5=99.590 | Loss= 0.34790
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=90.610, Acc@5=99.640 | Loss= 0.34434
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=90.710, Acc@5=99.610 | Loss= 0.34148
