save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/10.resnet20.2.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/10.resnet20.2.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 10, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 8959, 'cuda': True, 'use_cuda': True}
Random Seed: 8959
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/10.resnet20.2.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=10, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=8959, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=50.550, Acc@5=91.930 | Loss= 1.47389

==>>[2022-08-13 23:37:59] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=50.55, Error=49.45]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=53.570, Acc@5=95.080 | Loss= 1.51225

==>>[2022-08-13 23:38:42] [Epoch=001/160] [Need: 02:02:33] [learning_rate=0.1000] [Best : Acc@1=53.57, Error=46.43]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=66.310, Acc@5=96.410 | Loss= 1.00295

==>>[2022-08-13 23:39:26] [Epoch=002/160] [Need: 01:58:16] [learning_rate=0.1000] [Best : Acc@1=66.31, Error=33.69]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=71.360, Acc@5=98.290 | Loss= 0.83628

==>>[2022-08-13 23:40:09] [Epoch=003/160] [Need: 01:56:22] [learning_rate=0.1000] [Best : Acc@1=71.36, Error=28.64]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=71.510, Acc@5=97.790 | Loss= 0.83232

==>>[2022-08-13 23:40:53] [Epoch=004/160] [Need: 01:55:10] [learning_rate=0.1000] [Best : Acc@1=71.51, Error=28.49]
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=70.040, Acc@5=97.420 | Loss= 0.99604
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=72.730, Acc@5=97.080 | Loss= 0.84572

==>>[2022-08-13 23:42:19] [Epoch=006/160] [Need: 01:53:00] [learning_rate=0.1000] [Best : Acc@1=72.73, Error=27.27]
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=78.880, Acc@5=98.650 | Loss= 0.62500

==>>[2022-08-13 23:43:03] [Epoch=007/160] [Need: 01:51:53] [learning_rate=0.1000] [Best : Acc@1=78.88, Error=21.12]
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=65.430, Acc@5=94.010 | Loss= 1.25675
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=76.860, Acc@5=98.280 | Loss= 0.70650
Val Acc@1: 76.860, Acc@5: 98.280,  Loss: 0.70650
[Pruning Method: l1norm] Flop Reduction Rate: 0.010136/0.300000 [Pruned 1 filters from 23]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=75.140, Acc@5=98.460 | Loss= 0.72770

==>>[2022-08-13 23:46:03] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=75.14, Error=24.86]
[Pruning Method: eucl] Flop Reduction Rate: 0.019266/0.300000 [Pruned 2 filters from 50]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=71.890, Acc@5=98.320 | Loss= 0.84934

==>>[2022-08-13 23:46:59] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=71.89, Error=28.11]
[Pruning Method: cos] Flop Reduction Rate: 0.028397/0.300000 [Pruned 2 filters from 42]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=74.370, Acc@5=98.560 | Loss= 0.92287

==>>[2022-08-13 23:47:55] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=74.37, Error=25.63]
[Pruning Method: eucl] Flop Reduction Rate: 0.052753/0.300000 [Pruned 1 filters from 1]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=78.740, Acc@5=98.910 | Loss= 0.62376

==>>[2022-08-13 23:48:51] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.74, Error=21.26]
[Pruning Method: l2norm] Flop Reduction Rate: 0.063253/0.300000 [Pruned 3 filters from 29]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=73.390, Acc@5=98.400 | Loss= 0.82151

==>>[2022-08-13 23:49:46] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=73.39, Error=26.61]
[Pruning Method: l1norm] Flop Reduction Rate: 0.072383/0.300000 [Pruned 2 filters from 55]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=78.760, Acc@5=97.990 | Loss= 0.68208

==>>[2022-08-13 23:50:41] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.76, Error=21.24]
[Pruning Method: l1norm] Flop Reduction Rate: 0.082165/0.300000 [Pruned 1 filters from 31]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=78.760, Acc@5=98.900 | Loss= 0.64141

==>>[2022-08-13 23:51:36] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.76, Error=21.24]
[Pruning Method: eucl] Flop Reduction Rate: 0.091292/0.300000 [Pruned 2 filters from 42]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=79.520, Acc@5=98.780 | Loss= 0.65715

==>>[2022-08-13 23:52:32] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.52, Error=20.48]
[Pruning Method: cos] Flop Reduction Rate: 0.100776/0.300000 [Pruned 6 filters from 48]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=75.610, Acc@5=98.350 | Loss= 0.83973

==>>[2022-08-13 23:53:27] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=75.61, Error=24.39]
[Pruning Method: eucl] Flop Reduction Rate: 0.110554/0.300000 [Pruned 1 filters from 31]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=66.210, Acc@5=96.510 | Loss= 1.20922

==>>[2022-08-13 23:54:22] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=66.21, Error=33.79]
[Pruning Method: cos] Flop Reduction Rate: 0.117329/0.300000 [Pruned 1 filters from 15]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.810, Acc@5=98.900 | Loss= 0.57645

==>>[2022-08-13 23:55:17] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.81, Error=19.19]
[Pruning Method: l2norm] Flop Reduction Rate: 0.124103/0.300000 [Pruned 1 filters from 10]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=82.690, Acc@5=98.990 | Loss= 0.52663

==>>[2022-08-13 23:56:12] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=82.69, Error=17.31]
[Pruning Method: l1norm] Flop Reduction Rate: 0.133926/0.300000 [Pruned 3 filters from 29]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=81.210, Acc@5=99.180 | Loss= 0.56674

==>>[2022-08-13 23:57:07] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.21, Error=18.79]
[Pruning Method: l2norm] Flop Reduction Rate: 0.140700/0.300000 [Pruned 1 filters from 10]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=76.000, Acc@5=98.430 | Loss= 0.74379

==>>[2022-08-13 23:58:02] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=76.00, Error=24.00]
[Pruning Method: cos] Flop Reduction Rate: 0.150140/0.300000 [Pruned 1 filters from 31]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=79.040, Acc@5=98.770 | Loss= 0.67690

==>>[2022-08-13 23:58:57] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.04, Error=20.96]
[Pruning Method: cos] Flop Reduction Rate: 0.158922/0.300000 [Pruned 2 filters from 45]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=78.380, Acc@5=98.470 | Loss= 0.63796

==>>[2022-08-13 23:59:52] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=78.38, Error=21.62]
[Pruning Method: l2norm] Flop Reduction Rate: 0.168359/0.300000 [Pruned 1 filters from 36]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=79.570, Acc@5=99.000 | Loss= 0.64446

==>>[2022-08-14 00:00:46] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=79.57, Error=20.43]
[Pruning Method: l1norm] Flop Reduction Rate: 0.177138/0.300000 [Pruned 2 filters from 45]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=71.780, Acc@5=97.900 | Loss= 0.93979

==>>[2022-08-14 00:01:41] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=71.78, Error=28.22]
[Pruning Method: cos] Flop Reduction Rate: 0.185944/0.300000 [Pruned 6 filters from 53]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=73.310, Acc@5=97.270 | Loss= 0.94200

==>>[2022-08-14 00:02:36] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=73.31, Error=26.69]
[Pruning Method: cos] Flop Reduction Rate: 0.195090/0.300000 [Pruned 3 filters from 29]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.790, Acc@5=98.530 | Loss= 0.60555

==>>[2022-08-14 00:03:30] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.79, Error=19.21]
[Pruning Method: eucl] Flop Reduction Rate: 0.201864/0.300000 [Pruned 1 filters from 5]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.910, Acc@5=99.130 | Loss= 0.58554

==>>[2022-08-14 00:04:24] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.91, Error=19.09]
[Pruning Method: l2norm] Flop Reduction Rate: 0.208638/0.300000 [Pruned 1 filters from 10]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=73.760, Acc@5=97.700 | Loss= 1.01623

==>>[2022-08-14 00:05:19] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=73.76, Error=26.24]
[Pruning Method: eucl] Flop Reduction Rate: 0.217078/0.300000 [Pruned 2 filters from 50]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=74.100, Acc@5=98.330 | Loss= 0.77661

==>>[2022-08-14 00:06:13] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=74.10, Error=25.90]
[Pruning Method: cos] Flop Reduction Rate: 0.226170/0.300000 [Pruned 1 filters from 36]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.550, Acc@5=98.950 | Loss= 0.57920

==>>[2022-08-14 00:07:08] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.55, Error=19.45]
[Pruning Method: l1norm] Flop Reduction Rate: 0.235262/0.300000 [Pruned 1 filters from 26]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=82.630, Acc@5=99.030 | Loss= 0.53319

==>>[2022-08-14 00:08:02] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=82.63, Error=17.37]
[Pruning Method: cos] Flop Reduction Rate: 0.243696/0.300000 [Pruned 2 filters from 45]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.090, Acc@5=99.190 | Loss= 0.58306

==>>[2022-08-14 00:08:57] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.09, Error=19.91]
[Pruning Method: cos] Flop Reduction Rate: 0.252130/0.300000 [Pruned 2 filters from 45]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=83.550, Acc@5=99.300 | Loss= 0.47974

==>>[2022-08-14 00:09:51] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=83.55, Error=16.45]
[Pruning Method: l1norm] Flop Reduction Rate: 0.258904/0.300000 [Pruned 1 filters from 10]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=81.480, Acc@5=99.190 | Loss= 0.58706

==>>[2022-08-14 00:10:45] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=81.48, Error=18.52]
[Pruning Method: l1norm] Flop Reduction Rate: 0.266695/0.300000 [Pruned 6 filters from 53]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=82.240, Acc@5=99.020 | Loss= 0.52991

==>>[2022-08-14 00:11:39] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=82.24, Error=17.76]
[Pruning Method: l1norm] Flop Reduction Rate: 0.274485/0.300000 [Pruned 6 filters from 53]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.010, Acc@5=98.800 | Loss= 0.61013

==>>[2022-08-14 00:12:33] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.01, Error=19.99]
[Pruning Method: cos] Flop Reduction Rate: 0.282953/0.300000 [Pruned 3 filters from 29]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=76.950, Acc@5=99.020 | Loss= 0.68960

==>>[2022-08-14 00:13:27] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=76.95, Error=23.05]
[Pruning Method: l1norm] Flop Reduction Rate: 0.291985/0.300000 [Pruned 4 filters from 21]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=77.590, Acc@5=98.700 | Loss= 0.73100

==>>[2022-08-14 00:14:21] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=77.59, Error=22.41]
[Pruning Method: l1norm] Flop Reduction Rate: 0.300453/0.300000 [Pruned 3 filters from 34]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=80.970, Acc@5=98.820 | Loss= 0.57275

==>>[2022-08-14 00:15:15] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.97, Error=19.03]
Prune Stats: {'l1norm': 30, 'l2norm': 7, 'eucl': 9, 'cos': 29}
Final Flop Reduction Rate: 0.3005
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 15, 5: 15, 7: 15, 10: 12, 12: 15, 15: 15, 17: 15, 21: 28, 23: 25, 26: 25, 29: 20, 31: 25, 34: 29, 36: 25, 40: 64, 42: 46, 45: 46, 48: 58, 50: 46, 53: 46, 55: 46}
Layerwise Pruning Rate: {1: 0.0625, 5: 0.0625, 7: 0.0625, 10: 0.25, 12: 0.0625, 15: 0.0625, 17: 0.0625, 21: 0.125, 23: 0.21875, 26: 0.21875, 29: 0.375, 31: 0.21875, 34: 0.09375, 36: 0.21875, 40: 0.0, 42: 0.28125, 45: 0.28125, 48: 0.09375, 50: 0.28125, 53: 0.28125, 55: 0.28125}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(28, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 25, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(25, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(20, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(25, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(29, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(25, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(25, 46, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(46, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=46, out_features=10, bias=True)
)
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=76.690, Acc@5=98.620 | Loss= 0.73160

==>>[2022-08-14 00:15:58] [Epoch=010/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=76.69, Error=23.31]
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=78.540, Acc@5=98.790 | Loss= 0.67935

==>>[2022-08-14 00:16:41] [Epoch=011/160] [Need: 01:45:59] [learning_rate=0.1000] [Best : Acc@1=78.54, Error=21.46]
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=80.010, Acc@5=98.620 | Loss= 0.61518

==>>[2022-08-14 00:17:24] [Epoch=012/160] [Need: 01:45:40] [learning_rate=0.1000] [Best : Acc@1=80.01, Error=19.99]
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=78.650, Acc@5=98.880 | Loss= 0.67047
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=72.970, Acc@5=97.860 | Loss= 0.90391
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=72.300, Acc@5=97.330 | Loss= 1.02699
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=76.560, Acc@5=97.120 | Loss= 0.75916
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=82.120, Acc@5=99.160 | Loss= 0.57080

==>>[2022-08-14 00:20:59] [Epoch=017/160] [Need: 01:42:11] [learning_rate=0.1000] [Best : Acc@1=82.12, Error=17.88]
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=76.110, Acc@5=98.350 | Loss= 0.73018
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=79.960, Acc@5=99.030 | Loss= 0.63845
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=77.090, Acc@5=99.170 | Loss= 0.71950
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=81.190, Acc@5=98.810 | Loss= 0.57564
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=79.500, Acc@5=98.690 | Loss= 0.62080
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=73.690, Acc@5=97.680 | Loss= 0.85437
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=78.630, Acc@5=98.730 | Loss= 0.67465
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=80.140, Acc@5=98.890 | Loss= 0.63235
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=77.470, Acc@5=99.040 | Loss= 0.71571
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=78.340, Acc@5=98.660 | Loss= 0.73582
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=83.400, Acc@5=99.170 | Loss= 0.50336

==>>[2022-08-14 00:28:50] [Epoch=028/160] [Need: 01:34:17] [learning_rate=0.1000] [Best : Acc@1=83.40, Error=16.60]
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=77.430, Acc@5=98.610 | Loss= 0.68470
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=72.750, Acc@5=96.910 | Loss= 0.89373
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=81.530, Acc@5=98.700 | Loss= 0.58984
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=81.140, Acc@5=98.990 | Loss= 0.60348
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=80.730, Acc@5=99.190 | Loss= 0.58445
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=78.640, Acc@5=99.100 | Loss= 0.68192
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=80.330, Acc@5=98.940 | Loss= 0.60920
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=76.120, Acc@5=98.230 | Loss= 0.72693
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=83.130, Acc@5=99.380 | Loss= 0.50713
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=82.190, Acc@5=99.270 | Loss= 0.54469
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=82.180, Acc@5=99.090 | Loss= 0.55009
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.710, Acc@5=99.810 | Loss= 0.30682

==>>[2022-08-14 00:37:26] [Epoch=040/160] [Need: 01:25:48] [learning_rate=0.0200] [Best : Acc@1=89.71, Error=10.29]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=90.130, Acc@5=99.720 | Loss= 0.29424

==>>[2022-08-14 00:38:09] [Epoch=041/160] [Need: 01:25:06] [learning_rate=0.0200] [Best : Acc@1=90.13, Error=9.87]
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=90.050, Acc@5=99.690 | Loss= 0.29964
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=89.700, Acc@5=99.680 | Loss= 0.31975
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=89.750, Acc@5=99.740 | Loss= 0.30586
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=90.170, Acc@5=99.780 | Loss= 0.29950

==>>[2022-08-14 00:41:01] [Epoch=045/160] [Need: 01:22:15] [learning_rate=0.0200] [Best : Acc@1=90.17, Error=9.83]
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=90.300, Acc@5=99.790 | Loss= 0.29409

==>>[2022-08-14 00:41:44] [Epoch=046/160] [Need: 01:21:32] [learning_rate=0.0200] [Best : Acc@1=90.30, Error=9.70]
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=89.320, Acc@5=99.790 | Loss= 0.32708
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.830, Acc@5=99.690 | Loss= 0.31475
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=89.880, Acc@5=99.680 | Loss= 0.31925
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.010, Acc@5=99.590 | Loss= 0.34913
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.730, Acc@5=99.700 | Loss= 0.31986
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=88.480, Acc@5=99.690 | Loss= 0.36152
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=88.960, Acc@5=99.640 | Loss= 0.35231
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.700 | Loss= 0.36096
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=87.070, Acc@5=99.600 | Loss= 0.43517
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=87.990, Acc@5=99.590 | Loss= 0.39189
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=87.970, Acc@5=99.630 | Loss= 0.39121
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=86.840, Acc@5=99.630 | Loss= 0.41463
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=87.480, Acc@5=99.630 | Loss= 0.40661
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.640 | Loss= 0.35240
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=88.000, Acc@5=99.520 | Loss= 0.37284
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=88.300, Acc@5=99.630 | Loss= 0.35622
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=87.570, Acc@5=99.600 | Loss= 0.42487
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.800 | Loss= 0.35234
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=87.820, Acc@5=99.550 | Loss= 0.40600
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=88.600, Acc@5=99.620 | Loss= 0.37736
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=88.810, Acc@5=99.670 | Loss= 0.34908
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=88.210, Acc@5=99.610 | Loss= 0.38762
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=87.250, Acc@5=99.590 | Loss= 0.41390
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.880, Acc@5=99.660 | Loss= 0.36004
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=86.870, Acc@5=99.500 | Loss= 0.42716
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=88.740, Acc@5=99.610 | Loss= 0.35592
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=88.760, Acc@5=99.640 | Loss= 0.35681
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=88.260, Acc@5=99.730 | Loss= 0.38146
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=88.150, Acc@5=99.600 | Loss= 0.37482
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=87.360, Acc@5=99.520 | Loss= 0.41548
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=87.380, Acc@5=99.630 | Loss= 0.39247
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=86.780, Acc@5=99.330 | Loss= 0.43052
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=87.550, Acc@5=99.500 | Loss= 0.41400
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.260, Acc@5=99.750 | Loss= 0.27806

==>>[2022-08-14 01:06:01] [Epoch=080/160] [Need: 00:57:11] [learning_rate=0.0040] [Best : Acc@1=91.26, Error=8.74]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.730 | Loss= 0.27505
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.600, Acc@5=99.800 | Loss= 0.26981

==>>[2022-08-14 01:07:27] [Epoch=082/160] [Need: 00:55:45] [learning_rate=0.0040] [Best : Acc@1=91.60, Error=8.40]
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.260, Acc@5=99.790 | Loss= 0.28236
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.370, Acc@5=99.770 | Loss= 0.27313
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.560, Acc@5=99.730 | Loss= 0.28262
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.230, Acc@5=99.740 | Loss= 0.28057
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.390, Acc@5=99.750 | Loss= 0.28347
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.530, Acc@5=99.750 | Loss= 0.28289
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.300, Acc@5=99.740 | Loss= 0.28548
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.750 | Loss= 0.29777
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.480, Acc@5=99.750 | Loss= 0.28938
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.590, Acc@5=99.770 | Loss= 0.27808
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.790 | Loss= 0.29531
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.390, Acc@5=99.720 | Loss= 0.28779
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.140, Acc@5=99.730 | Loss= 0.29634
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=91.200, Acc@5=99.760 | Loss= 0.30138
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.290, Acc@5=99.750 | Loss= 0.29620
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.240, Acc@5=99.720 | Loss= 0.30443
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=90.990, Acc@5=99.730 | Loss= 0.31055
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.070, Acc@5=99.790 | Loss= 0.30456
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=91.250, Acc@5=99.730 | Loss= 0.30229
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.770 | Loss= 0.30936
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.730 | Loss= 0.31112
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=90.980, Acc@5=99.700 | Loss= 0.31789
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=91.140, Acc@5=99.650 | Loss= 0.30933
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=91.210, Acc@5=99.720 | Loss= 0.30806
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.160, Acc@5=99.680 | Loss= 0.31171
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=91.160, Acc@5=99.710 | Loss= 0.31402
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=91.220, Acc@5=99.740 | Loss= 0.31703
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=91.140, Acc@5=99.780 | Loss= 0.31118
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=90.700, Acc@5=99.690 | Loss= 0.33453
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.710 | Loss= 0.31318
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=90.880, Acc@5=99.710 | Loss= 0.32058
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=90.890, Acc@5=99.680 | Loss= 0.31899
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=90.900, Acc@5=99.690 | Loss= 0.32387
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=91.110, Acc@5=99.710 | Loss= 0.31882
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=90.730, Acc@5=99.680 | Loss= 0.32447
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.730 | Loss= 0.31705
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=91.210, Acc@5=99.640 | Loss= 0.31640
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.400, Acc@5=99.740 | Loss= 0.30175
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.690 | Loss= 0.29836
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.460, Acc@5=99.680 | Loss= 0.30142
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.670 | Loss= 0.29965
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.710 | Loss= 0.29800
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.680 | Loss= 0.30035
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.660 | Loss= 0.30286
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.670 | Loss= 0.30240
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.580, Acc@5=99.670 | Loss= 0.30157
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.470, Acc@5=99.690 | Loss= 0.30441
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.680 | Loss= 0.30222
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.690 | Loss= 0.30249
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.440, Acc@5=99.680 | Loss= 0.30250
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.690 | Loss= 0.30368
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.560, Acc@5=99.690 | Loss= 0.30648
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.500, Acc@5=99.720 | Loss= 0.30436
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.470, Acc@5=99.740 | Loss= 0.30576
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.690 | Loss= 0.30445
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.700 | Loss= 0.30781
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.520, Acc@5=99.690 | Loss= 0.30910
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.730 | Loss= 0.30690
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.430, Acc@5=99.720 | Loss= 0.31131
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.440, Acc@5=99.690 | Loss= 0.30922
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.410, Acc@5=99.740 | Loss= 0.30795
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.730 | Loss= 0.30844
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.720 | Loss= 0.30938
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.400, Acc@5=99.730 | Loss= 0.30489
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.400, Acc@5=99.700 | Loss= 0.30736
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.730 | Loss= 0.30898
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.490, Acc@5=99.740 | Loss= 0.30698
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.740 | Loss= 0.31054
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.750 | Loss= 0.31086
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.520, Acc@5=99.740 | Loss= 0.30651
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.430, Acc@5=99.750 | Loss= 0.31207
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.450, Acc@5=99.720 | Loss= 0.31054
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.510, Acc@5=99.730 | Loss= 0.31098
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.690 | Loss= 0.31129
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.680 | Loss= 0.31019
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.740 | Loss= 0.30933
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.480, Acc@5=99.730 | Loss= 0.31179
