save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/100.resnet20.3.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/100.resnet20.3.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 100, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 3877, 'cuda': True, 'use_cuda': True}
Random Seed: 3877
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/100.resnet20.3.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=100, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=3877, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=46.470, Acc@5=91.860 | Loss= 1.56655

==>>[2022-08-16 13:10:34] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=46.47, Error=53.53]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=58.460, Acc@5=95.780 | Loss= 1.23370

==>>[2022-08-16 13:11:18] [Epoch=001/160] [Need: 02:05:28] [learning_rate=0.1000] [Best : Acc@1=58.46, Error=41.54]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=68.510, Acc@5=97.780 | Loss= 0.90900

==>>[2022-08-16 13:12:02] [Epoch=002/160] [Need: 02:00:45] [learning_rate=0.1000] [Best : Acc@1=68.51, Error=31.49]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=66.660, Acc@5=97.580 | Loss= 0.97741
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=71.640, Acc@5=98.470 | Loss= 0.83718

==>>[2022-08-16 13:13:30] [Epoch=004/160] [Need: 01:57:02] [learning_rate=0.1000] [Best : Acc@1=71.64, Error=28.36]
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=64.750, Acc@5=96.090 | Loss= 1.15081
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=67.250, Acc@5=96.970 | Loss= 1.09008
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=69.960, Acc@5=97.530 | Loss= 0.94477
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=67.700, Acc@5=96.640 | Loss= 1.09398
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=76.770, Acc@5=98.360 | Loss= 0.69850

==>>[2022-08-16 13:17:10] [Epoch=009/160] [Need: 01:51:44] [learning_rate=0.1000] [Best : Acc@1=76.77, Error=23.23]
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=72.070, Acc@5=97.840 | Loss= 0.91693
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=74.640, Acc@5=97.920 | Loss= 0.76323
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=79.350, Acc@5=98.820 | Loss= 0.61314

==>>[2022-08-16 13:19:22] [Epoch=012/160] [Need: 01:49:16] [learning_rate=0.1000] [Best : Acc@1=79.35, Error=20.65]
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=77.920, Acc@5=98.530 | Loss= 0.67001
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=78.910, Acc@5=98.970 | Loss= 0.61598
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=78.570, Acc@5=99.180 | Loss= 0.64150
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=81.970, Acc@5=99.150 | Loss= 0.55567

==>>[2022-08-16 13:22:17] [Epoch=016/160] [Need: 01:46:03] [learning_rate=0.1000] [Best : Acc@1=81.97, Error=18.03]
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=79.660, Acc@5=98.590 | Loss= 0.61503
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=80.750, Acc@5=99.050 | Loss= 0.56393
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=75.790, Acc@5=98.490 | Loss= 0.74245
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=80.360, Acc@5=99.070 | Loss= 0.60277
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=77.670, Acc@5=98.410 | Loss= 0.72238
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=80.330, Acc@5=98.950 | Loss= 0.61864
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=78.280, Acc@5=99.030 | Loss= 0.63535
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=80.390, Acc@5=99.050 | Loss= 0.58428
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=81.900, Acc@5=98.910 | Loss= 0.55903
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=83.600, Acc@5=99.470 | Loss= 0.48579

==>>[2022-08-16 13:29:38] [Epoch=026/160] [Need: 01:38:34] [learning_rate=0.1000] [Best : Acc@1=83.60, Error=16.40]
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=76.600, Acc@5=98.770 | Loss= 0.75397
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=80.520, Acc@5=99.080 | Loss= 0.60996
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=77.220, Acc@5=99.050 | Loss= 0.68144
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=79.910, Acc@5=98.910 | Loss= 0.65353
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=79.770, Acc@5=99.180 | Loss= 0.61449
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=82.480, Acc@5=99.270 | Loss= 0.52932
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=82.880, Acc@5=98.950 | Loss= 0.52051
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=81.680, Acc@5=99.070 | Loss= 0.57844
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=81.370, Acc@5=98.690 | Loss= 0.57138
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=77.430, Acc@5=98.270 | Loss= 0.70286
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=79.700, Acc@5=99.090 | Loss= 0.61481
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=83.150, Acc@5=99.340 | Loss= 0.51400
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=84.890, Acc@5=99.200 | Loss= 0.46039

==>>[2022-08-16 13:39:09] [Epoch=039/160] [Need: 01:28:52] [learning_rate=0.1000] [Best : Acc@1=84.89, Error=15.11]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=90.240, Acc@5=99.690 | Loss= 0.29686

==>>[2022-08-16 13:39:53] [Epoch=040/160] [Need: 01:28:07] [learning_rate=0.0200] [Best : Acc@1=90.24, Error=9.76]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=90.150, Acc@5=99.690 | Loss= 0.29974
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=90.110, Acc@5=99.710 | Loss= 0.30231
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=89.640, Acc@5=99.680 | Loss= 0.32488
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=89.580, Acc@5=99.690 | Loss= 0.31799
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=89.860, Acc@5=99.650 | Loss= 0.31684
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=89.780, Acc@5=99.690 | Loss= 0.31614
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=90.170, Acc@5=99.690 | Loss= 0.31410
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.580 | Loss= 0.34302
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.620 | Loss= 0.34342
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.560, Acc@5=99.690 | Loss= 0.32958
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.540, Acc@5=99.620 | Loss= 0.33530
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=89.180, Acc@5=99.700 | Loss= 0.37047
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=89.030, Acc@5=99.700 | Loss= 0.35468
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=88.080, Acc@5=99.590 | Loss= 0.38936
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=88.400, Acc@5=99.680 | Loss= 0.37954
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=89.140, Acc@5=99.730 | Loss= 0.34230
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=88.820, Acc@5=99.640 | Loss= 0.35513
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=89.610, Acc@5=99.750 | Loss= 0.32379
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=88.370, Acc@5=99.690 | Loss= 0.38757
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=88.540, Acc@5=99.580 | Loss= 0.38747
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=88.050, Acc@5=99.630 | Loss= 0.40266
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=89.610, Acc@5=99.640 | Loss= 0.34571
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=87.420, Acc@5=99.420 | Loss= 0.40822
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.710 | Loss= 0.34221
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=88.480, Acc@5=99.640 | Loss= 0.38257
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.530 | Loss= 0.37555
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.490 | Loss= 0.39372
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=87.860, Acc@5=99.490 | Loss= 0.43149
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=88.340, Acc@5=99.530 | Loss= 0.39526
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.920, Acc@5=99.550 | Loss= 0.45000
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=88.920, Acc@5=99.670 | Loss= 0.36699
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=87.940, Acc@5=99.530 | Loss= 0.38947
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=88.110, Acc@5=99.440 | Loss= 0.40322
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=88.930, Acc@5=99.640 | Loss= 0.37854
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=86.080, Acc@5=99.640 | Loss= 0.46914
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=81.730, Acc@5=99.230 | Loss= 0.68557
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=88.660, Acc@5=99.620 | Loss= 0.37852
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=88.720, Acc@5=99.520 | Loss= 0.38352
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=88.210, Acc@5=99.600 | Loss= 0.39982
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.580, Acc@5=99.700 | Loss= 0.28087

==>>[2022-08-16 14:09:16] [Epoch=080/160] [Need: 00:58:46] [learning_rate=0.0040] [Best : Acc@1=91.58, Error=8.42]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.490, Acc@5=99.700 | Loss= 0.28111
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.880, Acc@5=99.750 | Loss= 0.27810

==>>[2022-08-16 14:10:45] [Epoch=082/160] [Need: 00:57:17] [learning_rate=0.0040] [Best : Acc@1=91.88, Error=8.12]
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.580, Acc@5=99.710 | Loss= 0.27894
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.710, Acc@5=99.760 | Loss= 0.28464
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.790, Acc@5=99.750 | Loss= 0.27728
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.640, Acc@5=99.760 | Loss= 0.28492
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.600, Acc@5=99.750 | Loss= 0.28740
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.600, Acc@5=99.770 | Loss= 0.28971
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.640, Acc@5=99.740 | Loss= 0.29606
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.840, Acc@5=99.770 | Loss= 0.29300
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.770, Acc@5=99.750 | Loss= 0.28914
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.230, Acc@5=99.760 | Loss= 0.30367
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.650, Acc@5=99.710 | Loss= 0.29136
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.840, Acc@5=99.770 | Loss= 0.29235
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.630, Acc@5=99.760 | Loss= 0.29830
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=91.650, Acc@5=99.790 | Loss= 0.29903
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.670, Acc@5=99.770 | Loss= 0.30004
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.700, Acc@5=99.770 | Loss= 0.30579
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=91.650, Acc@5=99.640 | Loss= 0.30799
Val Acc@1: 91.650, Acc@5: 99.640,  Loss: 0.30799
[Pruning Method: eucl] Flop Reduction Rate: 0.007226/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.310, Acc@5=99.770 | Loss= 0.30924

==>>[2022-08-16 14:24:54] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.31, Error=8.69]
[Pruning Method: eucl] Flop Reduction Rate: 0.014452/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.470, Acc@5=99.740 | Loss= 0.31727

==>>[2022-08-16 14:25:52] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.47, Error=8.53]
[Pruning Method: l1norm] Flop Reduction Rate: 0.021678/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.640, Acc@5=99.770 | Loss= 0.31474

==>>[2022-08-16 14:26:49] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.64, Error=8.36]
[Pruning Method: l1norm] Flop Reduction Rate: 0.028904/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.690 | Loss= 0.33901

==>>[2022-08-16 14:27:47] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.17, Error=8.83]
[Pruning Method: l1norm] Flop Reduction Rate: 0.036130/0.300000 [Pruned 1 filters from 10]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.610, Acc@5=99.690 | Loss= 0.32059

==>>[2022-08-16 14:28:44] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.61, Error=8.39]
[Pruning Method: l1norm] Flop Reduction Rate: 0.046968/0.300000 [Pruned 3 filters from 29]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.680 | Loss= 0.32289

==>>[2022-08-16 14:29:41] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.18, Error=8.82]
[Pruning Method: l1norm] Flop Reduction Rate: 0.057807/0.300000 [Pruned 3 filters from 29]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.120, Acc@5=99.740 | Loss= 0.32636

==>>[2022-08-16 14:30:38] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.12, Error=8.88]
[Pruning Method: l1norm] Flop Reduction Rate: 0.065033/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.370, Acc@5=99.650 | Loss= 0.31886

==>>[2022-08-16 14:31:34] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.37, Error=8.63]
[Pruning Method: l2norm] Flop Reduction Rate: 0.072259/0.300000 [Pruned 1 filters from 10]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.250, Acc@5=99.700 | Loss= 0.31723

==>>[2022-08-16 14:32:30] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.25, Error=8.75]
[Pruning Method: l1norm] Flop Reduction Rate: 0.079485/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.660 | Loss= 0.33031

==>>[2022-08-16 14:33:27] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.09, Error=8.91]
[Pruning Method: l1norm] Flop Reduction Rate: 0.086711/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.020, Acc@5=99.600 | Loss= 0.33388

==>>[2022-08-16 14:34:24] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.02, Error=8.98]
[Pruning Method: cos] Flop Reduction Rate: 0.097550/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.470, Acc@5=99.710 | Loss= 0.32170

==>>[2022-08-16 14:35:20] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.47, Error=8.53]
[Pruning Method: eucl] Flop Reduction Rate: 0.104776/0.300000 [Pruned 1 filters from 10]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.640, Acc@5=99.630 | Loss= 0.35639

==>>[2022-08-16 14:36:16] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.64, Error=9.36]
[Pruning Method: l1norm] Flop Reduction Rate: 0.115614/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.700, Acc@5=99.680 | Loss= 0.34541

==>>[2022-08-16 14:37:12] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.70, Error=9.30]
[Pruning Method: eucl] Flop Reduction Rate: 0.122840/0.300000 [Pruned 1 filters from 10]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.120, Acc@5=99.740 | Loss= 0.33614

==>>[2022-08-16 14:38:08] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.12, Error=8.88]
[Pruning Method: l1norm] Flop Reduction Rate: 0.130066/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.050, Acc@5=99.660 | Loss= 0.33512

==>>[2022-08-16 14:39:04] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.05, Error=8.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.140905/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.030, Acc@5=99.690 | Loss= 0.32779

==>>[2022-08-16 14:39:59] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.03, Error=8.97]
[Pruning Method: l1norm] Flop Reduction Rate: 0.148131/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.750, Acc@5=99.720 | Loss= 0.34249

==>>[2022-08-16 14:40:56] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.75, Error=9.25]
[Pruning Method: l1norm] Flop Reduction Rate: 0.155357/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.760, Acc@5=99.740 | Loss= 0.34678

==>>[2022-08-16 14:41:51] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.76, Error=9.24]
[Pruning Method: l1norm] Flop Reduction Rate: 0.166196/0.300000 [Pruned 3 filters from 29]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.380, Acc@5=99.640 | Loss= 0.35559

==>>[2022-08-16 14:42:47] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.38, Error=9.62]
[Pruning Method: eucl] Flop Reduction Rate: 0.173422/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.580, Acc@5=99.710 | Loss= 0.35972

==>>[2022-08-16 14:43:43] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.58, Error=9.42]
[Pruning Method: eucl] Flop Reduction Rate: 0.180648/0.300000 [Pruned 1 filters from 10]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.670, Acc@5=99.700 | Loss= 0.34329

==>>[2022-08-16 14:44:38] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.67, Error=9.33]
[Pruning Method: l1norm] Flop Reduction Rate: 0.187873/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.920, Acc@5=99.640 | Loss= 0.34453

==>>[2022-08-16 14:45:34] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.92, Error=9.08]
[Pruning Method: eucl] Flop Reduction Rate: 0.198712/0.300000 [Pruned 3 filters from 29]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.780, Acc@5=99.740 | Loss= 0.34924

==>>[2022-08-16 14:46:29] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.78, Error=9.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.209551/0.300000 [Pruned 3 filters from 29]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.870, Acc@5=99.690 | Loss= 0.34199

==>>[2022-08-16 14:47:24] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.87, Error=9.13]
[Pruning Method: l1norm] Flop Reduction Rate: 0.216777/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.250, Acc@5=99.660 | Loss= 0.35732

==>>[2022-08-16 14:48:20] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.25, Error=9.75]
[Pruning Method: l1norm] Flop Reduction Rate: 0.227616/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.380, Acc@5=99.680 | Loss= 0.34792

==>>[2022-08-16 14:49:15] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.38, Error=9.62]
[Pruning Method: l1norm] Flop Reduction Rate: 0.234842/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.300, Acc@5=99.680 | Loss= 0.36706

==>>[2022-08-16 14:50:10] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.30, Error=9.70]
[Pruning Method: l1norm] Flop Reduction Rate: 0.242068/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.610, Acc@5=99.650 | Loss= 0.35456

==>>[2022-08-16 14:51:05] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.61, Error=9.39]
[Pruning Method: l1norm] Flop Reduction Rate: 0.249294/0.300000 [Pruned 1 filters from 15]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.280, Acc@5=99.730 | Loss= 0.36775

==>>[2022-08-16 14:52:00] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.28, Error=9.72]
[Pruning Method: l2norm] Flop Reduction Rate: 0.258427/0.300000 [Pruned 2 filters from 50]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.290, Acc@5=99.690 | Loss= 0.36532

==>>[2022-08-16 14:52:55] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.29, Error=9.71]
[Pruning Method: l1norm] Flop Reduction Rate: 0.265512/0.300000 [Pruned 1 filters from 36]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.030, Acc@5=99.700 | Loss= 0.36742

==>>[2022-08-16 14:53:50] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.03, Error=9.97]
[Pruning Method: l1norm] Flop Reduction Rate: 0.276012/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=89.530, Acc@5=99.580 | Loss= 0.38560

==>>[2022-08-16 14:54:44] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=89.53, Error=10.47]
[Pruning Method: l2norm] Flop Reduction Rate: 0.283238/0.300000 [Pruned 1 filters from 5]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.090, Acc@5=99.720 | Loss= 0.36823

==>>[2022-08-16 14:55:38] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.09, Error=9.91]
[Pruning Method: cos] Flop Reduction Rate: 0.293738/0.300000 [Pruned 6 filters from 53]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=89.800, Acc@5=99.670 | Loss= 0.37390

==>>[2022-08-16 14:56:33] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=89.80, Error=10.20]
[Pruning Method: cos] Flop Reduction Rate: 0.304238/0.300000 [Pruned 3 filters from 34]
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=89.910, Acc@5=99.660 | Loss= 0.37516

==>>[2022-08-16 14:57:28] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=89.91, Error=10.09]
Prune Stats: {'l1norm': 39, 'l2norm': 4, 'eucl': 9, 'cos': 12}
Final Flop Reduction Rate: 0.3042
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 9, 7: 16, 10: 11, 12: 16, 15: 6, 17: 16, 21: 32, 23: 31, 26: 31, 29: 17, 31: 31, 34: 14, 36: 31, 40: 64, 42: 62, 45: 62, 48: 64, 50: 62, 53: 58, 55: 62}
Layerwise Pruning Rate: {1: 0.0, 5: 0.4375, 7: 0.0, 10: 0.3125, 12: 0.0, 15: 0.625, 17: 0.0, 21: 0.0, 23: 0.03125, 26: 0.03125, 29: 0.46875, 31: 0.03125, 34: 0.5625, 36: 0.03125, 40: 0.0, 42: 0.03125, 45: 0.03125, 48: 0.0, 50: 0.03125, 53: 0.09375, 55: 0.03125}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(31, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(17, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(31, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(31, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(31, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=62, out_features=10, bias=True)
)
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=89.730, Acc@5=99.720 | Loss= 0.38409

==>>[2022-08-16 14:58:11] [Epoch=100/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=89.73, Error=10.27]
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=89.570, Acc@5=99.630 | Loss= 0.39753
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=89.240, Acc@5=99.630 | Loss= 0.41166
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=90.010, Acc@5=99.690 | Loss= 0.37206

==>>[2022-08-16 15:00:22] [Epoch=103/160] [Need: 00:41:18] [learning_rate=0.0040] [Best : Acc@1=90.01, Error=9.99]
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=89.940, Acc@5=99.740 | Loss= 0.38651
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=90.070, Acc@5=99.700 | Loss= 0.37766

==>>[2022-08-16 15:01:49] [Epoch=105/160] [Need: 00:39:49] [learning_rate=0.0040] [Best : Acc@1=90.07, Error=9.93]
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=90.500, Acc@5=99.740 | Loss= 0.36250

==>>[2022-08-16 15:02:33] [Epoch=106/160] [Need: 00:39:09] [learning_rate=0.0040] [Best : Acc@1=90.50, Error=9.50]
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=90.050, Acc@5=99.690 | Loss= 0.38512
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=89.760, Acc@5=99.750 | Loss= 0.37311
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=90.400, Acc@5=99.700 | Loss= 0.35546
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=90.000, Acc@5=99.750 | Loss= 0.37199
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=89.990, Acc@5=99.810 | Loss= 0.38161
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=90.190, Acc@5=99.710 | Loss= 0.36205
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=89.870, Acc@5=99.700 | Loss= 0.36844
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=90.060, Acc@5=99.670 | Loss= 0.37049
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=90.220, Acc@5=99.800 | Loss= 0.37012
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=90.420, Acc@5=99.680 | Loss= 0.35544
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=89.900, Acc@5=99.720 | Loss= 0.39067
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=90.120, Acc@5=99.660 | Loss= 0.36851
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=90.030, Acc@5=99.690 | Loss= 0.39260
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.690 | Loss= 0.33513

==>>[2022-08-16 15:12:46] [Epoch=120/160] [Need: 00:29:01] [learning_rate=0.0008] [Best : Acc@1=91.16, Error=8.84]
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.060, Acc@5=99.690 | Loss= 0.33592
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=90.960, Acc@5=99.710 | Loss= 0.33558
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.080, Acc@5=99.680 | Loss= 0.34027
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.090, Acc@5=99.720 | Loss= 0.33754
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.690 | Loss= 0.33623

==>>[2022-08-16 15:16:24] [Epoch=125/160] [Need: 00:25:29] [learning_rate=0.0008] [Best : Acc@1=91.21, Error=8.79]
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.740 | Loss= 0.33762
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.070, Acc@5=99.720 | Loss= 0.33874
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.690 | Loss= 0.33919

==>>[2022-08-16 15:18:34] [Epoch=128/160] [Need: 00:23:17] [learning_rate=0.0008] [Best : Acc@1=91.23, Error=8.77]
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.070, Acc@5=99.690 | Loss= 0.34334
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.700 | Loss= 0.34064
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.110, Acc@5=99.650 | Loss= 0.34282
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.720 | Loss= 0.33933
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.090, Acc@5=99.680 | Loss= 0.34357
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.700 | Loss= 0.34085

==>>[2022-08-16 15:22:56] [Epoch=134/160] [Need: 00:18:55] [learning_rate=0.0008] [Best : Acc@1=91.37, Error=8.63]
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.700 | Loss= 0.34072
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.180, Acc@5=99.680 | Loss= 0.34093
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.660 | Loss= 0.34331
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.690 | Loss= 0.33960
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.050, Acc@5=99.720 | Loss= 0.34368
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.170, Acc@5=99.730 | Loss= 0.34658
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.690 | Loss= 0.34148
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.700 | Loss= 0.34378
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.190, Acc@5=99.710 | Loss= 0.34337
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.680 | Loss= 0.34103
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.350, Acc@5=99.710 | Loss= 0.34376
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.720 | Loss= 0.34287
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.700 | Loss= 0.34808
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.730 | Loss= 0.34639
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.710 | Loss= 0.34391
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.680 | Loss= 0.34739
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.720 | Loss= 0.34464
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.100, Acc@5=99.720 | Loss= 0.34670
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.690 | Loss= 0.34996
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.700 | Loss= 0.34367
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.660 | Loss= 0.34660
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.030, Acc@5=99.670 | Loss= 0.35313
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.110, Acc@5=99.680 | Loss= 0.34536
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=90.990, Acc@5=99.690 | Loss= 0.34601
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=90.990, Acc@5=99.670 | Loss= 0.34987
