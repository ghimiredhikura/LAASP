save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/90.resnet20.2.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/90.resnet20.2.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 90, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 1792, 'cuda': True, 'use_cuda': True}
Random Seed: 1792
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/90.resnet20.2.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=90, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=1792, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=44.110, Acc@5=91.990 | Loss= 1.56300

==>>[2022-08-14 19:17:55] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=44.11, Error=55.89]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=60.570, Acc@5=96.430 | Loss= 1.12512

==>>[2022-08-14 19:18:39] [Epoch=001/160] [Need: 02:03:08] [learning_rate=0.1000] [Best : Acc@1=60.57, Error=39.43]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=61.270, Acc@5=95.980 | Loss= 1.14703

==>>[2022-08-14 19:19:22] [Epoch=002/160] [Need: 01:58:17] [learning_rate=0.1000] [Best : Acc@1=61.27, Error=38.73]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=71.310, Acc@5=97.750 | Loss= 0.84938

==>>[2022-08-14 19:20:06] [Epoch=003/160] [Need: 01:56:12] [learning_rate=0.1000] [Best : Acc@1=71.31, Error=28.69]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=74.950, Acc@5=98.280 | Loss= 0.75647

==>>[2022-08-14 19:20:49] [Epoch=004/160] [Need: 01:54:55] [learning_rate=0.1000] [Best : Acc@1=74.95, Error=25.05]
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=75.250, Acc@5=98.650 | Loss= 0.70854

==>>[2022-08-14 19:21:33] [Epoch=005/160] [Need: 01:53:52] [learning_rate=0.1000] [Best : Acc@1=75.25, Error=24.75]
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=75.100, Acc@5=98.650 | Loss= 0.72778
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=75.170, Acc@5=98.700 | Loss= 0.71986
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=76.670, Acc@5=98.470 | Loss= 0.71359

==>>[2022-08-14 19:23:44] [Epoch=008/160] [Need: 01:51:19] [learning_rate=0.1000] [Best : Acc@1=76.67, Error=23.33]
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=75.520, Acc@5=98.510 | Loss= 0.72519
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=77.250, Acc@5=98.270 | Loss= 0.68192

==>>[2022-08-14 19:25:11] [Epoch=010/160] [Need: 01:49:41] [learning_rate=0.1000] [Best : Acc@1=77.25, Error=22.75]
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=69.220, Acc@5=96.720 | Loss= 1.04207
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=73.840, Acc@5=98.360 | Loss= 0.82355
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=75.860, Acc@5=98.540 | Loss= 0.76069
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=78.880, Acc@5=98.430 | Loss= 0.63096

==>>[2022-08-14 19:28:06] [Epoch=014/160] [Need: 01:46:39] [learning_rate=0.1000] [Best : Acc@1=78.88, Error=21.12]
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=75.230, Acc@5=98.510 | Loss= 0.77364
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=79.360, Acc@5=98.780 | Loss= 0.60763

==>>[2022-08-14 19:29:34] [Epoch=016/160] [Need: 01:45:09] [learning_rate=0.1000] [Best : Acc@1=79.36, Error=20.64]
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=81.140, Acc@5=99.000 | Loss= 0.57230

==>>[2022-08-14 19:30:18] [Epoch=017/160] [Need: 01:44:25] [learning_rate=0.1000] [Best : Acc@1=81.14, Error=18.86]
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=79.830, Acc@5=98.860 | Loss= 0.61501
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=75.930, Acc@5=98.650 | Loss= 0.74426
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=78.930, Acc@5=98.560 | Loss= 0.64792
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=75.380, Acc@5=97.900 | Loss= 0.83784
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=78.100, Acc@5=98.810 | Loss= 0.68164
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=76.900, Acc@5=98.280 | Loss= 0.70105
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=79.440, Acc@5=98.880 | Loss= 0.65050
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=81.250, Acc@5=99.040 | Loss= 0.61620

==>>[2022-08-14 19:36:07] [Epoch=025/160] [Need: 01:38:30] [learning_rate=0.1000] [Best : Acc@1=81.25, Error=18.75]
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=79.460, Acc@5=98.960 | Loss= 0.61310
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=70.930, Acc@5=98.230 | Loss= 1.02323
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=80.430, Acc@5=98.740 | Loss= 0.61348
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=82.310, Acc@5=99.050 | Loss= 0.54110

==>>[2022-08-14 19:39:02] [Epoch=029/160] [Need: 01:35:33] [learning_rate=0.1000] [Best : Acc@1=82.31, Error=17.69]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=78.450, Acc@5=98.920 | Loss= 0.66805
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=84.870, Acc@5=99.330 | Loss= 0.43990

==>>[2022-08-14 19:40:29] [Epoch=031/160] [Need: 01:34:06] [learning_rate=0.1000] [Best : Acc@1=84.87, Error=15.13]
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=68.950, Acc@5=98.760 | Loss= 1.09621
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=83.210, Acc@5=98.960 | Loss= 0.52904
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=80.770, Acc@5=99.250 | Loss= 0.58966
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=79.020, Acc@5=98.000 | Loss= 0.68513
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=84.350, Acc@5=99.320 | Loss= 0.47040
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=82.920, Acc@5=99.330 | Loss= 0.49580
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=80.070, Acc@5=99.130 | Loss= 0.63883
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=84.030, Acc@5=99.260 | Loss= 0.48729
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.530, Acc@5=99.710 | Loss= 0.31253

==>>[2022-08-14 19:47:05] [Epoch=040/160] [Need: 01:27:36] [learning_rate=0.0200] [Best : Acc@1=89.53, Error=10.47]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=90.070, Acc@5=99.810 | Loss= 0.30262

==>>[2022-08-14 19:47:48] [Epoch=041/160] [Need: 01:26:53] [learning_rate=0.0200] [Best : Acc@1=90.07, Error=9.93]
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=89.960, Acc@5=99.670 | Loss= 0.30145
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=89.900, Acc@5=99.710 | Loss= 0.30861
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=89.350, Acc@5=99.760 | Loss= 0.32862
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=89.000, Acc@5=99.630 | Loss= 0.35118
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=90.230, Acc@5=99.750 | Loss= 0.29925

==>>[2022-08-14 19:51:27] [Epoch=046/160] [Need: 01:23:13] [learning_rate=0.0200] [Best : Acc@1=90.23, Error=9.77]
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=89.890, Acc@5=99.690 | Loss= 0.31679
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.300, Acc@5=99.600 | Loss= 0.33464
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=89.560, Acc@5=99.700 | Loss= 0.33354
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.460, Acc@5=99.680 | Loss= 0.33012
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.700, Acc@5=99.740 | Loss= 0.32911
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=89.720, Acc@5=99.680 | Loss= 0.33038
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=87.350, Acc@5=99.570 | Loss= 0.42624
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=89.190, Acc@5=99.530 | Loss= 0.34757
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=89.020, Acc@5=99.600 | Loss= 0.35493
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=88.670, Acc@5=99.590 | Loss= 0.36150
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=89.110, Acc@5=99.610 | Loss= 0.35891
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=88.290, Acc@5=99.420 | Loss= 0.40034
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=87.180, Acc@5=99.690 | Loss= 0.43110
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=89.140, Acc@5=99.640 | Loss= 0.34857
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=87.810, Acc@5=99.570 | Loss= 0.41558
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.560 | Loss= 0.37970
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=88.450, Acc@5=99.480 | Loss= 0.39424
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=85.770, Acc@5=99.510 | Loss= 0.50434
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=86.890, Acc@5=99.420 | Loss= 0.43650
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=87.660, Acc@5=99.630 | Loss= 0.39416
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=88.250, Acc@5=99.510 | Loss= 0.40734
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=87.070, Acc@5=99.410 | Loss= 0.43932
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=88.820, Acc@5=99.490 | Loss= 0.39002
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.540, Acc@5=99.530 | Loss= 0.43109
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=88.680, Acc@5=99.540 | Loss= 0.39173
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=88.320, Acc@5=99.370 | Loss= 0.40950
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=88.710, Acc@5=99.560 | Loss= 0.37824
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=88.050, Acc@5=99.550 | Loss= 0.37971
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=88.430, Acc@5=99.510 | Loss= 0.37699
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=87.610, Acc@5=99.550 | Loss= 0.41727
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=87.360, Acc@5=99.550 | Loss= 0.43619
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.500 | Loss= 0.36753
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.580 | Loss= 0.35768
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.490, Acc@5=99.720 | Loss= 0.27742

==>>[2022-08-14 20:16:13] [Epoch=080/160] [Need: 00:58:20] [learning_rate=0.0040] [Best : Acc@1=91.49, Error=8.51]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.430, Acc@5=99.700 | Loss= 0.27987
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.480, Acc@5=99.700 | Loss= 0.28447
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.720, Acc@5=99.770 | Loss= 0.28005

==>>[2022-08-14 20:18:23] [Epoch=083/160] [Need: 00:56:08] [learning_rate=0.0040] [Best : Acc@1=91.72, Error=8.28]
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.590, Acc@5=99.700 | Loss= 0.28315
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=91.770, Acc@5=99.740 | Loss= 0.27701

==>>[2022-08-14 20:19:49] [Epoch=085/160] [Need: 00:54:39] [learning_rate=0.0040] [Best : Acc@1=91.77, Error=8.23]
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.680, Acc@5=99.730 | Loss= 0.28095
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.780, Acc@5=99.730 | Loss= 0.28055

==>>[2022-08-14 20:21:16] [Epoch=087/160] [Need: 00:53:11] [learning_rate=0.0040] [Best : Acc@1=91.78, Error=8.22]
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.750, Acc@5=99.730 | Loss= 0.28403
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.550, Acc@5=99.720 | Loss= 0.28801
Val Acc@1: 91.550, Acc@5: 99.720,  Loss: 0.28801
[Pruning Method: l1norm] Flop Reduction Rate: 0.007226/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.550, Acc@5=99.750 | Loss= 0.29114

==>>[2022-08-14 20:24:17] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.55, Error=8.45]
[Pruning Method: l1norm] Flop Reduction Rate: 0.014452/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.580, Acc@5=99.720 | Loss= 0.29265

==>>[2022-08-14 20:25:15] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.58, Error=8.42]
[Pruning Method: l1norm] Flop Reduction Rate: 0.021678/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.730 | Loss= 0.29594

==>>[2022-08-14 20:26:11] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.34, Error=8.66]
[Pruning Method: l1norm] Flop Reduction Rate: 0.028904/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.500, Acc@5=99.720 | Loss= 0.29444

==>>[2022-08-14 20:27:08] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.50, Error=8.50]
[Pruning Method: l1norm] Flop Reduction Rate: 0.036130/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.720, Acc@5=99.660 | Loss= 0.29935

==>>[2022-08-14 20:28:05] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.72, Error=8.28]
[Pruning Method: cos] Flop Reduction Rate: 0.043355/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.360, Acc@5=99.730 | Loss= 0.31352

==>>[2022-08-14 20:29:02] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.36, Error=8.64]
[Pruning Method: l1norm] Flop Reduction Rate: 0.050581/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.340, Acc@5=99.730 | Loss= 0.29844

==>>[2022-08-14 20:29:58] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.34, Error=8.66]
[Pruning Method: l1norm] Flop Reduction Rate: 0.057807/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.510, Acc@5=99.730 | Loss= 0.30182

==>>[2022-08-14 20:30:54] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.51, Error=8.49]
[Pruning Method: l1norm] Flop Reduction Rate: 0.065033/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.490, Acc@5=99.690 | Loss= 0.30799

==>>[2022-08-14 20:31:50] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.49, Error=8.51]
[Pruning Method: cos] Flop Reduction Rate: 0.072259/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.440, Acc@5=99.720 | Loss= 0.30828

==>>[2022-08-14 20:32:46] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.44, Error=8.56]
[Pruning Method: l1norm] Flop Reduction Rate: 0.079485/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.550, Acc@5=99.690 | Loss= 0.30107

==>>[2022-08-14 20:33:42] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.55, Error=8.45]
[Pruning Method: cos] Flop Reduction Rate: 0.086711/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.830, Acc@5=99.770 | Loss= 0.32354

==>>[2022-08-14 20:34:38] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.83, Error=9.17]
[Pruning Method: l2norm] Flop Reduction Rate: 0.097550/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.300, Acc@5=99.710 | Loss= 0.30910

==>>[2022-08-14 20:35:34] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.30, Error=8.70]
[Pruning Method: eucl] Flop Reduction Rate: 0.104776/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.220, Acc@5=99.690 | Loss= 0.31997

==>>[2022-08-14 20:36:29] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.22, Error=8.78]
[Pruning Method: eucl] Flop Reduction Rate: 0.112001/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.730 | Loss= 0.32248

==>>[2022-08-14 20:37:25] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.09, Error=8.91]
[Pruning Method: cos] Flop Reduction Rate: 0.122840/0.300000 [Pruned 3 filters from 29]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.320, Acc@5=99.640 | Loss= 0.31586

==>>[2022-08-14 20:38:20] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.32, Error=8.68]
[Pruning Method: eucl] Flop Reduction Rate: 0.133679/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.360, Acc@5=99.700 | Loss= 0.31291

==>>[2022-08-14 20:39:15] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.36, Error=8.64]
[Pruning Method: l1norm] Flop Reduction Rate: 0.140905/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.220, Acc@5=99.690 | Loss= 0.32144

==>>[2022-08-14 20:40:10] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.22, Error=8.78]
[Pruning Method: eucl] Flop Reduction Rate: 0.148131/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.930, Acc@5=99.700 | Loss= 0.32696

==>>[2022-08-14 20:41:05] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.93, Error=9.07]
[Pruning Method: l1norm] Flop Reduction Rate: 0.158970/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.670 | Loss= 0.31838

==>>[2022-08-14 20:42:00] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=91.17, Error=8.83]
[Pruning Method: l1norm] Flop Reduction Rate: 0.166196/0.300000 [Pruned 1 filters from 5]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.990, Acc@5=99.650 | Loss= 0.32629

==>>[2022-08-14 20:42:55] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.99, Error=9.01]
[Pruning Method: cos] Flop Reduction Rate: 0.177035/0.300000 [Pruned 3 filters from 29]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.930, Acc@5=99.640 | Loss= 0.34306

==>>[2022-08-14 20:43:50] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.93, Error=9.07]
[Pruning Method: l1norm] Flop Reduction Rate: 0.187873/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.700, Acc@5=99.690 | Loss= 0.34045

==>>[2022-08-14 20:44:44] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.70, Error=9.30]
[Pruning Method: l1norm] Flop Reduction Rate: 0.198712/0.300000 [Pruned 3 filters from 29]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.700, Acc@5=99.610 | Loss= 0.34771

==>>[2022-08-14 20:45:39] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.70, Error=9.30]
[Pruning Method: l1norm] Flop Reduction Rate: 0.209551/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.720, Acc@5=99.640 | Loss= 0.33896

==>>[2022-08-14 20:46:33] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.72, Error=9.28]
[Pruning Method: cos] Flop Reduction Rate: 0.216777/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.810, Acc@5=99.680 | Loss= 0.33292

==>>[2022-08-14 20:47:28] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.81, Error=9.19]
[Pruning Method: l1norm] Flop Reduction Rate: 0.227616/0.300000 [Pruned 3 filters from 29]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.280, Acc@5=99.670 | Loss= 0.35098

==>>[2022-08-14 20:48:23] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.28, Error=9.72]
[Pruning Method: cos] Flop Reduction Rate: 0.234842/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.350, Acc@5=99.630 | Loss= 0.35040

==>>[2022-08-14 20:49:17] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.35, Error=9.65]
[Pruning Method: cos] Flop Reduction Rate: 0.242068/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.410, Acc@5=99.630 | Loss= 0.35573

==>>[2022-08-14 20:50:11] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.41, Error=9.59]
[Pruning Method: eucl] Flop Reduction Rate: 0.249294/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.760, Acc@5=99.650 | Loss= 0.33793

==>>[2022-08-14 20:51:06] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.76, Error=9.24]
[Pruning Method: cos] Flop Reduction Rate: 0.256519/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.320, Acc@5=99.690 | Loss= 0.35943

==>>[2022-08-14 20:52:00] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.32, Error=9.68]
[Pruning Method: cos] Flop Reduction Rate: 0.263745/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.560, Acc@5=99.630 | Loss= 0.34672

==>>[2022-08-14 20:52:54] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.56, Error=9.44]
[Pruning Method: eucl] Flop Reduction Rate: 0.270971/0.300000 [Pruned 1 filters from 15]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.750, Acc@5=99.600 | Loss= 0.33985

==>>[2022-08-14 20:53:48] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.75, Error=9.25]
[Pruning Method: cos] Flop Reduction Rate: 0.278197/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.200, Acc@5=99.600 | Loss= 0.35913

==>>[2022-08-14 20:54:42] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.20, Error=9.80]
[Pruning Method: eucl] Flop Reduction Rate: 0.285423/0.300000 [Pruned 1 filters from 10]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.220, Acc@5=99.650 | Loss= 0.35837

==>>[2022-08-14 20:55:36] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.22, Error=9.78]
[Pruning Method: l1norm] Flop Reduction Rate: 0.296262/0.300000 [Pruned 3 filters from 34]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.660, Acc@5=99.590 | Loss= 0.35324

==>>[2022-08-14 20:56:30] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.66, Error=9.34]
[Pruning Method: l2norm] Flop Reduction Rate: 0.307101/0.300000 [Pruned 3 filters from 29]
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=90.150, Acc@5=99.630 | Loss= 0.37302

==>>[2022-08-14 20:57:24] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=90.15, Error=9.85]
Prune Stats: {'l1norm': 29, 'l2norm': 6, 'eucl': 9, 'cos': 15}
Final Flop Reduction Rate: 0.3071
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 8, 7: 16, 10: 7, 12: 16, 15: 7, 17: 16, 21: 32, 23: 32, 26: 32, 29: 17, 31: 32, 34: 14, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Layerwise Pruning Rate: {1: 0.0, 5: 0.5, 7: 0.0, 10: 0.5625, 12: 0.0, 15: 0.5625, 17: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.46875, 31: 0.0, 34: 0.5625, 36: 0.0, 40: 0.0, 42: 0.0, 45: 0.0, 48: 0.0, 50: 0.0, 53: 0.0, 55: 0.0}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=89.970, Acc@5=99.710 | Loss= 0.36945

==>>[2022-08-14 20:58:07] [Epoch=090/160] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=89.97, Error=10.03]
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=90.350, Acc@5=99.590 | Loss= 0.35962

==>>[2022-08-14 20:58:50] [Epoch=091/160] [Need: 00:48:35] [learning_rate=0.0040] [Best : Acc@1=90.35, Error=9.65]
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=90.130, Acc@5=99.610 | Loss= 0.36103
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=90.730, Acc@5=99.590 | Loss= 0.35258

==>>[2022-08-14 21:00:16] [Epoch=093/160] [Need: 00:47:39] [learning_rate=0.0040] [Best : Acc@1=90.73, Error=9.27]
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=89.830, Acc@5=99.660 | Loss= 0.37450
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=90.080, Acc@5=99.680 | Loss= 0.36904
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=90.610, Acc@5=99.650 | Loss= 0.35098
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=90.290, Acc@5=99.660 | Loss= 0.36335
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=90.170, Acc@5=99.640 | Loss= 0.36575
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=90.430, Acc@5=99.610 | Loss= 0.36514
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=90.000, Acc@5=99.580 | Loss= 0.37672
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=90.090, Acc@5=99.620 | Loss= 0.36153
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=89.810, Acc@5=99.590 | Loss= 0.38453
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=90.300, Acc@5=99.600 | Loss= 0.36418
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=89.960, Acc@5=99.600 | Loss= 0.38730
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=90.450, Acc@5=99.540 | Loss= 0.36676
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=90.540, Acc@5=99.580 | Loss= 0.35591
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=89.620, Acc@5=99.610 | Loss= 0.40144
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=90.220, Acc@5=99.540 | Loss= 0.37661
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=90.720, Acc@5=99.590 | Loss= 0.35434
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=90.730, Acc@5=99.610 | Loss= 0.35012
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=90.120, Acc@5=99.610 | Loss= 0.38519
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=90.160, Acc@5=99.610 | Loss= 0.38765
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=90.220, Acc@5=99.540 | Loss= 0.36804
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=90.630, Acc@5=99.590 | Loss= 0.34979
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=89.960, Acc@5=99.630 | Loss= 0.38520
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=90.370, Acc@5=99.560 | Loss= 0.37009
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=90.830, Acc@5=99.580 | Loss= 0.35413

==>>[2022-08-14 21:17:27] [Epoch=117/160] [Need: 00:30:46] [learning_rate=0.0040] [Best : Acc@1=90.83, Error=9.17]
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=90.400, Acc@5=99.560 | Loss= 0.37880
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=90.370, Acc@5=99.620 | Loss= 0.36805
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.120, Acc@5=99.660 | Loss= 0.33819

==>>[2022-08-14 21:19:35] [Epoch=120/160] [Need: 00:28:36] [learning_rate=0.0008] [Best : Acc@1=91.12, Error=8.88]
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.150, Acc@5=99.700 | Loss= 0.34398

==>>[2022-08-14 21:20:18] [Epoch=121/160] [Need: 00:27:54] [learning_rate=0.0008] [Best : Acc@1=91.15, Error=8.85]
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.690 | Loss= 0.33722

==>>[2022-08-14 21:21:01] [Epoch=122/160] [Need: 00:27:10] [learning_rate=0.0008] [Best : Acc@1=91.23, Error=8.77]
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.620 | Loss= 0.33823

==>>[2022-08-14 21:21:44] [Epoch=123/160] [Need: 00:26:28] [learning_rate=0.0008] [Best : Acc@1=91.24, Error=8.76]
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.660 | Loss= 0.33720

==>>[2022-08-14 21:22:27] [Epoch=124/160] [Need: 00:25:45] [learning_rate=0.0008] [Best : Acc@1=91.29, Error=8.71]
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.620 | Loss= 0.33601
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.090, Acc@5=99.650 | Loss= 0.33871
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.620 | Loss= 0.33883
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.120, Acc@5=99.630 | Loss= 0.33713
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.640 | Loss= 0.33709

==>>[2022-08-14 21:26:01] [Epoch=129/160] [Need: 00:22:10] [learning_rate=0.0008] [Best : Acc@1=91.33, Error=8.67]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.630 | Loss= 0.33560
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.640 | Loss= 0.33585
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.660 | Loss= 0.33986
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.630 | Loss= 0.33504

==>>[2022-08-14 21:28:54] [Epoch=133/160] [Need: 00:19:19] [learning_rate=0.0008] [Best : Acc@1=91.34, Error=8.66]
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.660 | Loss= 0.33740
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.530, Acc@5=99.650 | Loss= 0.33762

==>>[2022-08-14 21:30:20] [Epoch=135/160] [Need: 00:17:53] [learning_rate=0.0008] [Best : Acc@1=91.53, Error=8.47]
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.180, Acc@5=99.680 | Loss= 0.33535
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.660 | Loss= 0.33603
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.670 | Loss= 0.33582
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.650 | Loss= 0.33648
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.360, Acc@5=99.680 | Loss= 0.33717
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.220, Acc@5=99.680 | Loss= 0.34100
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.660 | Loss= 0.33658
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.680 | Loss= 0.33895
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.400, Acc@5=99.670 | Loss= 0.33653
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.660 | Loss= 0.33932
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.100, Acc@5=99.640 | Loss= 0.33881
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.640 | Loss= 0.33539
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.630 | Loss= 0.34094
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.650 | Loss= 0.33712
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.500, Acc@5=99.680 | Loss= 0.33555
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.360, Acc@5=99.670 | Loss= 0.33620
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.680 | Loss= 0.33725
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.640 | Loss= 0.33790
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.650 | Loss= 0.34410
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.640 | Loss= 0.33852
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.320, Acc@5=99.650 | Loss= 0.33774
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.320, Acc@5=99.640 | Loss= 0.33862
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.640 | Loss= 0.33818
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.640 | Loss= 0.33732
