save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/50.resnet20.1.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/50.resnet20.1.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 50, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 3625, 'cuda': True, 'use_cuda': True}
Random Seed: 3625
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/50.resnet20.1.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=50, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=3625, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=38.450, Acc@5=90.240 | Loss= 2.36799

==>>[2022-08-13 01:48:17] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=38.45, Error=61.55]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=59.510, Acc@5=96.320 | Loss= 1.16624

==>>[2022-08-13 01:49:01] [Epoch=001/160] [Need: 02:02:13] [learning_rate=0.1000] [Best : Acc@1=59.51, Error=40.49]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=64.600, Acc@5=95.900 | Loss= 1.15113

==>>[2022-08-13 01:49:44] [Epoch=002/160] [Need: 01:57:32] [learning_rate=0.1000] [Best : Acc@1=64.60, Error=35.40]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=69.080, Acc@5=97.880 | Loss= 0.92793

==>>[2022-08-13 01:50:27] [Epoch=003/160] [Need: 01:55:21] [learning_rate=0.1000] [Best : Acc@1=69.08, Error=30.92]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=73.030, Acc@5=98.420 | Loss= 0.79702

==>>[2022-08-13 01:51:10] [Epoch=004/160] [Need: 01:53:54] [learning_rate=0.1000] [Best : Acc@1=73.03, Error=26.97]
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=59.320, Acc@5=98.060 | Loss= 1.40960
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=60.150, Acc@5=96.840 | Loss= 1.29438
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=76.420, Acc@5=98.820 | Loss= 0.71532

==>>[2022-08-13 01:53:20] [Epoch=007/160] [Need: 01:51:10] [learning_rate=0.1000] [Best : Acc@1=76.42, Error=23.58]
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=77.780, Acc@5=98.910 | Loss= 0.64235

==>>[2022-08-13 01:54:03] [Epoch=008/160] [Need: 01:50:22] [learning_rate=0.1000] [Best : Acc@1=77.78, Error=22.22]
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=73.610, Acc@5=98.490 | Loss= 0.78209
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=77.260, Acc@5=98.810 | Loss= 0.70077
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=76.690, Acc@5=98.620 | Loss= 0.71398
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=65.450, Acc@5=96.550 | Loss= 1.17256
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=77.060, Acc@5=98.740 | Loss= 0.71071
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=78.450, Acc@5=98.740 | Loss= 0.65361

==>>[2022-08-13 01:58:21] [Epoch=014/160] [Need: 01:45:31] [learning_rate=0.1000] [Best : Acc@1=78.45, Error=21.55]
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=76.860, Acc@5=98.080 | Loss= 0.71531
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=80.260, Acc@5=98.120 | Loss= 0.62083

==>>[2022-08-13 01:59:48] [Epoch=016/160] [Need: 01:43:59] [learning_rate=0.1000] [Best : Acc@1=80.26, Error=19.74]
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=79.280, Acc@5=99.080 | Loss= 0.63065
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=80.430, Acc@5=99.070 | Loss= 0.58019

==>>[2022-08-13 02:01:14] [Epoch=018/160] [Need: 01:42:30] [learning_rate=0.1000] [Best : Acc@1=80.43, Error=19.57]
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=82.060, Acc@5=99.230 | Loss= 0.52118

==>>[2022-08-13 02:01:57] [Epoch=019/160] [Need: 01:41:44] [learning_rate=0.1000] [Best : Acc@1=82.06, Error=17.94]
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=81.210, Acc@5=99.190 | Loss= 0.57924
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=81.250, Acc@5=98.980 | Loss= 0.55444
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=82.430, Acc@5=99.160 | Loss= 0.51909

==>>[2022-08-13 02:04:06] [Epoch=022/160] [Need: 01:39:30] [learning_rate=0.1000] [Best : Acc@1=82.43, Error=17.57]
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=75.730, Acc@5=98.060 | Loss= 0.74367
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=77.690, Acc@5=98.360 | Loss= 0.69648
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=77.270, Acc@5=98.690 | Loss= 0.73624
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=65.010, Acc@5=96.810 | Loss= 1.24544
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=76.820, Acc@5=98.750 | Loss= 0.81991
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=80.860, Acc@5=98.800 | Loss= 0.59708
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=84.560, Acc@5=99.340 | Loss= 0.46072

==>>[2022-08-13 02:09:09] [Epoch=029/160] [Need: 01:34:26] [learning_rate=0.1000] [Best : Acc@1=84.56, Error=15.44]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=82.540, Acc@5=99.180 | Loss= 0.53057
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=81.600, Acc@5=98.920 | Loss= 0.56100
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=75.940, Acc@5=97.930 | Loss= 0.82937
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=79.120, Acc@5=98.310 | Loss= 0.64592
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=75.950, Acc@5=98.510 | Loss= 0.83507
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=80.940, Acc@5=98.790 | Loss= 0.60057
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=82.940, Acc@5=99.190 | Loss= 0.52420
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=81.100, Acc@5=99.200 | Loss= 0.58489
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=83.030, Acc@5=99.220 | Loss= 0.50263
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=83.450, Acc@5=99.010 | Loss= 0.50020
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.500, Acc@5=99.710 | Loss= 0.31912

==>>[2022-08-13 02:17:03] [Epoch=040/160] [Need: 01:26:25] [learning_rate=0.0200] [Best : Acc@1=89.50, Error=10.50]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=89.870, Acc@5=99.730 | Loss= 0.30262

==>>[2022-08-13 02:17:46] [Epoch=041/160] [Need: 01:25:41] [learning_rate=0.0200] [Best : Acc@1=89.87, Error=10.13]
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=89.940, Acc@5=99.740 | Loss= 0.30397

==>>[2022-08-13 02:18:29] [Epoch=042/160] [Need: 01:24:57] [learning_rate=0.0200] [Best : Acc@1=89.94, Error=10.06]
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=89.860, Acc@5=99.730 | Loss= 0.31418
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=90.060, Acc@5=99.720 | Loss= 0.30441

==>>[2022-08-13 02:19:55] [Epoch=044/160] [Need: 01:23:30] [learning_rate=0.0200] [Best : Acc@1=90.06, Error=9.94]
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=89.840, Acc@5=99.720 | Loss= 0.31332
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=89.750, Acc@5=99.700 | Loss= 0.31792
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=90.250, Acc@5=99.730 | Loss= 0.30346

==>>[2022-08-13 02:22:06] [Epoch=047/160] [Need: 01:21:22] [learning_rate=0.0200] [Best : Acc@1=90.25, Error=9.75]
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.830, Acc@5=99.770 | Loss= 0.32198
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=89.010, Acc@5=99.690 | Loss= 0.34064
Val Acc@1: 89.010, Acc@5: 99.690,  Loss: 0.34064
[Pruning Method: l1norm] Flop Reduction Rate: 0.007226/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.910, Acc@5=99.730 | Loss= 0.31897

==>>[2022-08-13 02:25:06] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.91, Error=10.09]
[Pruning Method: l1norm] Flop Reduction Rate: 0.018065/0.300000 [Pruned 3 filters from 29]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.350, Acc@5=99.730 | Loss= 0.33198

==>>[2022-08-13 02:26:01] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.35, Error=10.65]
[Pruning Method: eucl] Flop Reduction Rate: 0.025291/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.210, Acc@5=99.600 | Loss= 0.34896

==>>[2022-08-13 02:26:57] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.21, Error=10.79]
[Pruning Method: eucl] Flop Reduction Rate: 0.036130/0.300000 [Pruned 3 filters from 29]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.830, Acc@5=99.560 | Loss= 0.37134

==>>[2022-08-13 02:27:53] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.83, Error=11.17]
[Pruning Method: cos] Flop Reduction Rate: 0.043355/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.630, Acc@5=99.590 | Loss= 0.36815

==>>[2022-08-13 02:28:48] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.63, Error=11.37]
[Pruning Method: l2norm] Flop Reduction Rate: 0.050581/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.650, Acc@5=99.580 | Loss= 0.38888

==>>[2022-08-13 02:29:44] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.65, Error=11.35]
[Pruning Method: l1norm] Flop Reduction Rate: 0.059715/0.300000 [Pruned 2 filters from 50]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.000, Acc@5=99.480 | Loss= 0.38709

==>>[2022-08-13 02:30:39] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.00, Error=12.00]
[Pruning Method: l2norm] Flop Reduction Rate: 0.070215/0.300000 [Pruned 6 filters from 48]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=85.710, Acc@5=99.520 | Loss= 0.45912

==>>[2022-08-13 02:31:34] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.71, Error=14.29]
[Pruning Method: cos] Flop Reduction Rate: 0.079670/0.300000 [Pruned 1 filters from 26]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.230, Acc@5=99.400 | Loss= 0.41932

==>>[2022-08-13 02:32:30] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.23, Error=12.77]
[Pruning Method: l1norm] Flop Reduction Rate: 0.086896/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.850, Acc@5=99.450 | Loss= 0.40023

==>>[2022-08-13 02:33:25] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.85, Error=12.15]
[Pruning Method: l1norm] Flop Reduction Rate: 0.094122/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.780, Acc@5=99.640 | Loss= 0.36308

==>>[2022-08-13 02:34:21] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.78, Error=11.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.101348/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.790, Acc@5=99.430 | Loss= 0.40769

==>>[2022-08-13 02:35:16] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.79, Error=12.21]
[Pruning Method: cos] Flop Reduction Rate: 0.108574/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.330, Acc@5=99.600 | Loss= 0.36138

==>>[2022-08-13 02:36:11] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.33, Error=10.67]
[Pruning Method: l1norm] Flop Reduction Rate: 0.119074/0.300000 [Pruned 3 filters from 29]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.350, Acc@5=99.680 | Loss= 0.36360

==>>[2022-08-13 02:37:06] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.35, Error=11.65]
[Pruning Method: l1norm] Flop Reduction Rate: 0.126300/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.740, Acc@5=99.610 | Loss= 0.36304

==>>[2022-08-13 02:38:01] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.74, Error=11.26]
[Pruning Method: l1norm] Flop Reduction Rate: 0.133526/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.640, Acc@5=99.590 | Loss= 0.41335

==>>[2022-08-13 02:38:56] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.64, Error=12.36]
[Pruning Method: eucl] Flop Reduction Rate: 0.140752/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.650, Acc@5=99.750 | Loss= 0.37038

==>>[2022-08-13 02:39:51] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.65, Error=11.35]
[Pruning Method: eucl] Flop Reduction Rate: 0.147978/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.470, Acc@5=99.590 | Loss= 0.37312

==>>[2022-08-13 02:40:46] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.47, Error=11.53]
[Pruning Method: l1norm] Flop Reduction Rate: 0.158478/0.300000 [Pruned 3 filters from 34]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=85.880, Acc@5=99.330 | Loss= 0.46899

==>>[2022-08-13 02:41:41] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.88, Error=14.12]
[Pruning Method: l2norm] Flop Reduction Rate: 0.168978/0.300000 [Pruned 3 filters from 34]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=86.450, Acc@5=99.580 | Loss= 0.43591

==>>[2022-08-13 02:42:36] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.45, Error=13.55]
[Pruning Method: l1norm] Flop Reduction Rate: 0.176204/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.220, Acc@5=99.560 | Loss= 0.38762

==>>[2022-08-13 02:43:30] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.22, Error=11.78]
[Pruning Method: l1norm] Flop Reduction Rate: 0.183430/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=84.430, Acc@5=99.370 | Loss= 0.52876

==>>[2022-08-13 02:44:25] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=84.43, Error=15.57]
[Pruning Method: eucl] Flop Reduction Rate: 0.190656/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.500, Acc@5=99.520 | Loss= 0.42606

==>>[2022-08-13 02:45:20] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.50, Error=12.50]
[Pruning Method: l2norm] Flop Reduction Rate: 0.201156/0.300000 [Pruned 3 filters from 29]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.430, Acc@5=99.500 | Loss= 0.42301

==>>[2022-08-13 02:46:15] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.43, Error=12.57]
[Pruning Method: l1norm] Flop Reduction Rate: 0.211656/0.300000 [Pruned 6 filters from 53]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.190, Acc@5=99.580 | Loss= 0.37693

==>>[2022-08-13 02:47:09] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.19, Error=11.81]
[Pruning Method: l1norm] Flop Reduction Rate: 0.218882/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.340, Acc@5=99.440 | Loss= 0.40597

==>>[2022-08-13 02:48:04] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.34, Error=12.66]
[Pruning Method: eucl] Flop Reduction Rate: 0.229495/0.300000 [Pruned 4 filters from 21]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.680, Acc@5=99.430 | Loss= 0.40933

==>>[2022-08-13 02:48:58] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.68, Error=12.32]
[Pruning Method: l1norm] Flop Reduction Rate: 0.239995/0.300000 [Pruned 3 filters from 29]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=82.620, Acc@5=99.070 | Loss= 0.70117

==>>[2022-08-13 02:49:53] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=82.62, Error=17.38]
[Pruning Method: eucl] Flop Reduction Rate: 0.247221/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=85.570, Acc@5=99.400 | Loss= 0.48381

==>>[2022-08-13 02:50:47] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.57, Error=14.43]
[Pruning Method: l1norm] Flop Reduction Rate: 0.254447/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=86.810, Acc@5=99.360 | Loss= 0.43822

==>>[2022-08-13 02:51:41] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.81, Error=13.19]
[Pruning Method: cos] Flop Reduction Rate: 0.261673/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.570 | Loss= 0.38330

==>>[2022-08-13 02:52:36] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.09, Error=11.91]
[Pruning Method: l1norm] Flop Reduction Rate: 0.268899/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.310, Acc@5=99.470 | Loss= 0.40711

==>>[2022-08-13 02:53:30] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.31, Error=12.69]
[Pruning Method: l1norm] Flop Reduction Rate: 0.276125/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.460, Acc@5=99.470 | Loss= 0.41308

==>>[2022-08-13 02:54:24] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.46, Error=12.54]
[Pruning Method: l2norm] Flop Reduction Rate: 0.283350/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=88.420, Acc@5=99.460 | Loss= 0.38162

==>>[2022-08-13 02:55:18] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.42, Error=11.58]
[Pruning Method: eucl] Flop Reduction Rate: 0.290576/0.300000 [Pruned 1 filters from 10]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=87.320, Acc@5=99.490 | Loss= 0.42097

==>>[2022-08-13 02:56:12] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.32, Error=12.68]
[Pruning Method: l2norm] Flop Reduction Rate: 0.297802/0.300000 [Pruned 1 filters from 5]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=85.530, Acc@5=99.240 | Loss= 0.48074

==>>[2022-08-13 02:57:05] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.53, Error=14.47]
[Pruning Method: l1norm] Flop Reduction Rate: 0.305028/0.300000 [Pruned 1 filters from 15]
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=86.830, Acc@5=99.490 | Loss= 0.42619

==>>[2022-08-13 02:57:59] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.83, Error=13.17]
Prune Stats: {'l1norm': 33, 'l2norm': 15, 'eucl': 13, 'cos': 4}
Final Flop Reduction Rate: 0.3050
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 9, 7: 16, 10: 6, 12: 16, 15: 8, 17: 16, 21: 28, 23: 31, 26: 31, 29: 17, 31: 31, 34: 26, 36: 31, 40: 64, 42: 62, 45: 62, 48: 58, 50: 62, 53: 58, 55: 62}
Layerwise Pruning Rate: {1: 0.0, 5: 0.4375, 7: 0.0, 10: 0.625, 12: 0.0, 15: 0.5, 17: 0.0, 21: 0.125, 23: 0.03125, 26: 0.03125, 29: 0.46875, 31: 0.03125, 34: 0.1875, 36: 0.03125, 40: 0.0, 42: 0.03125, 45: 0.03125, 48: 0.09375, 50: 0.03125, 53: 0.09375, 55: 0.03125}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(28, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(31, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(17, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(31, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(26, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(31, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(31, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=62, out_features=10, bias=True)
)
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=85.300, Acc@5=99.280 | Loss= 0.49823

==>>[2022-08-13 02:58:41] [Epoch=050/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.30, Error=14.70]
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=85.530, Acc@5=99.560 | Loss= 0.46533

==>>[2022-08-13 02:59:24] [Epoch=051/160] [Need: 01:17:09] [learning_rate=0.0200] [Best : Acc@1=85.53, Error=14.47]
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=87.150, Acc@5=99.500 | Loss= 0.44532

==>>[2022-08-13 03:00:07] [Epoch=052/160] [Need: 01:16:44] [learning_rate=0.0200] [Best : Acc@1=87.15, Error=12.85]
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=88.310, Acc@5=99.620 | Loss= 0.38535

==>>[2022-08-13 03:00:50] [Epoch=053/160] [Need: 01:16:07] [learning_rate=0.0200] [Best : Acc@1=88.31, Error=11.69]
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=87.320, Acc@5=99.570 | Loss= 0.41328
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=87.450, Acc@5=99.500 | Loss= 0.42037
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=87.730, Acc@5=99.580 | Loss= 0.38964
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=88.190, Acc@5=99.470 | Loss= 0.39054
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=87.300, Acc@5=99.440 | Loss= 0.41414
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=86.230, Acc@5=99.440 | Loss= 0.48367
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=87.060, Acc@5=99.430 | Loss= 0.44047
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=84.760, Acc@5=99.180 | Loss= 0.53649
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=89.300, Acc@5=99.630 | Loss= 0.35124

==>>[2022-08-13 03:07:15] [Epoch=062/160] [Need: 01:09:48] [learning_rate=0.0200] [Best : Acc@1=89.30, Error=10.70]
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=88.410, Acc@5=99.500 | Loss= 0.39393
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=86.700, Acc@5=99.550 | Loss= 0.45015
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=87.040, Acc@5=99.490 | Loss= 0.42290
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=86.660, Acc@5=99.350 | Loss= 0.44392
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=88.110, Acc@5=99.450 | Loss= 0.39008
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=88.320, Acc@5=99.580 | Loss= 0.38232
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=84.440, Acc@5=99.510 | Loss= 0.53201
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.660, Acc@5=99.310 | Loss= 0.45287
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=87.810, Acc@5=99.310 | Loss= 0.40878
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=87.800, Acc@5=99.540 | Loss= 0.40044
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=85.720, Acc@5=99.360 | Loss= 0.49993
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=87.480, Acc@5=99.480 | Loss= 0.42033
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=86.630, Acc@5=99.560 | Loss= 0.45411
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=86.750, Acc@5=99.410 | Loss= 0.43357
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=86.640, Acc@5=99.190 | Loss= 0.47438
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=87.150, Acc@5=99.580 | Loss= 0.41682
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=87.310, Acc@5=99.520 | Loss= 0.42693
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=91.160, Acc@5=99.670 | Loss= 0.29320

==>>[2022-08-13 03:20:05] [Epoch=080/160] [Need: 00:57:01] [learning_rate=0.0040] [Best : Acc@1=91.16, Error=8.84]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=91.240, Acc@5=99.700 | Loss= 0.29455

==>>[2022-08-13 03:20:47] [Epoch=081/160] [Need: 00:56:18] [learning_rate=0.0040] [Best : Acc@1=91.24, Error=8.76]
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.100, Acc@5=99.730 | Loss= 0.28971
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=90.980, Acc@5=99.690 | Loss= 0.29534
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=91.270, Acc@5=99.650 | Loss= 0.29054

==>>[2022-08-13 03:22:56] [Epoch=084/160] [Need: 00:54:10] [learning_rate=0.0040] [Best : Acc@1=91.27, Error=8.73]
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=90.990, Acc@5=99.700 | Loss= 0.29910
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=91.180, Acc@5=99.700 | Loss= 0.29379
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.050, Acc@5=99.680 | Loss= 0.30003
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=91.030, Acc@5=99.700 | Loss= 0.30322
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.040, Acc@5=99.740 | Loss= 0.30950
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.110, Acc@5=99.690 | Loss= 0.31479
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=91.010, Acc@5=99.680 | Loss= 0.31405
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=91.100, Acc@5=99.710 | Loss= 0.30658
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=90.900, Acc@5=99.660 | Loss= 0.31996
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=91.150, Acc@5=99.740 | Loss= 0.31623
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=91.100, Acc@5=99.680 | Loss= 0.33064
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=90.940, Acc@5=99.630 | Loss= 0.32398
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=91.110, Acc@5=99.660 | Loss= 0.32863
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=90.960, Acc@5=99.670 | Loss= 0.33162
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.690 | Loss= 0.32485
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.650 | Loss= 0.33522
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=91.060, Acc@5=99.640 | Loss= 0.32675
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=90.770, Acc@5=99.640 | Loss= 0.33226
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=91.320, Acc@5=99.670 | Loss= 0.32015

==>>[2022-08-13 03:36:27] [Epoch=103/160] [Need: 00:40:36] [learning_rate=0.0040] [Best : Acc@1=91.32, Error=8.68]
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=91.060, Acc@5=99.640 | Loss= 0.32668
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.650 | Loss= 0.32444
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.640 | Loss= 0.33290
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.710 | Loss= 0.33586
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=91.000, Acc@5=99.620 | Loss= 0.32856
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=91.070, Acc@5=99.590 | Loss= 0.33416
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=91.040, Acc@5=99.650 | Loss= 0.34055
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=90.810, Acc@5=99.600 | Loss= 0.34095
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=90.960, Acc@5=99.600 | Loss= 0.34578
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.600 | Loss= 0.33854
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=91.010, Acc@5=99.620 | Loss= 0.34894
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=91.030, Acc@5=99.640 | Loss= 0.34735
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=91.170, Acc@5=99.660 | Loss= 0.32952
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=90.770, Acc@5=99.610 | Loss= 0.35183
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=90.680, Acc@5=99.630 | Loss= 0.36175
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=90.900, Acc@5=99.640 | Loss= 0.33966
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=91.190, Acc@5=99.650 | Loss= 0.32732
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.670 | Loss= 0.32753

==>>[2022-08-13 03:49:15] [Epoch=121/160] [Need: 00:27:45] [learning_rate=0.0008] [Best : Acc@1=91.34, Error=8.66]
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.690 | Loss= 0.32676
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.650 | Loss= 0.32674
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.650 | Loss= 0.32798
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.620 | Loss= 0.32848
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.670 | Loss= 0.32851
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.420, Acc@5=99.640 | Loss= 0.32825

==>>[2022-08-13 03:53:33] [Epoch=127/160] [Need: 00:23:30] [learning_rate=0.0008] [Best : Acc@1=91.42, Error=8.58]
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.200, Acc@5=99.620 | Loss= 0.33089
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.320, Acc@5=99.680 | Loss= 0.33384
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.300, Acc@5=99.680 | Loss= 0.33220
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.690 | Loss= 0.33007
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.250, Acc@5=99.670 | Loss= 0.32920
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.650 | Loss= 0.33180
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.630 | Loss= 0.33323
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.650 | Loss= 0.33189
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.660 | Loss= 0.33041
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.660 | Loss= 0.33293
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.660 | Loss= 0.33240
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.620 | Loss= 0.33112
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.360, Acc@5=99.630 | Loss= 0.33397
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.620 | Loss= 0.33420
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.320, Acc@5=99.600 | Loss= 0.33387
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.610 | Loss= 0.33387
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.630 | Loss= 0.33283
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.430, Acc@5=99.620 | Loss= 0.33235

==>>[2022-08-13 04:06:29] [Epoch=145/160] [Need: 00:10:42] [learning_rate=0.0008] [Best : Acc@1=91.43, Error=8.57]
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.640 | Loss= 0.33702
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.660 | Loss= 0.33410
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.350, Acc@5=99.640 | Loss= 0.33351
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.290, Acc@5=99.640 | Loss= 0.33407
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.660 | Loss= 0.33890
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.640 | Loss= 0.33833
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.640 | Loss= 0.33943
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.120, Acc@5=99.620 | Loss= 0.33848
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.220, Acc@5=99.600 | Loss= 0.33819
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.220, Acc@5=99.640 | Loss= 0.34141
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.600 | Loss= 0.33641
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.610 | Loss= 0.33792
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.370, Acc@5=99.660 | Loss= 0.33844
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.630 | Loss= 0.33410
