save path : C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/70.resnet20.3.0.300
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet20', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/70.resnet20.3.0.300', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 160, 'prune_epoch': 70, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [40, 80, 120], 'gammas': [0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.3, 'manualSeed': 418, 'cuda': True, 'use_cuda': True}
Random Seed: 418
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet20'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet20', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot_Abla_Prune_Epoch/70.resnet20.3.0.300', mode='prune', batch_size=256, verbose=False, total_epoches=160, prune_epoch=70, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[40, 80, 120], gammas=[0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.3, manualSeed=418, cuda=True, use_cuda=True)
Epoch 0/160 [learning_rate=0.100000] Val [Acc@1=41.020, Acc@5=90.400 | Loss= 1.78447

==>>[2022-08-16 05:36:27] [Epoch=000/160] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=41.02, Error=58.98]
Epoch 1/160 [learning_rate=0.100000] Val [Acc@1=60.670, Acc@5=96.720 | Loss= 1.12812

==>>[2022-08-16 05:37:11] [Epoch=001/160] [Need: 02:03:30] [learning_rate=0.1000] [Best : Acc@1=60.67, Error=39.33]
Epoch 2/160 [learning_rate=0.100000] Val [Acc@1=62.210, Acc@5=96.380 | Loss= 1.13161

==>>[2022-08-16 05:37:55] [Epoch=002/160] [Need: 01:58:58] [learning_rate=0.1000] [Best : Acc@1=62.21, Error=37.79]
Epoch 3/160 [learning_rate=0.100000] Val [Acc@1=73.310, Acc@5=98.350 | Loss= 0.79406

==>>[2022-08-16 05:38:39] [Epoch=003/160] [Need: 01:57:12] [learning_rate=0.1000] [Best : Acc@1=73.31, Error=26.69]
Epoch 4/160 [learning_rate=0.100000] Val [Acc@1=66.700, Acc@5=96.490 | Loss= 1.02180
Epoch 5/160 [learning_rate=0.100000] Val [Acc@1=72.790, Acc@5=98.070 | Loss= 0.84673
Epoch 6/160 [learning_rate=0.100000] Val [Acc@1=76.820, Acc@5=98.430 | Loss= 0.68226

==>>[2022-08-16 05:40:50] [Epoch=006/160] [Need: 01:53:50] [learning_rate=0.1000] [Best : Acc@1=76.82, Error=23.18]
Epoch 7/160 [learning_rate=0.100000] Val [Acc@1=74.000, Acc@5=98.400 | Loss= 0.83476
Epoch 8/160 [learning_rate=0.100000] Val [Acc@1=75.690, Acc@5=98.300 | Loss= 0.76481
Epoch 9/160 [learning_rate=0.100000] Val [Acc@1=73.840, Acc@5=98.000 | Loss= 0.81649
Epoch 10/160 [learning_rate=0.100000] Val [Acc@1=74.120, Acc@5=98.420 | Loss= 0.82304
Epoch 11/160 [learning_rate=0.100000] Val [Acc@1=68.270, Acc@5=96.200 | Loss= 1.09654
Epoch 12/160 [learning_rate=0.100000] Val [Acc@1=76.870, Acc@5=98.430 | Loss= 0.67887

==>>[2022-08-16 05:45:16] [Epoch=012/160] [Need: 01:49:12] [learning_rate=0.1000] [Best : Acc@1=76.87, Error=23.13]
Epoch 13/160 [learning_rate=0.100000] Val [Acc@1=77.860, Acc@5=98.710 | Loss= 0.67985

==>>[2022-08-16 05:46:00] [Epoch=013/160] [Need: 01:48:26] [learning_rate=0.1000] [Best : Acc@1=77.86, Error=22.14]
Epoch 14/160 [learning_rate=0.100000] Val [Acc@1=76.470, Acc@5=98.040 | Loss= 0.70645
Epoch 15/160 [learning_rate=0.100000] Val [Acc@1=78.560, Acc@5=98.710 | Loss= 0.66183

==>>[2022-08-16 05:47:29] [Epoch=015/160] [Need: 01:47:00] [learning_rate=0.1000] [Best : Acc@1=78.56, Error=21.44]
Epoch 16/160 [learning_rate=0.100000] Val [Acc@1=79.140, Acc@5=98.780 | Loss= 0.64614

==>>[2022-08-16 05:48:13] [Epoch=016/160] [Need: 01:46:15] [learning_rate=0.1000] [Best : Acc@1=79.14, Error=20.86]
Epoch 17/160 [learning_rate=0.100000] Val [Acc@1=78.760, Acc@5=98.780 | Loss= 0.68636
Epoch 18/160 [learning_rate=0.100000] Val [Acc@1=79.460, Acc@5=99.010 | Loss= 0.63917

==>>[2022-08-16 05:49:42] [Epoch=018/160] [Need: 01:44:50] [learning_rate=0.1000] [Best : Acc@1=79.46, Error=20.54]
Epoch 19/160 [learning_rate=0.100000] Val [Acc@1=79.280, Acc@5=98.920 | Loss= 0.62760
Epoch 20/160 [learning_rate=0.100000] Val [Acc@1=77.960, Acc@5=98.690 | Loss= 0.67794
Epoch 21/160 [learning_rate=0.100000] Val [Acc@1=75.740, Acc@5=98.840 | Loss= 0.77735
Epoch 22/160 [learning_rate=0.100000] Val [Acc@1=81.420, Acc@5=99.020 | Loss= 0.54401

==>>[2022-08-16 05:52:36] [Epoch=022/160] [Need: 01:41:38] [learning_rate=0.1000] [Best : Acc@1=81.42, Error=18.58]
Epoch 23/160 [learning_rate=0.100000] Val [Acc@1=76.000, Acc@5=98.330 | Loss= 0.78903
Epoch 24/160 [learning_rate=0.100000] Val [Acc@1=82.290, Acc@5=99.310 | Loss= 0.51488

==>>[2022-08-16 05:54:04] [Epoch=024/160] [Need: 01:40:04] [learning_rate=0.1000] [Best : Acc@1=82.29, Error=17.71]
Epoch 25/160 [learning_rate=0.100000] Val [Acc@1=75.310, Acc@5=98.540 | Loss= 0.88705
Epoch 26/160 [learning_rate=0.100000] Val [Acc@1=79.310, Acc@5=98.840 | Loss= 0.66055
Epoch 27/160 [learning_rate=0.100000] Val [Acc@1=78.170, Acc@5=98.820 | Loss= 0.68629
Epoch 28/160 [learning_rate=0.100000] Val [Acc@1=78.820, Acc@5=98.920 | Loss= 0.71397
Epoch 29/160 [learning_rate=0.100000] Val [Acc@1=82.820, Acc@5=99.200 | Loss= 0.51036

==>>[2022-08-16 05:57:43] [Epoch=029/160] [Need: 01:36:13] [learning_rate=0.1000] [Best : Acc@1=82.82, Error=17.18]
Epoch 30/160 [learning_rate=0.100000] Val [Acc@1=84.140, Acc@5=99.190 | Loss= 0.46753

==>>[2022-08-16 05:58:27] [Epoch=030/160] [Need: 01:35:30] [learning_rate=0.1000] [Best : Acc@1=84.14, Error=15.86]
Epoch 31/160 [learning_rate=0.100000] Val [Acc@1=78.600, Acc@5=98.680 | Loss= 0.71367
Epoch 32/160 [learning_rate=0.100000] Val [Acc@1=80.130, Acc@5=98.570 | Loss= 0.60916
Epoch 33/160 [learning_rate=0.100000] Val [Acc@1=78.440, Acc@5=98.920 | Loss= 0.66273
Epoch 34/160 [learning_rate=0.100000] Val [Acc@1=73.870, Acc@5=98.800 | Loss= 0.87339
Epoch 35/160 [learning_rate=0.100000] Val [Acc@1=84.450, Acc@5=99.120 | Loss= 0.44725

==>>[2022-08-16 06:02:08] [Epoch=035/160] [Need: 01:31:50] [learning_rate=0.1000] [Best : Acc@1=84.45, Error=15.55]
Epoch 36/160 [learning_rate=0.100000] Val [Acc@1=78.150, Acc@5=98.840 | Loss= 0.73442
Epoch 37/160 [learning_rate=0.100000] Val [Acc@1=73.900, Acc@5=98.800 | Loss= 0.87928
Epoch 38/160 [learning_rate=0.100000] Val [Acc@1=79.660, Acc@5=98.710 | Loss= 0.63638
Epoch 39/160 [learning_rate=0.100000] Val [Acc@1=84.810, Acc@5=99.310 | Loss= 0.45009

==>>[2022-08-16 06:05:06] [Epoch=039/160] [Need: 01:29:02] [learning_rate=0.1000] [Best : Acc@1=84.81, Error=15.19]
Epoch 40/160 [learning_rate=0.020000] Val [Acc@1=89.810, Acc@5=99.700 | Loss= 0.30605

==>>[2022-08-16 06:05:51] [Epoch=040/160] [Need: 01:28:17] [learning_rate=0.0200] [Best : Acc@1=89.81, Error=10.19]
Epoch 41/160 [learning_rate=0.020000] Val [Acc@1=89.380, Acc@5=99.710 | Loss= 0.32262
Epoch 42/160 [learning_rate=0.020000] Val [Acc@1=90.280, Acc@5=99.760 | Loss= 0.30340

==>>[2022-08-16 06:07:18] [Epoch=042/160] [Need: 01:26:48] [learning_rate=0.0200] [Best : Acc@1=90.28, Error=9.72]
Epoch 43/160 [learning_rate=0.020000] Val [Acc@1=90.110, Acc@5=99.680 | Loss= 0.29750
Epoch 44/160 [learning_rate=0.020000] Val [Acc@1=90.190, Acc@5=99.720 | Loss= 0.29717
Epoch 45/160 [learning_rate=0.020000] Val [Acc@1=88.920, Acc@5=99.690 | Loss= 0.33626
Epoch 46/160 [learning_rate=0.020000] Val [Acc@1=89.600, Acc@5=99.690 | Loss= 0.32103
Epoch 47/160 [learning_rate=0.020000] Val [Acc@1=89.610, Acc@5=99.660 | Loss= 0.32484
Epoch 48/160 [learning_rate=0.020000] Val [Acc@1=89.390, Acc@5=99.700 | Loss= 0.33489
Epoch 49/160 [learning_rate=0.020000] Val [Acc@1=88.200, Acc@5=99.490 | Loss= 0.38160
Epoch 50/160 [learning_rate=0.020000] Val [Acc@1=89.170, Acc@5=99.670 | Loss= 0.34459
Epoch 51/160 [learning_rate=0.020000] Val [Acc@1=89.460, Acc@5=99.560 | Loss= 0.33989
Epoch 52/160 [learning_rate=0.020000] Val [Acc@1=90.030, Acc@5=99.700 | Loss= 0.31673
Epoch 53/160 [learning_rate=0.020000] Val [Acc@1=88.770, Acc@5=99.630 | Loss= 0.35468
Epoch 54/160 [learning_rate=0.020000] Val [Acc@1=88.110, Acc@5=99.600 | Loss= 0.39157
Epoch 55/160 [learning_rate=0.020000] Val [Acc@1=88.800, Acc@5=99.650 | Loss= 0.36491
Epoch 56/160 [learning_rate=0.020000] Val [Acc@1=89.450, Acc@5=99.680 | Loss= 0.33777
Epoch 57/160 [learning_rate=0.020000] Val [Acc@1=88.410, Acc@5=99.650 | Loss= 0.36106
Epoch 58/160 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.540 | Loss= 0.40020
Epoch 59/160 [learning_rate=0.020000] Val [Acc@1=87.960, Acc@5=99.480 | Loss= 0.40466
Epoch 60/160 [learning_rate=0.020000] Val [Acc@1=88.390, Acc@5=99.590 | Loss= 0.38467
Epoch 61/160 [learning_rate=0.020000] Val [Acc@1=87.160, Acc@5=99.610 | Loss= 0.43331
Epoch 62/160 [learning_rate=0.020000] Val [Acc@1=88.320, Acc@5=99.620 | Loss= 0.37499
Epoch 63/160 [learning_rate=0.020000] Val [Acc@1=87.130, Acc@5=99.360 | Loss= 0.45343
Epoch 64/160 [learning_rate=0.020000] Val [Acc@1=87.820, Acc@5=99.360 | Loss= 0.40301
Epoch 65/160 [learning_rate=0.020000] Val [Acc@1=88.560, Acc@5=99.710 | Loss= 0.36187
Epoch 66/160 [learning_rate=0.020000] Val [Acc@1=88.750, Acc@5=99.620 | Loss= 0.35611
Epoch 67/160 [learning_rate=0.020000] Val [Acc@1=89.650, Acc@5=99.610 | Loss= 0.33875
Epoch 68/160 [learning_rate=0.020000] Val [Acc@1=88.370, Acc@5=99.480 | Loss= 0.37268
Epoch 69/160 [learning_rate=0.020000] Val [Acc@1=87.260, Acc@5=99.650 | Loss= 0.42412
Val Acc@1: 87.260, Acc@5: 99.650,  Loss: 0.42412
[Pruning Method: cos] Flop Reduction Rate: 0.010839/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.940, Acc@5=99.550 | Loss= 0.40773

==>>[2022-08-16 06:28:38] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.94, Error=12.06]
[Pruning Method: cos] Flop Reduction Rate: 0.021678/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.710, Acc@5=99.470 | Loss= 0.37384

==>>[2022-08-16 06:29:35] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.71, Error=11.29]
[Pruning Method: cos] Flop Reduction Rate: 0.028904/0.300000 [Pruned 1 filters from 10]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=89.180, Acc@5=99.640 | Loss= 0.35338

==>>[2022-08-16 06:30:32] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.18, Error=10.82]
[Pruning Method: l1norm] Flop Reduction Rate: 0.036130/0.300000 [Pruned 1 filters from 10]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.850, Acc@5=99.500 | Loss= 0.40243

==>>[2022-08-16 06:31:28] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.85, Error=12.15]
[Pruning Method: l1norm] Flop Reduction Rate: 0.043355/0.300000 [Pruned 1 filters from 15]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.380, Acc@5=99.660 | Loss= 0.39422

==>>[2022-08-16 06:32:25] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.38, Error=11.62]
[Pruning Method: cos] Flop Reduction Rate: 0.054194/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.780, Acc@5=99.530 | Loss= 0.41170

==>>[2022-08-16 06:33:21] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.78, Error=12.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.061420/0.300000 [Pruned 1 filters from 15]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.560, Acc@5=99.520 | Loss= 0.40068

==>>[2022-08-16 06:34:18] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.56, Error=12.44]
[Pruning Method: cos] Flop Reduction Rate: 0.072259/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.460, Acc@5=99.490 | Loss= 0.45248

==>>[2022-08-16 06:35:14] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.46, Error=13.54]
[Pruning Method: cos] Flop Reduction Rate: 0.079485/0.300000 [Pruned 1 filters from 10]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.170, Acc@5=99.390 | Loss= 0.39833

==>>[2022-08-16 06:36:10] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.17, Error=11.83]
[Pruning Method: l2norm] Flop Reduction Rate: 0.086711/0.300000 [Pruned 1 filters from 15]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.660, Acc@5=99.440 | Loss= 0.45202

==>>[2022-08-16 06:37:06] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.66, Error=13.34]
[Pruning Method: cos] Flop Reduction Rate: 0.097550/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.460, Acc@5=99.530 | Loss= 0.38342

==>>[2022-08-16 06:38:02] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.46, Error=11.54]
[Pruning Method: l1norm] Flop Reduction Rate: 0.108389/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.500, Acc@5=99.360 | Loss= 0.43696

==>>[2022-08-16 06:38:58] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.50, Error=12.50]
[Pruning Method: cos] Flop Reduction Rate: 0.115614/0.300000 [Pruned 1 filters from 15]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.330, Acc@5=99.450 | Loss= 0.40060

==>>[2022-08-16 06:39:55] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.33, Error=11.67]
[Pruning Method: l1norm] Flop Reduction Rate: 0.122840/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.500, Acc@5=99.390 | Loss= 0.46782

==>>[2022-08-16 06:40:53] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.50, Error=13.50]
[Pruning Method: l1norm] Flop Reduction Rate: 0.133679/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.420, Acc@5=99.300 | Loss= 0.42924

==>>[2022-08-16 06:41:49] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.42, Error=12.58]
[Pruning Method: eucl] Flop Reduction Rate: 0.144518/0.300000 [Pruned 4 filters from 21]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.000, Acc@5=99.500 | Loss= 0.43579

==>>[2022-08-16 06:42:46] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.00, Error=13.00]
[Pruning Method: cos] Flop Reduction Rate: 0.155357/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=84.610, Acc@5=99.520 | Loss= 0.51160

==>>[2022-08-16 06:43:43] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=84.61, Error=15.39]
[Pruning Method: eucl] Flop Reduction Rate: 0.166196/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=85.810, Acc@5=99.380 | Loss= 0.48359

==>>[2022-08-16 06:44:40] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.81, Error=14.19]
[Pruning Method: l1norm] Flop Reduction Rate: 0.177035/0.300000 [Pruned 6 filters from 53]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.490, Acc@5=99.590 | Loss= 0.40474

==>>[2022-08-16 06:45:37] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.49, Error=12.51]
[Pruning Method: cos] Flop Reduction Rate: 0.187873/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.160, Acc@5=99.600 | Loss= 0.39209

==>>[2022-08-16 06:46:34] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.16, Error=11.84]
[Pruning Method: l1norm] Flop Reduction Rate: 0.195099/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.540, Acc@5=99.470 | Loss= 0.43956

==>>[2022-08-16 06:47:31] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.54, Error=13.46]
[Pruning Method: l1norm] Flop Reduction Rate: 0.205938/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=83.100, Acc@5=99.160 | Loss= 0.57921

==>>[2022-08-16 06:48:28] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=83.10, Error=16.90]
[Pruning Method: cos] Flop Reduction Rate: 0.213164/0.300000 [Pruned 1 filters from 10]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.310, Acc@5=99.450 | Loss= 0.41288

==>>[2022-08-16 06:49:24] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.31, Error=12.69]
[Pruning Method: eucl] Flop Reduction Rate: 0.220390/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.920, Acc@5=99.390 | Loss= 0.42945

==>>[2022-08-16 06:50:20] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.92, Error=13.08]
[Pruning Method: l1norm] Flop Reduction Rate: 0.226575/0.300000 [Pruned 1 filters from 36]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.850, Acc@5=99.340 | Loss= 0.43317

==>>[2022-08-16 06:51:17] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.85, Error=13.15]
[Pruning Method: l2norm] Flop Reduction Rate: 0.237075/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.100, Acc@5=99.460 | Loss= 0.43530

==>>[2022-08-16 06:52:14] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.10, Error=12.90]
[Pruning Method: l1norm] Flop Reduction Rate: 0.244301/0.300000 [Pruned 1 filters from 15]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.060, Acc@5=99.490 | Loss= 0.43531

==>>[2022-08-16 06:53:11] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.06, Error=12.94]
[Pruning Method: l1norm] Flop Reduction Rate: 0.251527/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.080, Acc@5=99.550 | Loss= 0.42638

==>>[2022-08-16 06:54:07] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.08, Error=12.92]
[Pruning Method: eucl] Flop Reduction Rate: 0.258752/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.050, Acc@5=99.470 | Loss= 0.39209

==>>[2022-08-16 06:55:03] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.05, Error=11.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.265978/0.300000 [Pruned 1 filters from 10]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=85.400, Acc@5=99.280 | Loss= 0.49565

==>>[2022-08-16 06:55:59] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.40, Error=14.60]
[Pruning Method: cos] Flop Reduction Rate: 0.271824/0.300000 [Pruned 1 filters from 31]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=85.490, Acc@5=99.350 | Loss= 0.49217

==>>[2022-08-16 06:56:55] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.49, Error=14.51]
[Pruning Method: l1norm] Flop Reduction Rate: 0.281986/0.300000 [Pruned 3 filters from 29]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=86.870, Acc@5=99.420 | Loss= 0.43726

==>>[2022-08-16 06:57:51] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.87, Error=13.13]
[Pruning Method: cos] Flop Reduction Rate: 0.292147/0.300000 [Pruned 3 filters from 34]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=85.510, Acc@5=99.530 | Loss= 0.47706

==>>[2022-08-16 06:58:47] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.51, Error=14.49]
[Pruning Method: eucl] Flop Reduction Rate: 0.299373/0.300000 [Pruned 1 filters from 5]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.490, Acc@5=99.620 | Loss= 0.39771

==>>[2022-08-16 06:59:41] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.49, Error=12.51]
[Pruning Method: cos] Flop Reduction Rate: 0.310212/0.300000 [Pruned 6 filters from 53]
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=88.250, Acc@5=99.640 | Loss= 0.37029

==>>[2022-08-16 07:00:36] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.25, Error=11.75]
Prune Stats: {'l1norm': 27, 'l2norm': 4, 'eucl': 10, 'cos': 35}
Final Flop Reduction Rate: 0.3102
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 21: 32, 23: 32, 26: 32, 29: 32, 31: 32, 34: 32, 36: 32, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 64, 55: 64}
Conv Filters After Pruning: {1: 16, 5: 10, 7: 16, 10: 11, 12: 16, 15: 11, 17: 16, 21: 28, 23: 30, 26: 30, 29: 11, 31: 30, 34: 11, 36: 30, 40: 64, 42: 64, 45: 64, 48: 64, 50: 64, 53: 52, 55: 64}
Layerwise Pruning Rate: {1: 0.0, 5: 0.375, 7: 0.0, 10: 0.3125, 12: 0.0, 15: 0.3125, 17: 0.0, 21: 0.125, 23: 0.0625, 26: 0.0625, 29: 0.65625, 31: 0.0625, 34: 0.65625, 36: 0.0625, 40: 0.0, 42: 0.0, 45: 0.0, 48: 0.0, 50: 0.0, 53: 0.1875, 55: 0.0}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 30, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(30, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(30, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(11, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(30, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(30, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 70/160 [learning_rate=0.020000] Val [Acc@1=87.300, Acc@5=99.590 | Loss= 0.40463

==>>[2022-08-16 07:01:20] [Epoch=070/160] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.30, Error=12.70]
Epoch 71/160 [learning_rate=0.020000] Val [Acc@1=86.520, Acc@5=99.370 | Loss= 0.44284
Epoch 72/160 [learning_rate=0.020000] Val [Acc@1=87.900, Acc@5=99.480 | Loss= 0.38276

==>>[2022-08-16 07:02:48] [Epoch=072/160] [Need: 01:04:10] [learning_rate=0.0200] [Best : Acc@1=87.90, Error=12.10]
Epoch 73/160 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.570 | Loss= 0.38676

==>>[2022-08-16 07:03:33] [Epoch=073/160] [Need: 01:03:44] [learning_rate=0.0200] [Best : Acc@1=88.09, Error=11.91]
Epoch 74/160 [learning_rate=0.020000] Val [Acc@1=86.170, Acc@5=99.460 | Loss= 0.48883
Epoch 75/160 [learning_rate=0.020000] Val [Acc@1=88.210, Acc@5=99.470 | Loss= 0.38103

==>>[2022-08-16 07:05:00] [Epoch=075/160] [Need: 01:02:20] [learning_rate=0.0200] [Best : Acc@1=88.21, Error=11.79]
Epoch 76/160 [learning_rate=0.020000] Val [Acc@1=86.910, Acc@5=99.410 | Loss= 0.43527
Epoch 77/160 [learning_rate=0.020000] Val [Acc@1=87.680, Acc@5=99.540 | Loss= 0.41532
Epoch 78/160 [learning_rate=0.020000] Val [Acc@1=86.840, Acc@5=99.410 | Loss= 0.42074
Epoch 79/160 [learning_rate=0.020000] Val [Acc@1=86.500, Acc@5=99.320 | Loss= 0.43196
Epoch 80/160 [learning_rate=0.004000] Val [Acc@1=90.700, Acc@5=99.700 | Loss= 0.30156

==>>[2022-08-16 07:08:40] [Epoch=080/160] [Need: 00:58:40] [learning_rate=0.0040] [Best : Acc@1=90.70, Error=9.30]
Epoch 81/160 [learning_rate=0.004000] Val [Acc@1=90.710, Acc@5=99.650 | Loss= 0.30379

==>>[2022-08-16 07:09:23] [Epoch=081/160] [Need: 00:57:52] [learning_rate=0.0040] [Best : Acc@1=90.71, Error=9.29]
Epoch 82/160 [learning_rate=0.004000] Val [Acc@1=91.070, Acc@5=99.660 | Loss= 0.29275

==>>[2022-08-16 07:10:07] [Epoch=082/160] [Need: 00:57:05] [learning_rate=0.0040] [Best : Acc@1=91.07, Error=8.93]
Epoch 83/160 [learning_rate=0.004000] Val [Acc@1=91.050, Acc@5=99.690 | Loss= 0.30056
Epoch 84/160 [learning_rate=0.004000] Val [Acc@1=90.710, Acc@5=99.680 | Loss= 0.30591
Epoch 85/160 [learning_rate=0.004000] Val [Acc@1=90.770, Acc@5=99.710 | Loss= 0.30587
Epoch 86/160 [learning_rate=0.004000] Val [Acc@1=90.780, Acc@5=99.660 | Loss= 0.30686
Epoch 87/160 [learning_rate=0.004000] Val [Acc@1=91.140, Acc@5=99.710 | Loss= 0.29883

==>>[2022-08-16 07:13:46] [Epoch=087/160] [Need: 00:53:23] [learning_rate=0.0040] [Best : Acc@1=91.14, Error=8.86]
Epoch 88/160 [learning_rate=0.004000] Val [Acc@1=90.970, Acc@5=99.710 | Loss= 0.30941
Epoch 89/160 [learning_rate=0.004000] Val [Acc@1=91.000, Acc@5=99.670 | Loss= 0.32174
Epoch 90/160 [learning_rate=0.004000] Val [Acc@1=91.090, Acc@5=99.710 | Loss= 0.30624
Epoch 91/160 [learning_rate=0.004000] Val [Acc@1=90.790, Acc@5=99.640 | Loss= 0.32068
Epoch 92/160 [learning_rate=0.004000] Val [Acc@1=90.850, Acc@5=99.690 | Loss= 0.31705
Epoch 93/160 [learning_rate=0.004000] Val [Acc@1=91.190, Acc@5=99.680 | Loss= 0.31019

==>>[2022-08-16 07:18:10] [Epoch=093/160] [Need: 00:49:03] [learning_rate=0.0040] [Best : Acc@1=91.19, Error=8.81]
Epoch 94/160 [learning_rate=0.004000] Val [Acc@1=90.940, Acc@5=99.660 | Loss= 0.32443
Epoch 95/160 [learning_rate=0.004000] Val [Acc@1=90.770, Acc@5=99.680 | Loss= 0.32624
Epoch 96/160 [learning_rate=0.004000] Val [Acc@1=90.980, Acc@5=99.690 | Loss= 0.31910
Epoch 97/160 [learning_rate=0.004000] Val [Acc@1=90.660, Acc@5=99.670 | Loss= 0.33044
Epoch 98/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.690 | Loss= 0.32586
Epoch 99/160 [learning_rate=0.004000] Val [Acc@1=90.890, Acc@5=99.670 | Loss= 0.33136
Epoch 100/160 [learning_rate=0.004000] Val [Acc@1=91.250, Acc@5=99.690 | Loss= 0.31818

==>>[2022-08-16 07:23:16] [Epoch=100/160] [Need: 00:43:50] [learning_rate=0.0040] [Best : Acc@1=91.25, Error=8.75]
Epoch 101/160 [learning_rate=0.004000] Val [Acc@1=90.950, Acc@5=99.580 | Loss= 0.33548
Epoch 102/160 [learning_rate=0.004000] Val [Acc@1=90.920, Acc@5=99.630 | Loss= 0.33024
Epoch 103/160 [learning_rate=0.004000] Val [Acc@1=90.840, Acc@5=99.670 | Loss= 0.34081
Epoch 104/160 [learning_rate=0.004000] Val [Acc@1=90.770, Acc@5=99.700 | Loss= 0.33156
Epoch 105/160 [learning_rate=0.004000] Val [Acc@1=90.980, Acc@5=99.680 | Loss= 0.33470
Epoch 106/160 [learning_rate=0.004000] Val [Acc@1=91.130, Acc@5=99.610 | Loss= 0.33855
Epoch 107/160 [learning_rate=0.004000] Val [Acc@1=91.110, Acc@5=99.670 | Loss= 0.33237
Epoch 108/160 [learning_rate=0.004000] Val [Acc@1=91.080, Acc@5=99.630 | Loss= 0.32881
Epoch 109/160 [learning_rate=0.004000] Val [Acc@1=90.970, Acc@5=99.620 | Loss= 0.33497
Epoch 110/160 [learning_rate=0.004000] Val [Acc@1=90.620, Acc@5=99.650 | Loss= 0.35061
Epoch 111/160 [learning_rate=0.004000] Val [Acc@1=91.350, Acc@5=99.670 | Loss= 0.32697

==>>[2022-08-16 07:31:19] [Epoch=111/160] [Need: 00:35:49] [learning_rate=0.0040] [Best : Acc@1=91.35, Error=8.65]
Epoch 112/160 [learning_rate=0.004000] Val [Acc@1=90.760, Acc@5=99.650 | Loss= 0.33295
Epoch 113/160 [learning_rate=0.004000] Val [Acc@1=90.920, Acc@5=99.630 | Loss= 0.33858
Epoch 114/160 [learning_rate=0.004000] Val [Acc@1=90.620, Acc@5=99.630 | Loss= 0.34002
Epoch 115/160 [learning_rate=0.004000] Val [Acc@1=90.780, Acc@5=99.660 | Loss= 0.34603
Epoch 116/160 [learning_rate=0.004000] Val [Acc@1=90.800, Acc@5=99.650 | Loss= 0.34462
Epoch 117/160 [learning_rate=0.004000] Val [Acc@1=90.980, Acc@5=99.640 | Loss= 0.34093
Epoch 118/160 [learning_rate=0.004000] Val [Acc@1=90.920, Acc@5=99.670 | Loss= 0.34145
Epoch 119/160 [learning_rate=0.004000] Val [Acc@1=90.910, Acc@5=99.680 | Loss= 0.34394
Epoch 120/160 [learning_rate=0.000800] Val [Acc@1=90.990, Acc@5=99.660 | Loss= 0.33275
Epoch 121/160 [learning_rate=0.000800] Val [Acc@1=91.170, Acc@5=99.670 | Loss= 0.33040
Epoch 122/160 [learning_rate=0.000800] Val [Acc@1=91.110, Acc@5=99.670 | Loss= 0.33066
Epoch 123/160 [learning_rate=0.000800] Val [Acc@1=91.070, Acc@5=99.690 | Loss= 0.33378
Epoch 124/160 [learning_rate=0.000800] Val [Acc@1=91.190, Acc@5=99.660 | Loss= 0.33274
Epoch 125/160 [learning_rate=0.000800] Val [Acc@1=91.130, Acc@5=99.670 | Loss= 0.33017
Epoch 126/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.650 | Loss= 0.32989
Epoch 127/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.660 | Loss= 0.32940
Epoch 128/160 [learning_rate=0.000800] Val [Acc@1=91.330, Acc@5=99.670 | Loss= 0.33373
Epoch 129/160 [learning_rate=0.000800] Val [Acc@1=91.390, Acc@5=99.640 | Loss= 0.33396

==>>[2022-08-16 07:44:27] [Epoch=129/160] [Need: 00:22:39] [learning_rate=0.0008] [Best : Acc@1=91.39, Error=8.61]
Epoch 130/160 [learning_rate=0.000800] Val [Acc@1=91.260, Acc@5=99.640 | Loss= 0.33201
Epoch 131/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.660 | Loss= 0.33232
Epoch 132/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.650 | Loss= 0.33290
Epoch 133/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.640 | Loss= 0.33632
Epoch 134/160 [learning_rate=0.000800] Val [Acc@1=91.230, Acc@5=99.660 | Loss= 0.33155
Epoch 135/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.640 | Loss= 0.33332
Epoch 136/160 [learning_rate=0.000800] Val [Acc@1=91.400, Acc@5=99.630 | Loss= 0.33204

==>>[2022-08-16 07:49:34] [Epoch=136/160] [Need: 00:17:31] [learning_rate=0.0008] [Best : Acc@1=91.40, Error=8.60]
Epoch 137/160 [learning_rate=0.000800] Val [Acc@1=91.380, Acc@5=99.620 | Loss= 0.33092
Epoch 138/160 [learning_rate=0.000800] Val [Acc@1=91.550, Acc@5=99.640 | Loss= 0.32957

==>>[2022-08-16 07:51:03] [Epoch=138/160] [Need: 00:16:04] [learning_rate=0.0008] [Best : Acc@1=91.55, Error=8.45]
Epoch 139/160 [learning_rate=0.000800] Val [Acc@1=91.360, Acc@5=99.620 | Loss= 0.33376
Epoch 140/160 [learning_rate=0.000800] Val [Acc@1=91.220, Acc@5=99.630 | Loss= 0.33566
Epoch 141/160 [learning_rate=0.000800] Val [Acc@1=91.040, Acc@5=99.630 | Loss= 0.33893
Epoch 142/160 [learning_rate=0.000800] Val [Acc@1=91.160, Acc@5=99.610 | Loss= 0.33934
Epoch 143/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.640 | Loss= 0.33349
Epoch 144/160 [learning_rate=0.000800] Val [Acc@1=91.340, Acc@5=99.640 | Loss= 0.33547
Epoch 145/160 [learning_rate=0.000800] Val [Acc@1=91.190, Acc@5=99.660 | Loss= 0.33721
Epoch 146/160 [learning_rate=0.000800] Val [Acc@1=91.210, Acc@5=99.650 | Loss= 0.33699
Epoch 147/160 [learning_rate=0.000800] Val [Acc@1=91.240, Acc@5=99.630 | Loss= 0.33582
Epoch 148/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.640 | Loss= 0.33729
Epoch 149/160 [learning_rate=0.000800] Val [Acc@1=91.150, Acc@5=99.630 | Loss= 0.33945
Epoch 150/160 [learning_rate=0.000800] Val [Acc@1=91.260, Acc@5=99.610 | Loss= 0.33833
Epoch 151/160 [learning_rate=0.000800] Val [Acc@1=91.360, Acc@5=99.620 | Loss= 0.34044
Epoch 152/160 [learning_rate=0.000800] Val [Acc@1=91.270, Acc@5=99.660 | Loss= 0.33823
Epoch 153/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.640 | Loss= 0.33648
Epoch 154/160 [learning_rate=0.000800] Val [Acc@1=91.320, Acc@5=99.650 | Loss= 0.33302
Epoch 155/160 [learning_rate=0.000800] Val [Acc@1=91.280, Acc@5=99.660 | Loss= 0.33972
Epoch 156/160 [learning_rate=0.000800] Val [Acc@1=91.310, Acc@5=99.630 | Loss= 0.33764
Epoch 157/160 [learning_rate=0.000800] Val [Acc@1=91.070, Acc@5=99.650 | Loss= 0.33920
Epoch 158/160 [learning_rate=0.000800] Val [Acc@1=91.220, Acc@5=99.630 | Loss= 0.33930
Epoch 159/160 [learning_rate=0.000800] Val [Acc@1=91.110, Acc@5=99.640 | Loss= 0.33982
