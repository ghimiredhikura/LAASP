save path : C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522
{'data_path': './data/cifar.python', 'pretrain_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522/resnet110.epoch.60.pth.tar', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet110', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'prune_epoch': 60, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.522, 'recover_flop': 0.0, 'manualSeed': 7383, 'cuda': True, 'use_cuda': True}
Random Seed: 7383
python version : 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.12.0
cudnn  version : 8302
Pretrain path: C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522/resnet110.epoch.60.pth.tar
Pruned path: ./
=> creating model 'resnet110'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522/resnet110.epoch.60.pth.tar', pruned_path='./', dataset='cifar10', arch='resnet110', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet110.1.0.522', mode='prune', batch_size=256, verbose=False, total_epoches=200, prune_epoch=60, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.522, recover_flop=0.0, manualSeed=7383, cuda=True, use_cuda=True)
[Pruning Method: cos] Flop Reduction Rate: 0.004660/0.522000 [Pruned 4 filters from 80]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.890, Acc@5=99.820 | Loss= 0.24342

==>>[2022-08-25 04:07:09] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.89, Error=8.11]
[Pruning Method: l1norm] Flop Reduction Rate: 0.009611/0.522000 [Pruned 17 filters from 258]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.470, Acc@5=99.910 | Loss= 0.23700

==>>[2022-08-25 04:15:10] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.47, Error=7.53]
[Pruning Method: cos] Flop Reduction Rate: 0.014853/0.522000 [Pruned 9 filters from 169]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.740, Acc@5=99.870 | Loss= 0.24063

==>>[2022-08-25 04:23:11] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.74, Error=7.26]
[Pruning Method: l2norm] Flop Reduction Rate: 0.019804/0.522000 [Pruned 17 filters from 218]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.460, Acc@5=99.820 | Loss= 0.24609

==>>[2022-08-25 04:31:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.46, Error=7.54]
[Pruning Method: cos] Flop Reduction Rate: 0.025047/0.522000 [Pruned 9 filters from 154]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.150, Acc@5=99.850 | Loss= 0.25607

==>>[2022-08-25 04:39:05] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.15, Error=7.85]
[Pruning Method: l1norm] Flop Reduction Rate: 0.029998/0.522000 [Pruned 17 filters from 218]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.180, Acc@5=99.830 | Loss= 0.26262

==>>[2022-08-25 04:46:59] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.18, Error=7.82]
[Pruning Method: cos] Flop Reduction Rate: 0.035240/0.522000 [Pruned 9 filters from 169]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.020, Acc@5=99.850 | Loss= 0.27814

==>>[2022-08-25 04:54:53] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.02, Error=7.98]
[Pruning Method: l2norm] Flop Reduction Rate: 0.040483/0.522000 [Pruned 9 filters from 124]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=92.160, Acc@5=99.770 | Loss= 0.27841

==>>[2022-08-25 05:02:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=92.16, Error=7.84]
[Pruning Method: cos] Flop Reduction Rate: 0.045434/0.522000 [Pruned 17 filters from 218]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.800, Acc@5=99.780 | Loss= 0.28868

==>>[2022-08-25 05:10:37] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.80, Error=8.20]
[Pruning Method: eucl] Flop Reduction Rate: 0.050676/0.522000 [Pruned 9 filters from 174]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.390, Acc@5=99.710 | Loss= 0.31582

==>>[2022-08-25 05:18:23] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.39, Error=8.61]
[Pruning Method: cos] Flop Reduction Rate: 0.055919/0.522000 [Pruned 9 filters from 109]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.990, Acc@5=99.820 | Loss= 0.32044

==>>[2022-08-25 05:26:07] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.99, Error=9.01]
[Pruning Method: cos] Flop Reduction Rate: 0.060578/0.522000 [Pruned 4 filters from 10]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.450, Acc@5=99.670 | Loss= 0.33058

==>>[2022-08-25 05:33:48] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.45, Error=9.55]
[Pruning Method: eucl] Flop Reduction Rate: 0.065530/0.522000 [Pruned 17 filters from 203]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.370, Acc@5=99.840 | Loss= 0.31303

==>>[2022-08-25 05:41:29] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.37, Error=8.63]
[Pruning Method: cos] Flop Reduction Rate: 0.070772/0.522000 [Pruned 9 filters from 134]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.080, Acc@5=99.630 | Loss= 0.32779

==>>[2022-08-25 05:49:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.08, Error=8.92]
[Pruning Method: eucl] Flop Reduction Rate: 0.075577/0.522000 [Pruned 11 filters from 96]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.360, Acc@5=99.830 | Loss= 0.32187

==>>[2022-08-25 05:56:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.36, Error=8.64]
[Pruning Method: cos] Flop Reduction Rate: 0.080820/0.522000 [Pruned 9 filters from 164]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.660, Acc@5=99.730 | Loss= 0.33107

==>>[2022-08-25 06:04:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.66, Error=9.34]
[Pruning Method: eucl] Flop Reduction Rate: 0.085538/0.522000 [Pruned 1 filters from 275]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.430, Acc@5=99.720 | Loss= 0.34624

==>>[2022-08-25 06:11:55] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.43, Error=9.57]
[Pruning Method: cos] Flop Reduction Rate: 0.090412/0.522000 [Pruned 17 filters from 238]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.690, Acc@5=99.800 | Loss= 0.46630

==>>[2022-08-25 06:19:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.69, Error=12.31]
[Pruning Method: cos] Flop Reduction Rate: 0.099372/0.522000 [Pruned 1 filters from 136]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.830, Acc@5=99.660 | Loss= 0.33736

==>>[2022-08-25 06:26:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.83, Error=9.17]
[Pruning Method: l1norm] Flop Reduction Rate: 0.108333/0.522000 [Pruned 1 filters from 111]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.930, Acc@5=99.710 | Loss= 0.30972

==>>[2022-08-25 06:33:52] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.93, Error=9.07]
[Pruning Method: eucl] Flop Reduction Rate: 0.113206/0.522000 [Pruned 17 filters from 273]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.460, Acc@5=99.710 | Loss= 0.34895

==>>[2022-08-25 06:40:57] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.46, Error=9.54]
[Pruning Method: cos] Flop Reduction Rate: 0.118121/0.522000 [Pruned 9 filters from 149]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.740, Acc@5=99.580 | Loss= 0.46241

==>>[2022-08-25 06:48:02] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.74, Error=12.26]
[Pruning Method: cos] Flop Reduction Rate: 0.126918/0.522000 [Pruned 1 filters from 126]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.490, Acc@5=99.670 | Loss= 0.33371

==>>[2022-08-25 06:55:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.49, Error=9.51]
[Pruning Method: l1norm] Flop Reduction Rate: 0.131792/0.522000 [Pruned 17 filters from 263]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.460, Acc@5=99.640 | Loss= 0.37986

==>>[2022-08-25 07:02:12] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.46, Error=10.54]
[Pruning Method: cos] Flop Reduction Rate: 0.136542/0.522000 [Pruned 9 filters from 134]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.090, Acc@5=99.650 | Loss= 0.34341

==>>[2022-08-25 07:09:16] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.09, Error=9.91]
[Pruning Method: cos] Flop Reduction Rate: 0.141293/0.522000 [Pruned 9 filters from 144]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.350, Acc@5=99.530 | Loss= 0.39381

==>>[2022-08-25 07:16:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.35, Error=10.65]
[Pruning Method: eucl] Flop Reduction Rate: 0.146167/0.522000 [Pruned 17 filters from 273]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.610 | Loss= 0.40432

==>>[2022-08-25 07:23:22] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.05, Error=10.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.150918/0.522000 [Pruned 9 filters from 134]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.410, Acc@5=99.660 | Loss= 0.34166

==>>[2022-08-25 07:30:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.41, Error=9.59]
[Pruning Method: cos] Flop Reduction Rate: 0.155669/0.522000 [Pruned 9 filters from 149]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.050, Acc@5=99.630 | Loss= 0.34313

==>>[2022-08-25 07:37:23] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.05, Error=9.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.160420/0.522000 [Pruned 9 filters from 129]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.900, Acc@5=99.510 | Loss= 0.43984

==>>[2022-08-25 07:44:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.90, Error=12.10]
[Pruning Method: l1norm] Flop Reduction Rate: 0.181136/0.522000 [Pruned 1 filters from 1]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.370, Acc@5=99.620 | Loss= 0.36040

==>>[2022-08-25 07:51:20] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.37, Error=10.63]
[Pruning Method: cos] Flop Reduction Rate: 0.185887/0.522000 [Pruned 9 filters from 129]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.090, Acc@5=99.420 | Loss= 0.40384

==>>[2022-08-25 07:58:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.09, Error=10.91]
[Pruning Method: l1norm] Flop Reduction Rate: 0.190295/0.522000 [Pruned 1 filters from 210]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.500, Acc@5=99.600 | Loss= 0.36143

==>>[2022-08-25 08:06:12] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.50, Error=10.50]
[Pruning Method: cos] Flop Reduction Rate: 0.194664/0.522000 [Pruned 4 filters from 80]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.610 | Loss= 0.36754

==>>[2022-08-25 08:13:38] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.55, Error=10.45]
[Pruning Method: cos] Flop Reduction Rate: 0.199033/0.522000 [Pruned 4 filters from 80]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.980, Acc@5=99.660 | Loss= 0.40149

==>>[2022-08-25 08:21:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.98, Error=11.02]
[Pruning Method: cos] Flop Reduction Rate: 0.203783/0.522000 [Pruned 9 filters from 149]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.250, Acc@5=99.440 | Loss= 0.39538

==>>[2022-08-25 08:28:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.25, Error=10.75]
[Pruning Method: cos] Flop Reduction Rate: 0.208152/0.522000 [Pruned 4 filters from 65]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.810 | Loss= 0.36051

==>>[2022-08-25 08:35:40] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.42, Error=10.58]
[Pruning Method: cos] Flop Reduction Rate: 0.212903/0.522000 [Pruned 9 filters from 119]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.980, Acc@5=99.260 | Loss= 0.47467

==>>[2022-08-25 08:42:55] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.98, Error=13.02]
[Pruning Method: cos] Flop Reduction Rate: 0.217654/0.522000 [Pruned 9 filters from 129]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.930, Acc@5=99.480 | Loss= 0.39666

==>>[2022-08-25 08:50:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.93, Error=11.07]
[Pruning Method: cos] Flop Reduction Rate: 0.222022/0.522000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.760 | Loss= 0.38070

==>>[2022-08-25 08:57:17] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.87, Error=11.13]
[Pruning Method: cos] Flop Reduction Rate: 0.226391/0.522000 [Pruned 4 filters from 45]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.360, Acc@5=99.550 | Loss= 0.34421

==>>[2022-08-25 09:04:23] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.36, Error=9.64]
[Pruning Method: cos] Flop Reduction Rate: 0.231188/0.522000 [Pruned 17 filters from 213]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.910, Acc@5=99.640 | Loss= 0.38925

==>>[2022-08-25 09:11:27] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.91, Error=11.09]
[Pruning Method: cos] Flop Reduction Rate: 0.235984/0.522000 [Pruned 17 filters from 213]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.520, Acc@5=99.520 | Loss= 0.39289

==>>[2022-08-25 09:18:30] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.52, Error=10.48]
[Pruning Method: eucl] Flop Reduction Rate: 0.240735/0.522000 [Pruned 9 filters from 124]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.030, Acc@5=99.500 | Loss= 0.34546

==>>[2022-08-25 09:25:33] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.03, Error=9.97]
[Pruning Method: l1norm] Flop Reduction Rate: 0.245486/0.522000 [Pruned 9 filters from 159]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.290, Acc@5=99.300 | Loss= 0.41850

==>>[2022-08-25 09:32:36] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.29, Error=11.71]
[Pruning Method: cos] Flop Reduction Rate: 0.249854/0.522000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.460, Acc@5=99.410 | Loss= 0.38638

==>>[2022-08-25 09:39:38] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.46, Error=10.54]
[Pruning Method: cos] Flop Reduction Rate: 0.254651/0.522000 [Pruned 17 filters from 238]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.640, Acc@5=99.260 | Loss= 0.41862

==>>[2022-08-25 09:46:40] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.64, Error=11.36]
[Pruning Method: cos] Flop Reduction Rate: 0.259402/0.522000 [Pruned 9 filters from 114]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.470, Acc@5=99.620 | Loss= 0.36520

==>>[2022-08-25 09:53:41] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.47, Error=10.53]
[Pruning Method: cos] Flop Reduction Rate: 0.264198/0.522000 [Pruned 17 filters from 238]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.870, Acc@5=99.420 | Loss= 0.48092

==>>[2022-08-25 10:00:42] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.87, Error=13.13]
[Pruning Method: cos] Flop Reduction Rate: 0.268567/0.522000 [Pruned 4 filters from 70]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.430, Acc@5=99.660 | Loss= 0.38576

==>>[2022-08-25 10:07:39] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.43, Error=10.57]
[Pruning Method: cos] Flop Reduction Rate: 0.273318/0.522000 [Pruned 9 filters from 139]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.230, Acc@5=99.660 | Loss= 0.43450

==>>[2022-08-25 10:14:34] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.23, Error=11.77]
[Pruning Method: l1norm] Flop Reduction Rate: 0.277416/0.522000 [Pruned 1 filters from 192]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.830, Acc@5=99.610 | Loss= 0.36837

==>>[2022-08-25 10:21:27] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.83, Error=10.17]
[Pruning Method: eucl] Flop Reduction Rate: 0.282135/0.522000 [Pruned 17 filters from 203]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.610, Acc@5=99.520 | Loss= 0.43434

==>>[2022-08-25 10:28:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.61, Error=11.39]
[Pruning Method: cos] Flop Reduction Rate: 0.286886/0.522000 [Pruned 9 filters from 124]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.390, Acc@5=99.500 | Loss= 0.41117

==>>[2022-08-25 10:35:14] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.39, Error=11.61]
[Pruning Method: l1norm] Flop Reduction Rate: 0.290907/0.522000 [Pruned 1 filters from 260]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.630 | Loss= 0.38779

==>>[2022-08-25 10:42:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.90, Error=11.10]
[Pruning Method: cos] Flop Reduction Rate: 0.295658/0.522000 [Pruned 9 filters from 159]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.100, Acc@5=99.730 | Loss= 0.35952

==>>[2022-08-25 10:48:53] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.10, Error=9.90]
[Pruning Method: cos] Flop Reduction Rate: 0.300300/0.522000 [Pruned 17 filters from 258]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.080, Acc@5=99.660 | Loss= 0.37740

==>>[2022-08-25 10:55:42] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.08, Error=10.92]
[Pruning Method: cos] Flop Reduction Rate: 0.305050/0.522000 [Pruned 9 filters from 164]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.620 | Loss= 0.42722

==>>[2022-08-25 11:02:31] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.09, Error=11.91]
[Pruning Method: cos] Flop Reduction Rate: 0.309419/0.522000 [Pruned 4 filters from 65]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.530, Acc@5=99.670 | Loss= 0.34898

==>>[2022-08-25 11:09:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.53, Error=10.47]
[Pruning Method: l1norm] Flop Reduction Rate: 0.314061/0.522000 [Pruned 17 filters from 213]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=85.360, Acc@5=99.290 | Loss= 0.52037

==>>[2022-08-25 11:16:07] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.36, Error=14.64]
[Pruning Method: cos] Flop Reduction Rate: 0.318812/0.522000 [Pruned 9 filters from 179]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.260, Acc@5=99.550 | Loss= 0.50793

==>>[2022-08-25 11:22:51] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.26, Error=13.74]
[Pruning Method: l1norm] Flop Reduction Rate: 0.324822/0.522000 [Pruned 1 filters from 126]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.120, Acc@5=99.190 | Loss= 0.51502

==>>[2022-08-25 11:29:35] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.12, Error=13.88]
[Pruning Method: eucl] Flop Reduction Rate: 0.330831/0.522000 [Pruned 1 filters from 111]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.540, Acc@5=99.530 | Loss= 0.48962

==>>[2022-08-25 11:36:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.54, Error=13.46]
[Pruning Method: eucl] Flop Reduction Rate: 0.334697/0.522000 [Pruned 1 filters from 215]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.230, Acc@5=99.600 | Loss= 0.41699

==>>[2022-08-25 11:43:01] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.23, Error=11.77]
[Pruning Method: l1norm] Flop Reduction Rate: 0.338563/0.522000 [Pruned 1 filters from 230]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.290, Acc@5=99.480 | Loss= 0.46981

==>>[2022-08-25 11:49:43] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.29, Error=12.71]
[Pruning Method: l1norm] Flop Reduction Rate: 0.342429/0.522000 [Pruned 1 filters from 225]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.760, Acc@5=99.740 | Loss= 0.34027

==>>[2022-08-25 11:56:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.76, Error=10.24]
[Pruning Method: l1norm] Flop Reduction Rate: 0.346852/0.522000 [Pruned 9 filters from 154]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.800, Acc@5=99.630 | Loss= 0.36106

==>>[2022-08-25 12:03:06] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.80, Error=10.20]
[Pruning Method: eucl] Flop Reduction Rate: 0.351262/0.522000 [Pruned 17 filters from 248]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.630, Acc@5=99.720 | Loss= 0.35528

==>>[2022-08-25 12:09:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.63, Error=10.37]
[Pruning Method: l1norm] Flop Reduction Rate: 0.355671/0.522000 [Pruned 17 filters from 198]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.280, Acc@5=99.520 | Loss= 0.43948

==>>[2022-08-25 12:16:28] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.28, Error=11.72]
[Pruning Method: cos] Flop Reduction Rate: 0.360040/0.522000 [Pruned 4 filters from 50]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.410, Acc@5=99.350 | Loss= 0.52569

==>>[2022-08-25 12:23:09] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.41, Error=13.59]
[Pruning Method: l1norm] Flop Reduction Rate: 0.364463/0.522000 [Pruned 9 filters from 154]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.630, Acc@5=99.520 | Loss= 0.46653

==>>[2022-08-25 12:29:47] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.63, Error=13.37]
[Pruning Method: cos] Flop Reduction Rate: 0.368887/0.522000 [Pruned 9 filters from 179]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.990, Acc@5=99.640 | Loss= 0.35982

==>>[2022-08-25 12:36:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.99, Error=10.01]
[Pruning Method: cos] Flop Reduction Rate: 0.373255/0.522000 [Pruned 4 filters from 55]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.800, Acc@5=99.670 | Loss= 0.42017

==>>[2022-08-25 12:42:55] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.80, Error=12.20]
[Pruning Method: l2norm] Flop Reduction Rate: 0.377665/0.522000 [Pruned 17 filters from 258]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.360, Acc@5=99.200 | Loss= 0.47161

==>>[2022-08-25 12:49:25] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.36, Error=12.64]
[Pruning Method: l1norm] Flop Reduction Rate: 0.382088/0.522000 [Pruned 9 filters from 139]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.850, Acc@5=99.630 | Loss= 0.36977

==>>[2022-08-25 12:55:52] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.85, Error=10.15]
[Pruning Method: cos] Flop Reduction Rate: 0.386498/0.522000 [Pruned 17 filters from 268]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.390, Acc@5=99.500 | Loss= 0.48522

==>>[2022-08-25 13:02:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.39, Error=12.61]
[Pruning Method: cos] Flop Reduction Rate: 0.390054/0.522000 [Pruned 1 filters from 220]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.340, Acc@5=99.480 | Loss= 0.50439

==>>[2022-08-25 13:08:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.34, Error=13.66]
[Pruning Method: l1norm] Flop Reduction Rate: 0.395408/0.522000 [Pruned 1 filters from 156]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.760, Acc@5=99.350 | Loss= 0.42004

==>>[2022-08-25 13:15:12] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.76, Error=12.24]
[Pruning Method: l2norm] Flop Reduction Rate: 0.399667/0.522000 [Pruned 9 filters from 164]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.340, Acc@5=99.640 | Loss= 0.40926

==>>[2022-08-25 13:21:37] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.34, Error=11.66]
[Pruning Method: cos] Flop Reduction Rate: 0.404036/0.522000 [Pruned 4 filters from 70]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.030, Acc@5=99.540 | Loss= 0.43519

==>>[2022-08-25 13:27:58] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.03, Error=11.97]
[Pruning Method: l1norm] Flop Reduction Rate: 0.407592/0.522000 [Pruned 1 filters from 260]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.610, Acc@5=99.460 | Loss= 0.39555

==>>[2022-08-25 13:34:18] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.61, Error=11.39]
[Pruning Method: l1norm] Flop Reduction Rate: 0.411148/0.522000 [Pruned 1 filters from 220]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.370, Acc@5=99.440 | Loss= 0.39321

==>>[2022-08-25 13:40:39] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.37, Error=10.63]
[Pruning Method: l1norm] Flop Reduction Rate: 0.415325/0.522000 [Pruned 17 filters from 203]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.370, Acc@5=99.570 | Loss= 0.44493

==>>[2022-08-25 13:47:00] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.37, Error=12.63]
[Pruning Method: l2norm] Flop Reduction Rate: 0.419503/0.522000 [Pruned 17 filters from 198]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.320, Acc@5=99.540 | Loss= 0.37081

==>>[2022-08-25 13:53:16] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.32, Error=10.68]
[Pruning Method: cos] Flop Reduction Rate: 0.423872/0.522000 [Pruned 4 filters from 45]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.140, Acc@5=99.680 | Loss= 0.39373

==>>[2022-08-25 13:59:33] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.14, Error=10.86]
[Pruning Method: cos] Flop Reduction Rate: 0.428131/0.522000 [Pruned 9 filters from 174]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.290, Acc@5=99.610 | Loss= 0.36361

==>>[2022-08-25 14:05:49] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.29, Error=10.71]
[Pruning Method: l1norm] Flop Reduction Rate: 0.432308/0.522000 [Pruned 17 filters from 268]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.680, Acc@5=99.560 | Loss= 0.38479

==>>[2022-08-25 14:12:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.68, Error=11.32]
[Pruning Method: cos] Flop Reduction Rate: 0.436486/0.522000 [Pruned 17 filters from 208]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.960, Acc@5=99.600 | Loss= 0.38998

==>>[2022-08-25 14:18:20] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.96, Error=11.04]
[Pruning Method: l1norm] Flop Reduction Rate: 0.440855/0.522000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.950, Acc@5=99.660 | Loss= 0.39526

==>>[2022-08-25 14:24:36] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.95, Error=11.05]
[Pruning Method: cos] Flop Reduction Rate: 0.445223/0.522000 [Pruned 4 filters from 10]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.950, Acc@5=99.640 | Loss= 0.43481

==>>[2022-08-25 14:30:48] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.95, Error=12.05]
[Pruning Method: cos] Flop Reduction Rate: 0.449592/0.522000 [Pruned 4 filters from 70]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.690 | Loss= 0.40503

==>>[2022-08-25 14:36:58] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.87, Error=11.13]
[Pruning Method: eucl] Flop Reduction Rate: 0.453769/0.522000 [Pruned 17 filters from 228]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.330 | Loss= 0.41181

==>>[2022-08-25 14:43:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.87, Error=11.13]
[Pruning Method: cos] Flop Reduction Rate: 0.458029/0.522000 [Pruned 9 filters from 169]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.690 | Loss= 0.39472

==>>[2022-08-25 14:49:10] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.90, Error=11.10]
[Pruning Method: cos] Flop Reduction Rate: 0.462288/0.522000 [Pruned 9 filters from 184]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.380, Acc@5=99.540 | Loss= 0.41695

==>>[2022-08-25 14:55:14] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.38, Error=11.62]
[Pruning Method: cos] Flop Reduction Rate: 0.466657/0.522000 [Pruned 4 filters from 55]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.860, Acc@5=99.300 | Loss= 0.40753

==>>[2022-08-25 15:01:16] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.86, Error=11.14]
[Pruning Method: l1norm] Flop Reduction Rate: 0.470916/0.522000 [Pruned 9 filters from 184]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.450, Acc@5=99.370 | Loss= 0.43089

==>>[2022-08-25 15:07:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.45, Error=11.55]
[Pruning Method: l1norm] Flop Reduction Rate: 0.475176/0.522000 [Pruned 9 filters from 119]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.780, Acc@5=99.540 | Loss= 0.42813

==>>[2022-08-25 15:13:20] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.78, Error=11.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.479546/0.522000 [Pruned 1 filters from 98]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.080, Acc@5=99.370 | Loss= 0.48646

==>>[2022-08-25 15:19:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.08, Error=12.92]
[Pruning Method: eucl] Flop Reduction Rate: 0.483723/0.522000 [Pruned 17 filters from 278]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.900, Acc@5=99.380 | Loss= 0.43504

==>>[2022-08-25 15:25:22] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.90, Error=12.10]
[Pruning Method: l1norm] Flop Reduction Rate: 0.487819/0.522000 [Pruned 9 filters from 119]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.510, Acc@5=99.350 | Loss= 0.49239

==>>[2022-08-25 15:31:22] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.51, Error=13.49]
[Pruning Method: cos] Flop Reduction Rate: 0.491915/0.522000 [Pruned 9 filters from 184]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.680, Acc@5=99.610 | Loss= 0.34563

==>>[2022-08-25 15:37:19] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.68, Error=10.32]
[Pruning Method: eucl] Flop Reduction Rate: 0.496283/0.522000 [Pruned 4 filters from 10]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.760, Acc@5=99.560 | Loss= 0.39349

==>>[2022-08-25 15:43:13] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.76, Error=11.24]
[Pruning Method: cos] Flop Reduction Rate: 0.500461/0.522000 [Pruned 17 filters from 233]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.990, Acc@5=99.650 | Loss= 0.37969

==>>[2022-08-25 15:49:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.99, Error=11.01]
[Pruning Method: cos] Flop Reduction Rate: 0.504829/0.522000 [Pruned 4 filters from 50]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.520 | Loss= 0.42344

==>>[2022-08-25 15:54:54] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.62, Error=11.38]
[Pruning Method: eucl] Flop Reduction Rate: 0.509007/0.522000 [Pruned 17 filters from 268]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.760, Acc@5=99.410 | Loss= 0.44378

==>>[2022-08-25 16:00:43] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.76, Error=12.24]
[Pruning Method: cos] Flop Reduction Rate: 0.513184/0.522000 [Pruned 17 filters from 223]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.520 | Loss= 0.39357

==>>[2022-08-25 16:06:29] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.90, Error=11.10]
[Pruning Method: l2norm] Flop Reduction Rate: 0.517362/0.522000 [Pruned 17 filters from 248]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.820, Acc@5=99.080 | Loss= 0.43379

==>>[2022-08-25 16:12:15] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.82, Error=12.18]
[Pruning Method: cos] Flop Reduction Rate: 0.521458/0.522000 [Pruned 9 filters from 114]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.360, Acc@5=99.430 | Loss= 0.40755

==>>[2022-08-25 16:18:01] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.36, Error=11.64]
[Pruning Method: l1norm] Flop Reduction Rate: 0.525553/0.522000 [Pruned 9 filters from 179]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.780, Acc@5=99.460 | Loss= 0.45482

==>>[2022-08-25 16:23:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.78, Error=12.22]
Prune Stats: {'l1norm': 225, 'l2norm': 86, 'eucl': 172, 'cos': 496}
Final Flop Reduction Rate: 0.5256
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 20: 16, 22: 16, 25: 16, 27: 16, 30: 16, 32: 16, 35: 16, 37: 16, 40: 16, 42: 16, 45: 16, 47: 16, 50: 16, 52: 16, 55: 16, 57: 16, 60: 16, 62: 16, 65: 16, 67: 16, 70: 16, 72: 16, 75: 16, 77: 16, 80: 16, 82: 16, 85: 16, 87: 16, 90: 16, 92: 16, 96: 32, 98: 32, 101: 32, 104: 32, 106: 32, 109: 32, 111: 32, 114: 32, 116: 32, 119: 32, 121: 32, 124: 32, 126: 32, 129: 32, 131: 32, 134: 32, 136: 32, 139: 32, 141: 32, 144: 32, 146: 32, 149: 32, 151: 32, 154: 32, 156: 32, 159: 32, 161: 32, 164: 32, 166: 32, 169: 32, 171: 32, 174: 32, 176: 32, 179: 32, 181: 32, 184: 32, 186: 32, 190: 64, 192: 64, 195: 64, 198: 64, 200: 64, 203: 64, 205: 64, 208: 64, 210: 64, 213: 64, 215: 64, 218: 64, 220: 64, 223: 64, 225: 64, 228: 64, 230: 64, 233: 64, 235: 64, 238: 64, 240: 64, 243: 64, 245: 64, 248: 64, 250: 64, 253: 64, 255: 64, 258: 64, 260: 64, 263: 64, 265: 64, 268: 64, 270: 64, 273: 64, 275: 64, 278: 64, 280: 64}
Conv Filters After Pruning: {1: 15, 5: 16, 7: 15, 10: 4, 12: 15, 15: 16, 17: 15, 20: 16, 22: 15, 25: 16, 27: 15, 30: 16, 32: 15, 35: 16, 37: 15, 40: 4, 42: 15, 45: 8, 47: 15, 50: 8, 52: 15, 55: 8, 57: 15, 60: 16, 62: 15, 65: 8, 67: 15, 70: 4, 72: 15, 75: 16, 77: 15, 80: 4, 82: 15, 85: 16, 87: 15, 90: 16, 92: 15, 96: 21, 98: 25, 101: 25, 104: 32, 106: 25, 109: 23, 111: 25, 114: 14, 116: 25, 119: 5, 121: 25, 124: 5, 126: 25, 129: 5, 131: 25, 134: 5, 136: 25, 139: 14, 141: 25, 144: 23, 146: 25, 149: 5, 151: 25, 154: 5, 156: 25, 159: 14, 161: 25, 164: 5, 166: 25, 169: 5, 171: 25, 174: 14, 176: 25, 179: 5, 181: 25, 184: 5, 186: 25, 190: 64, 192: 54, 195: 54, 198: 30, 200: 54, 203: 13, 205: 54, 208: 47, 210: 54, 213: 13, 215: 54, 218: 13, 220: 54, 223: 47, 225: 54, 228: 47, 230: 54, 233: 47, 235: 54, 238: 13, 240: 54, 243: 64, 245: 54, 248: 30, 250: 54, 253: 64, 255: 54, 258: 13, 260: 54, 263: 47, 265: 54, 268: 13, 270: 54, 273: 30, 275: 54, 278: 47, 280: 54}
Layerwise Pruning Rate: {1: 0.0625, 5: 0.0, 7: 0.0625, 10: 0.75, 12: 0.0625, 15: 0.0, 17: 0.0625, 20: 0.0, 22: 0.0625, 25: 0.0, 27: 0.0625, 30: 0.0, 32: 0.0625, 35: 0.0, 37: 0.0625, 40: 0.75, 42: 0.0625, 45: 0.5, 47: 0.0625, 50: 0.5, 52: 0.0625, 55: 0.5, 57: 0.0625, 60: 0.0, 62: 0.0625, 65: 0.5, 67: 0.0625, 70: 0.75, 72: 0.0625, 75: 0.0, 77: 0.0625, 80: 0.75, 82: 0.0625, 85: 0.0, 87: 0.0625, 90: 0.0, 92: 0.0625, 96: 0.34375, 98: 0.21875, 101: 0.21875, 104: 0.0, 106: 0.21875, 109: 0.28125, 111: 0.21875, 114: 0.5625, 116: 0.21875, 119: 0.84375, 121: 0.21875, 124: 0.84375, 126: 0.21875, 129: 0.84375, 131: 0.21875, 134: 0.84375, 136: 0.21875, 139: 0.5625, 141: 0.21875, 144: 0.28125, 146: 0.21875, 149: 0.84375, 151: 0.21875, 154: 0.84375, 156: 0.21875, 159: 0.5625, 161: 0.21875, 164: 0.84375, 166: 0.21875, 169: 0.84375, 171: 0.21875, 174: 0.5625, 176: 0.21875, 179: 0.84375, 181: 0.21875, 184: 0.84375, 186: 0.21875, 190: 0.0, 192: 0.15625, 195: 0.15625, 198: 0.53125, 200: 0.15625, 203: 0.796875, 205: 0.15625, 208: 0.265625, 210: 0.15625, 213: 0.796875, 215: 0.15625, 218: 0.796875, 220: 0.15625, 223: 0.265625, 225: 0.15625, 228: 0.265625, 230: 0.15625, 233: 0.265625, 235: 0.15625, 238: 0.796875, 240: 0.15625, 243: 0.0, 245: 0.15625, 248: 0.53125, 250: 0.15625, 253: 0.0, 255: 0.15625, 258: 0.796875, 260: 0.15625, 263: 0.265625, 265: 0.15625, 268: 0.796875, 270: 0.15625, 273: 0.53125, 275: 0.15625, 278: 0.265625, 280: 0.15625}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(21, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 25, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(25, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(25, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(25, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(25, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(25, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(25, 54, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(54, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(30, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(54, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(30, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(54, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(54, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(30, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(54, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=54, out_features=10, bias=True)
)
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.830, Acc@5=99.700 | Loss= 0.38748

==>>[2022-08-25 16:24:41] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.83, Error=11.17]
Epoch 61/200 [learning_rate=0.020000] Val [Acc@1=88.860, Acc@5=99.450 | Loss= 0.41196

==>>[2022-08-25 16:25:36] [Epoch=061/200] [Need: 02:05:43] [learning_rate=0.0200] [Best : Acc@1=88.86, Error=11.14]
Epoch 62/200 [learning_rate=0.020000] Val [Acc@1=88.050, Acc@5=99.540 | Loss= 0.44469
Epoch 63/200 [learning_rate=0.020000] Val [Acc@1=88.180, Acc@5=99.510 | Loss= 0.42953
Epoch 64/200 [learning_rate=0.020000] Val [Acc@1=90.180, Acc@5=99.590 | Loss= 0.34255

==>>[2022-08-25 16:28:19] [Epoch=064/200] [Need: 02:03:13] [learning_rate=0.0200] [Best : Acc@1=90.18, Error=9.82]
Epoch 65/200 [learning_rate=0.020000] Val [Acc@1=89.060, Acc@5=99.680 | Loss= 0.38520
Epoch 66/200 [learning_rate=0.020000] Val [Acc@1=90.330, Acc@5=99.660 | Loss= 0.32929

==>>[2022-08-25 16:30:08] [Epoch=066/200] [Need: 02:01:37] [learning_rate=0.0200] [Best : Acc@1=90.33, Error=9.67]
Epoch 67/200 [learning_rate=0.020000] Val [Acc@1=88.890, Acc@5=99.570 | Loss= 0.39903
Epoch 68/200 [learning_rate=0.020000] Val [Acc@1=90.140, Acc@5=99.640 | Loss= 0.33777
Epoch 69/200 [learning_rate=0.020000] Val [Acc@1=86.860, Acc@5=99.370 | Loss= 0.47111
Epoch 70/200 [learning_rate=0.020000] Val [Acc@1=88.940, Acc@5=99.660 | Loss= 0.38341
Epoch 71/200 [learning_rate=0.020000] Val [Acc@1=89.320, Acc@5=99.710 | Loss= 0.37954
Epoch 72/200 [learning_rate=0.020000] Val [Acc@1=89.980, Acc@5=99.660 | Loss= 0.36065
Epoch 73/200 [learning_rate=0.020000] Val [Acc@1=89.650, Acc@5=99.550 | Loss= 0.37327
Epoch 74/200 [learning_rate=0.020000] Val [Acc@1=87.920, Acc@5=99.460 | Loss= 0.44836
Epoch 75/200 [learning_rate=0.020000] Val [Acc@1=89.190, Acc@5=99.640 | Loss= 0.38091
Epoch 76/200 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.670 | Loss= 0.39737
Epoch 77/200 [learning_rate=0.020000] Val [Acc@1=89.770, Acc@5=99.550 | Loss= 0.36482
Epoch 78/200 [learning_rate=0.020000] Val [Acc@1=87.620, Acc@5=99.520 | Loss= 0.45354
Epoch 79/200 [learning_rate=0.020000] Val [Acc@1=88.940, Acc@5=99.650 | Loss= 0.40330
Epoch 80/200 [learning_rate=0.020000] Val [Acc@1=90.000, Acc@5=99.720 | Loss= 0.36153
Epoch 81/200 [learning_rate=0.020000] Val [Acc@1=88.070, Acc@5=99.560 | Loss= 0.43018
Epoch 82/200 [learning_rate=0.020000] Val [Acc@1=89.690, Acc@5=99.400 | Loss= 0.39794
Epoch 83/200 [learning_rate=0.020000] Val [Acc@1=88.770, Acc@5=99.610 | Loss= 0.39700
Epoch 84/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.640 | Loss= 0.38179
Epoch 85/200 [learning_rate=0.020000] Val [Acc@1=90.220, Acc@5=99.640 | Loss= 0.35099
Epoch 86/200 [learning_rate=0.020000] Val [Acc@1=88.760, Acc@5=99.600 | Loss= 0.41430
Epoch 87/200 [learning_rate=0.020000] Val [Acc@1=90.470, Acc@5=99.600 | Loss= 0.33380

==>>[2022-08-25 16:49:14] [Epoch=087/200] [Need: 01:42:44] [learning_rate=0.0200] [Best : Acc@1=90.47, Error=9.53]
Epoch 88/200 [learning_rate=0.020000] Val [Acc@1=89.810, Acc@5=99.530 | Loss= 0.40555
Epoch 89/200 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.460 | Loss= 0.42953
Epoch 90/200 [learning_rate=0.020000] Val [Acc@1=89.430, Acc@5=99.540 | Loss= 0.37102
Epoch 91/200 [learning_rate=0.020000] Val [Acc@1=88.880, Acc@5=99.660 | Loss= 0.38702
Epoch 92/200 [learning_rate=0.020000] Val [Acc@1=88.230, Acc@5=99.260 | Loss= 0.43190
Epoch 93/200 [learning_rate=0.020000] Val [Acc@1=88.060, Acc@5=99.540 | Loss= 0.46340
Epoch 94/200 [learning_rate=0.020000] Val [Acc@1=89.910, Acc@5=99.540 | Loss= 0.35854
Epoch 95/200 [learning_rate=0.020000] Val [Acc@1=88.940, Acc@5=99.570 | Loss= 0.40153
Epoch 96/200 [learning_rate=0.020000] Val [Acc@1=89.850, Acc@5=99.630 | Loss= 0.36534
Epoch 97/200 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.730 | Loss= 0.43369
Epoch 98/200 [learning_rate=0.020000] Val [Acc@1=87.860, Acc@5=99.310 | Loss= 0.45654
Epoch 99/200 [learning_rate=0.020000] Val [Acc@1=88.090, Acc@5=99.650 | Loss= 0.43203
Epoch 100/200 [learning_rate=0.020000] Val [Acc@1=88.690, Acc@5=99.590 | Loss= 0.42467
Epoch 101/200 [learning_rate=0.020000] Val [Acc@1=90.490, Acc@5=99.560 | Loss= 0.35282

==>>[2022-08-25 17:01:58] [Epoch=101/200] [Need: 01:30:01] [learning_rate=0.0200] [Best : Acc@1=90.49, Error=9.51]
Epoch 102/200 [learning_rate=0.020000] Val [Acc@1=89.320, Acc@5=99.560 | Loss= 0.39331
Epoch 103/200 [learning_rate=0.020000] Val [Acc@1=88.970, Acc@5=99.580 | Loss= 0.40382
Epoch 104/200 [learning_rate=0.020000] Val [Acc@1=89.440, Acc@5=99.470 | Loss= 0.37763
Epoch 105/200 [learning_rate=0.020000] Val [Acc@1=89.120, Acc@5=99.430 | Loss= 0.41783
Epoch 106/200 [learning_rate=0.020000] Val [Acc@1=88.650, Acc@5=99.410 | Loss= 0.42122
Epoch 107/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.610 | Loss= 0.36779
Epoch 108/200 [learning_rate=0.020000] Val [Acc@1=88.760, Acc@5=99.630 | Loss= 0.41171
Epoch 109/200 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.540 | Loss= 0.42499
Epoch 110/200 [learning_rate=0.020000] Val [Acc@1=89.320, Acc@5=99.550 | Loss= 0.37447
Epoch 111/200 [learning_rate=0.020000] Val [Acc@1=89.000, Acc@5=99.420 | Loss= 0.41893
Epoch 112/200 [learning_rate=0.020000] Val [Acc@1=90.490, Acc@5=99.630 | Loss= 0.35302
Epoch 113/200 [learning_rate=0.020000] Val [Acc@1=83.950, Acc@5=98.530 | Loss= 0.66135
Epoch 114/200 [learning_rate=0.020000] Val [Acc@1=89.490, Acc@5=99.410 | Loss= 0.38632
Epoch 115/200 [learning_rate=0.020000] Val [Acc@1=89.270, Acc@5=99.480 | Loss= 0.38248
Epoch 116/200 [learning_rate=0.020000] Val [Acc@1=88.300, Acc@5=99.430 | Loss= 0.44293
Epoch 117/200 [learning_rate=0.020000] Val [Acc@1=89.870, Acc@5=99.530 | Loss= 0.38958
Epoch 118/200 [learning_rate=0.020000] Val [Acc@1=87.520, Acc@5=99.420 | Loss= 0.48573
Epoch 119/200 [learning_rate=0.020000] Val [Acc@1=86.210, Acc@5=99.350 | Loss= 0.53820
Epoch 120/200 [learning_rate=0.004000] Val [Acc@1=93.270, Acc@5=99.860 | Loss= 0.25663

==>>[2022-08-25 17:19:14] [Epoch=120/200] [Need: 01:12:44] [learning_rate=0.0040] [Best : Acc@1=93.27, Error=6.73]
Epoch 121/200 [learning_rate=0.004000] Val [Acc@1=93.550, Acc@5=99.830 | Loss= 0.24766

==>>[2022-08-25 17:20:09] [Epoch=121/200] [Need: 01:11:49] [learning_rate=0.0040] [Best : Acc@1=93.55, Error=6.45]
Epoch 122/200 [learning_rate=0.004000] Val [Acc@1=93.710, Acc@5=99.860 | Loss= 0.25218

==>>[2022-08-25 17:21:03] [Epoch=122/200] [Need: 01:10:54] [learning_rate=0.0040] [Best : Acc@1=93.71, Error=6.29]
Epoch 123/200 [learning_rate=0.004000] Val [Acc@1=93.680, Acc@5=99.820 | Loss= 0.25911
Epoch 124/200 [learning_rate=0.004000] Val [Acc@1=93.520, Acc@5=99.800 | Loss= 0.26143
Epoch 125/200 [learning_rate=0.004000] Val [Acc@1=93.560, Acc@5=99.830 | Loss= 0.25921
Epoch 126/200 [learning_rate=0.004000] Val [Acc@1=93.640, Acc@5=99.830 | Loss= 0.26089
Epoch 127/200 [learning_rate=0.004000] Val [Acc@1=93.680, Acc@5=99.830 | Loss= 0.26018
Epoch 128/200 [learning_rate=0.004000] Val [Acc@1=93.760, Acc@5=99.880 | Loss= 0.26344

==>>[2022-08-25 17:26:30] [Epoch=128/200] [Need: 01:05:27] [learning_rate=0.0040] [Best : Acc@1=93.76, Error=6.24]
Epoch 129/200 [learning_rate=0.004000] Val [Acc@1=93.650, Acc@5=99.820 | Loss= 0.26640
Epoch 130/200 [learning_rate=0.004000] Val [Acc@1=93.640, Acc@5=99.880 | Loss= 0.27091
Epoch 131/200 [learning_rate=0.004000] Val [Acc@1=93.690, Acc@5=99.850 | Loss= 0.27342
Epoch 132/200 [learning_rate=0.004000] Val [Acc@1=93.620, Acc@5=99.860 | Loss= 0.27386
Epoch 133/200 [learning_rate=0.004000] Val [Acc@1=93.750, Acc@5=99.850 | Loss= 0.27470
Epoch 134/200 [learning_rate=0.004000] Val [Acc@1=93.720, Acc@5=99.840 | Loss= 0.27828
Epoch 135/200 [learning_rate=0.004000] Val [Acc@1=93.640, Acc@5=99.870 | Loss= 0.27754
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=93.500, Acc@5=99.860 | Loss= 0.27710
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=93.740, Acc@5=99.840 | Loss= 0.28173
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=93.560, Acc@5=99.850 | Loss= 0.28319
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=93.650, Acc@5=99.880 | Loss= 0.27823
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=93.640, Acc@5=99.810 | Loss= 0.28183
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=93.740, Acc@5=99.830 | Loss= 0.28309
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=93.680, Acc@5=99.840 | Loss= 0.28720
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=93.780, Acc@5=99.830 | Loss= 0.28150

==>>[2022-08-25 17:40:09] [Epoch=143/200] [Need: 00:51:49] [learning_rate=0.0040] [Best : Acc@1=93.78, Error=6.22]
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=93.890, Acc@5=99.850 | Loss= 0.27558

==>>[2022-08-25 17:41:04] [Epoch=144/200] [Need: 00:50:54] [learning_rate=0.0040] [Best : Acc@1=93.89, Error=6.11]
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=93.660, Acc@5=99.790 | Loss= 0.28404
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=93.670, Acc@5=99.830 | Loss= 0.28470
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=93.780, Acc@5=99.830 | Loss= 0.28423
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=93.770, Acc@5=99.830 | Loss= 0.28200
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=93.690, Acc@5=99.820 | Loss= 0.28256
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=93.720, Acc@5=99.810 | Loss= 0.28556
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=93.680, Acc@5=99.820 | Loss= 0.29010
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=93.600, Acc@5=99.800 | Loss= 0.28619
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=93.760, Acc@5=99.780 | Loss= 0.28170
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=93.600, Acc@5=99.810 | Loss= 0.29100
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=93.860, Acc@5=99.810 | Loss= 0.28067
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=93.700, Acc@5=99.780 | Loss= 0.28682
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=93.750, Acc@5=99.770 | Loss= 0.27996
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=93.450, Acc@5=99.840 | Loss= 0.29357
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=93.460, Acc@5=99.830 | Loss= 0.29009
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=93.760, Acc@5=99.830 | Loss= 0.27889
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=93.700, Acc@5=99.800 | Loss= 0.28039
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=93.810, Acc@5=99.830 | Loss= 0.27914
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=93.830, Acc@5=99.830 | Loss= 0.27788
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=93.780, Acc@5=99.800 | Loss= 0.27914
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=93.770, Acc@5=99.820 | Loss= 0.28011
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=93.870, Acc@5=99.820 | Loss= 0.27724
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=93.840, Acc@5=99.820 | Loss= 0.27677
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=93.790, Acc@5=99.830 | Loss= 0.27644
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=93.690, Acc@5=99.870 | Loss= 0.28215
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=93.880, Acc@5=99.870 | Loss= 0.28363
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=93.810, Acc@5=99.800 | Loss= 0.28075
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=93.840, Acc@5=99.830 | Loss= 0.27884
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=93.790, Acc@5=99.850 | Loss= 0.27995
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.790, Acc@5=99.810 | Loss= 0.27935
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=93.780, Acc@5=99.830 | Loss= 0.27765
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=93.910, Acc@5=99.830 | Loss= 0.27819

==>>[2022-08-25 18:10:09] [Epoch=176/200] [Need: 00:21:49] [learning_rate=0.0008] [Best : Acc@1=93.91, Error=6.09]
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=93.930, Acc@5=99.810 | Loss= 0.27788

==>>[2022-08-25 18:11:03] [Epoch=177/200] [Need: 00:20:54] [learning_rate=0.0008] [Best : Acc@1=93.93, Error=6.07]
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=93.820, Acc@5=99.820 | Loss= 0.27591
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.890, Acc@5=99.820 | Loss= 0.27752
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=93.830, Acc@5=99.820 | Loss= 0.27652
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.870, Acc@5=99.810 | Loss= 0.27730
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.780, Acc@5=99.850 | Loss= 0.27796
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=93.810, Acc@5=99.820 | Loss= 0.28023
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=93.830, Acc@5=99.820 | Loss= 0.28137
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=93.920, Acc@5=99.800 | Loss= 0.27652
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=94.000, Acc@5=99.820 | Loss= 0.27692

==>>[2022-08-25 18:19:14] [Epoch=186/200] [Need: 00:12:43] [learning_rate=0.0008] [Best : Acc@1=94.00, Error=6.00]
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=93.800, Acc@5=99.800 | Loss= 0.27956
Epoch 188/200 [learning_rate=0.000800] Val [Acc@1=93.900, Acc@5=99.810 | Loss= 0.27888
Epoch 189/200 [learning_rate=0.000800] Val [Acc@1=93.890, Acc@5=99.840 | Loss= 0.27855
Epoch 190/200 [learning_rate=0.000160] Val [Acc@1=93.940, Acc@5=99.850 | Loss= 0.28062
Epoch 191/200 [learning_rate=0.000160] Val [Acc@1=93.900, Acc@5=99.860 | Loss= 0.27842
Epoch 192/200 [learning_rate=0.000160] Val [Acc@1=93.970, Acc@5=99.850 | Loss= 0.27977
Epoch 193/200 [learning_rate=0.000160] Val [Acc@1=93.990, Acc@5=99.840 | Loss= 0.27919
Epoch 194/200 [learning_rate=0.000160] Val [Acc@1=93.950, Acc@5=99.840 | Loss= 0.27839
Epoch 195/200 [learning_rate=0.000160] Val [Acc@1=93.950, Acc@5=99.830 | Loss= 0.28025
Epoch 196/200 [learning_rate=0.000160] Val [Acc@1=93.950, Acc@5=99.850 | Loss= 0.27885
Epoch 197/200 [learning_rate=0.000160] Val [Acc@1=93.960, Acc@5=99.860 | Loss= 0.27821
Epoch 198/200 [learning_rate=0.000160] Val [Acc@1=93.940, Acc@5=99.850 | Loss= 0.27789
Epoch 199/200 [learning_rate=0.000160] Val [Acc@1=93.850, Acc@5=99.830 | Loss= 0.27854
