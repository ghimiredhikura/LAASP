save path : C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet56.1.0.525
{'data_path': './data/cifar.python', 'pretrain_path': './', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet56', 'save_path': 'C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet56.1.0.525', 'mode': 'prune', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'prune_epoch': 60, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.525, 'recover_flop': 0.0, 'manualSeed': 8020, 'cuda': True, 'use_cuda': True}
Random Seed: 8020
python version : 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.10.2
cudnn  version : 8200
Pretrain path: ./
Pruned path: ./
=> creating model 'resnet56'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='./', pruned_path='./', dataset='cifar10', arch='resnet56', save_path='C:/Deepak/CIFAR10_PRUNE_OneShot/60.resnet56.1.0.525', mode='prune', batch_size=256, verbose=False, total_epoches=200, prune_epoch=60, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.525, recover_flop=0.0, manualSeed=8020, cuda=True, use_cuda=True)
Epoch 0/200 [learning_rate=0.100000] Val [Acc@1=33.200, Acc@5=85.760 | Loss= 1.78567

==>>[2022-08-27 19:42:42] [Epoch=000/200] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=33.20, Error=66.80]
Epoch 1/200 [learning_rate=0.100000] Val [Acc@1=40.520, Acc@5=90.320 | Loss= 1.63141

==>>[2022-08-27 19:43:29] [Epoch=001/200] [Need: 02:45:58] [learning_rate=0.1000] [Best : Acc@1=40.52, Error=59.48]
Epoch 2/200 [learning_rate=0.100000] Val [Acc@1=47.760, Acc@5=93.360 | Loss= 1.43707

==>>[2022-08-27 19:44:16] [Epoch=002/200] [Need: 02:40:00] [learning_rate=0.1000] [Best : Acc@1=47.76, Error=52.24]
Epoch 3/200 [learning_rate=0.100000] Val [Acc@1=50.520, Acc@5=92.410 | Loss= 1.40096

==>>[2022-08-27 19:45:03] [Epoch=003/200] [Need: 02:37:33] [learning_rate=0.1000] [Best : Acc@1=50.52, Error=49.48]
Epoch 4/200 [learning_rate=0.100000] Val [Acc@1=57.130, Acc@5=94.990 | Loss= 1.23049

==>>[2022-08-27 19:45:50] [Epoch=004/200] [Need: 02:36:06] [learning_rate=0.1000] [Best : Acc@1=57.13, Error=42.87]
Epoch 5/200 [learning_rate=0.100000] Val [Acc@1=60.910, Acc@5=96.350 | Loss= 1.14701

==>>[2022-08-27 19:46:37] [Epoch=005/200] [Need: 02:34:54] [learning_rate=0.1000] [Best : Acc@1=60.91, Error=39.09]
Epoch 6/200 [learning_rate=0.100000] Val [Acc@1=64.610, Acc@5=96.470 | Loss= 1.09470

==>>[2022-08-27 19:47:25] [Epoch=006/200] [Need: 02:33:45] [learning_rate=0.1000] [Best : Acc@1=64.61, Error=35.39]
Epoch 7/200 [learning_rate=0.100000] Val [Acc@1=67.500, Acc@5=96.900 | Loss= 0.99488

==>>[2022-08-27 19:48:11] [Epoch=007/200] [Need: 02:32:47] [learning_rate=0.1000] [Best : Acc@1=67.50, Error=32.50]
Epoch 8/200 [learning_rate=0.100000] Val [Acc@1=70.300, Acc@5=97.800 | Loss= 0.90671

==>>[2022-08-27 19:48:59] [Epoch=008/200] [Need: 02:31:43] [learning_rate=0.1000] [Best : Acc@1=70.30, Error=29.70]
Epoch 9/200 [learning_rate=0.100000] Val [Acc@1=66.960, Acc@5=93.920 | Loss= 1.13232
Epoch 10/200 [learning_rate=0.100000] Val [Acc@1=69.310, Acc@5=98.600 | Loss= 0.99135
Epoch 11/200 [learning_rate=0.100000] Val [Acc@1=67.440, Acc@5=96.830 | Loss= 1.04258
Epoch 12/200 [learning_rate=0.100000] Val [Acc@1=75.450, Acc@5=98.160 | Loss= 0.75570

==>>[2022-08-27 19:52:06] [Epoch=012/200] [Need: 02:28:06] [learning_rate=0.1000] [Best : Acc@1=75.45, Error=24.55]
Epoch 13/200 [learning_rate=0.100000] Val [Acc@1=78.800, Acc@5=98.510 | Loss= 0.64654

==>>[2022-08-27 19:52:53] [Epoch=013/200] [Need: 02:27:15] [learning_rate=0.1000] [Best : Acc@1=78.80, Error=21.20]
Epoch 14/200 [learning_rate=0.100000] Val [Acc@1=78.060, Acc@5=98.660 | Loss= 0.68164
Epoch 15/200 [learning_rate=0.100000] Val [Acc@1=66.880, Acc@5=97.450 | Loss= 1.24702
Epoch 16/200 [learning_rate=0.100000] Val [Acc@1=78.220, Acc@5=98.590 | Loss= 0.65399
Epoch 17/200 [learning_rate=0.100000] Val [Acc@1=76.230, Acc@5=97.840 | Loss= 0.71871
Epoch 18/200 [learning_rate=0.100000] Val [Acc@1=78.900, Acc@5=98.700 | Loss= 0.65411

==>>[2022-08-27 19:56:49] [Epoch=018/200] [Need: 02:23:13] [learning_rate=0.1000] [Best : Acc@1=78.90, Error=21.10]
Epoch 19/200 [learning_rate=0.100000] Val [Acc@1=82.140, Acc@5=98.910 | Loss= 0.55361

==>>[2022-08-27 19:57:36] [Epoch=019/200] [Need: 02:22:26] [learning_rate=0.1000] [Best : Acc@1=82.14, Error=17.86]
Epoch 20/200 [learning_rate=0.100000] Val [Acc@1=77.370, Acc@5=97.850 | Loss= 0.71799
Epoch 21/200 [learning_rate=0.100000] Val [Acc@1=79.250, Acc@5=98.440 | Loss= 0.64153
Epoch 22/200 [learning_rate=0.100000] Val [Acc@1=76.920, Acc@5=98.750 | Loss= 0.74427
Epoch 23/200 [learning_rate=0.100000] Val [Acc@1=75.570, Acc@5=97.570 | Loss= 0.79954
Epoch 24/200 [learning_rate=0.100000] Val [Acc@1=81.750, Acc@5=98.620 | Loss= 0.55694
Epoch 25/200 [learning_rate=0.100000] Val [Acc@1=75.960, Acc@5=98.830 | Loss= 0.78251
Epoch 26/200 [learning_rate=0.100000] Val [Acc@1=77.880, Acc@5=98.720 | Loss= 0.68282
Epoch 27/200 [learning_rate=0.100000] Val [Acc@1=82.970, Acc@5=99.300 | Loss= 0.50779

==>>[2022-08-27 20:03:51] [Epoch=027/200] [Need: 02:15:50] [learning_rate=0.1000] [Best : Acc@1=82.97, Error=17.03]
Epoch 28/200 [learning_rate=0.100000] Val [Acc@1=78.420, Acc@5=98.100 | Loss= 0.68750
Epoch 29/200 [learning_rate=0.100000] Val [Acc@1=73.360, Acc@5=97.840 | Loss= 0.89028
Epoch 30/200 [learning_rate=0.100000] Val [Acc@1=75.580, Acc@5=99.070 | Loss= 0.76974
Epoch 31/200 [learning_rate=0.100000] Val [Acc@1=78.820, Acc@5=98.370 | Loss= 0.67356
Epoch 32/200 [learning_rate=0.100000] Val [Acc@1=82.070, Acc@5=98.950 | Loss= 0.54796
Epoch 33/200 [learning_rate=0.100000] Val [Acc@1=75.130, Acc@5=98.580 | Loss= 0.81744
Epoch 34/200 [learning_rate=0.100000] Val [Acc@1=81.110, Acc@5=98.980 | Loss= 0.60655
Epoch 35/200 [learning_rate=0.100000] Val [Acc@1=78.140, Acc@5=98.240 | Loss= 0.69969
Epoch 36/200 [learning_rate=0.100000] Val [Acc@1=81.770, Acc@5=99.050 | Loss= 0.59887
Epoch 37/200 [learning_rate=0.100000] Val [Acc@1=80.170, Acc@5=99.040 | Loss= 0.64591
Epoch 38/200 [learning_rate=0.100000] Val [Acc@1=80.190, Acc@5=99.210 | Loss= 0.61741
Epoch 39/200 [learning_rate=0.100000] Val [Acc@1=83.090, Acc@5=99.040 | Loss= 0.52755

==>>[2022-08-27 20:13:15] [Epoch=039/200] [Need: 02:06:17] [learning_rate=0.1000] [Best : Acc@1=83.09, Error=16.91]
Epoch 40/200 [learning_rate=0.100000] Val [Acc@1=83.390, Acc@5=98.870 | Loss= 0.53319

==>>[2022-08-27 20:14:01] [Epoch=040/200] [Need: 02:05:30] [learning_rate=0.1000] [Best : Acc@1=83.39, Error=16.61]
Epoch 41/200 [learning_rate=0.100000] Val [Acc@1=81.210, Acc@5=99.110 | Loss= 0.59274
Epoch 42/200 [learning_rate=0.100000] Val [Acc@1=81.340, Acc@5=99.020 | Loss= 0.59578
Epoch 43/200 [learning_rate=0.100000] Val [Acc@1=82.120, Acc@5=98.990 | Loss= 0.56796
Epoch 44/200 [learning_rate=0.100000] Val [Acc@1=83.580, Acc@5=99.390 | Loss= 0.52126

==>>[2022-08-27 20:17:10] [Epoch=044/200] [Need: 02:02:21] [learning_rate=0.1000] [Best : Acc@1=83.58, Error=16.42]
Epoch 45/200 [learning_rate=0.100000] Val [Acc@1=82.510, Acc@5=99.060 | Loss= 0.55148
Epoch 46/200 [learning_rate=0.100000] Val [Acc@1=81.840, Acc@5=99.160 | Loss= 0.57493
Epoch 47/200 [learning_rate=0.100000] Val [Acc@1=80.220, Acc@5=98.780 | Loss= 0.63907
Epoch 48/200 [learning_rate=0.100000] Val [Acc@1=77.410, Acc@5=97.930 | Loss= 0.74731
Epoch 49/200 [learning_rate=0.100000] Val [Acc@1=80.480, Acc@5=98.800 | Loss= 0.60518
Epoch 50/200 [learning_rate=0.100000] Val [Acc@1=84.270, Acc@5=99.250 | Loss= 0.49108

==>>[2022-08-27 20:21:52] [Epoch=050/200] [Need: 01:57:36] [learning_rate=0.1000] [Best : Acc@1=84.27, Error=15.73]
Epoch 51/200 [learning_rate=0.100000] Val [Acc@1=82.270, Acc@5=99.020 | Loss= 0.56829
Epoch 52/200 [learning_rate=0.100000] Val [Acc@1=81.340, Acc@5=98.730 | Loss= 0.63701
Epoch 53/200 [learning_rate=0.100000] Val [Acc@1=75.620, Acc@5=98.160 | Loss= 0.87026
Epoch 54/200 [learning_rate=0.100000] Val [Acc@1=73.270, Acc@5=98.760 | Loss= 0.96614
Epoch 55/200 [learning_rate=0.100000] Val [Acc@1=84.280, Acc@5=99.200 | Loss= 0.48869

==>>[2022-08-27 20:25:47] [Epoch=055/200] [Need: 01:53:41] [learning_rate=0.1000] [Best : Acc@1=84.28, Error=15.72]
Epoch 56/200 [learning_rate=0.100000] Val [Acc@1=82.950, Acc@5=99.180 | Loss= 0.52311
Epoch 57/200 [learning_rate=0.100000] Val [Acc@1=84.770, Acc@5=99.400 | Loss= 0.47823

==>>[2022-08-27 20:27:21] [Epoch=057/200] [Need: 01:52:08] [learning_rate=0.1000] [Best : Acc@1=84.77, Error=15.23]
Epoch 58/200 [learning_rate=0.100000] Val [Acc@1=78.830, Acc@5=98.950 | Loss= 0.66298
Epoch 59/200 [learning_rate=0.100000] Val [Acc@1=77.730, Acc@5=99.230 | Loss= 0.72234
Val Acc@1: 77.730, Acc@5: 99.230,  Loss: 0.72234
[Pruning Method: l1norm] Flop Reduction Rate: 0.021979/0.525000 [Pruned 1 filters from 12]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.380, Acc@5=99.770 | Loss= 0.27188

==>>[2022-08-27 20:32:02] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.38, Error=8.62]
[Pruning Method: l2norm] Flop Reduction Rate: 0.032532/0.525000 [Pruned 9 filters from 94]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.460, Acc@5=99.820 | Loss= 0.27422

==>>[2022-08-27 20:34:29] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.46, Error=8.54]
[Pruning Method: l1norm] Flop Reduction Rate: 0.043086/0.525000 [Pruned 9 filters from 84]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.570, Acc@5=99.700 | Loss= 0.30408

==>>[2022-08-27 20:36:57] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.57, Error=9.43]
[Pruning Method: cos] Flop Reduction Rate: 0.051881/0.525000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.650, Acc@5=99.800 | Loss= 0.27436

==>>[2022-08-27 20:39:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.65, Error=8.35]
[Pruning Method: l1norm] Flop Reduction Rate: 0.062435/0.525000 [Pruned 9 filters from 74]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.130, Acc@5=99.740 | Loss= 0.30325

==>>[2022-08-27 20:41:50] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.13, Error=8.87]
[Pruning Method: l2norm] Flop Reduction Rate: 0.072988/0.525000 [Pruned 9 filters from 74]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.420, Acc@5=99.740 | Loss= 0.28965

==>>[2022-08-27 20:44:15] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.42, Error=8.58]
[Pruning Method: l1norm] Flop Reduction Rate: 0.081783/0.525000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.590, Acc@5=99.750 | Loss= 0.29032

==>>[2022-08-27 20:46:39] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.59, Error=8.41]
[Pruning Method: cos] Flop Reduction Rate: 0.090578/0.525000 [Pruned 4 filters from 20]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.410, Acc@5=99.780 | Loss= 0.29320

==>>[2022-08-27 20:49:03] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.41, Error=8.59]
[Pruning Method: cos] Flop Reduction Rate: 0.099372/0.525000 [Pruned 4 filters from 15]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.540, Acc@5=99.760 | Loss= 0.30291

==>>[2022-08-27 20:51:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.54, Error=8.46]
[Pruning Method: l1norm] Flop Reduction Rate: 0.109926/0.525000 [Pruned 9 filters from 64]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.970, Acc@5=99.760 | Loss= 0.30528

==>>[2022-08-27 20:53:44] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.97, Error=9.03]
[Pruning Method: l2norm] Flop Reduction Rate: 0.118721/0.525000 [Pruned 4 filters from 20]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.090, Acc@5=99.740 | Loss= 0.35198

==>>[2022-08-27 20:56:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.09, Error=9.91]
[Pruning Method: cos] Flop Reduction Rate: 0.127396/0.525000 [Pruned 1 filters from 91]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.680, Acc@5=99.720 | Loss= 0.31672

==>>[2022-08-27 20:58:24] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.68, Error=9.32]
[Pruning Method: cos] Flop Reduction Rate: 0.136190/0.525000 [Pruned 4 filters from 30]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.920, Acc@5=99.700 | Loss= 0.32244

==>>[2022-08-27 21:00:44] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.92, Error=9.08]
[Pruning Method: l1norm] Flop Reduction Rate: 0.144985/0.525000 [Pruned 4 filters from 15]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.920, Acc@5=99.660 | Loss= 0.32398

==>>[2022-08-27 21:03:02] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.92, Error=9.08]
[Pruning Method: cos] Flop Reduction Rate: 0.155209/0.525000 [Pruned 9 filters from 69]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.940, Acc@5=99.730 | Loss= 0.34034

==>>[2022-08-27 21:05:20] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.94, Error=9.06]
[Pruning Method: l1norm] Flop Reduction Rate: 0.165433/0.525000 [Pruned 9 filters from 69]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.750, Acc@5=99.700 | Loss= 0.40756

==>>[2022-08-27 21:07:37] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.75, Error=11.25]
[Pruning Method: l1norm] Flop Reduction Rate: 0.175432/0.525000 [Pruned 2 filters from 135]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.180, Acc@5=99.660 | Loss= 0.34943

==>>[2022-08-27 21:09:54] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.18, Error=9.82]
[Pruning Method: l1norm] Flop Reduction Rate: 0.185656/0.525000 [Pruned 9 filters from 89]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.220, Acc@5=99.740 | Loss= 0.34809

==>>[2022-08-27 21:12:09] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.22, Error=9.78]
[Pruning Method: l1norm] Flop Reduction Rate: 0.194451/0.525000 [Pruned 4 filters from 20]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.180, Acc@5=99.740 | Loss= 0.34157

==>>[2022-08-27 21:14:25] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.18, Error=9.82]
[Pruning Method: l1norm] Flop Reduction Rate: 0.204674/0.525000 [Pruned 9 filters from 84]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.560, Acc@5=99.630 | Loss= 0.34068

==>>[2022-08-27 21:16:38] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.56, Error=9.44]
[Pruning Method: cos] Flop Reduction Rate: 0.213469/0.525000 [Pruned 4 filters from 30]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.920, Acc@5=99.660 | Loss= 0.40354

==>>[2022-08-27 21:18:54] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.92, Error=11.08]
[Pruning Method: l2norm] Flop Reduction Rate: 0.223125/0.525000 [Pruned 17 filters from 143]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.670, Acc@5=99.800 | Loss= 0.33718

==>>[2022-08-27 21:21:06] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.67, Error=9.33]
[Pruning Method: l1norm] Flop Reduction Rate: 0.231920/0.525000 [Pruned 4 filters from 25]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.150, Acc@5=99.720 | Loss= 0.34606

==>>[2022-08-27 21:23:16] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.15, Error=9.85]
[Pruning Method: cos] Flop Reduction Rate: 0.240715/0.525000 [Pruned 4 filters from 30]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.540, Acc@5=99.620 | Loss= 0.36424

==>>[2022-08-27 21:25:26] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.54, Error=10.46]
[Pruning Method: cos] Flop Reduction Rate: 0.250938/0.525000 [Pruned 9 filters from 89]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.640 | Loss= 0.37540

==>>[2022-08-27 21:27:34] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.42, Error=10.58]
[Pruning Method: cos] Flop Reduction Rate: 0.261162/0.525000 [Pruned 9 filters from 79]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.220, Acc@5=99.410 | Loss= 0.52391

==>>[2022-08-27 21:29:40] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.22, Error=13.78]
[Pruning Method: cos] Flop Reduction Rate: 0.270850/0.525000 [Pruned 2 filters from 120]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.580, Acc@5=99.530 | Loss= 0.39853

==>>[2022-08-27 21:31:46] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.58, Error=11.42]
[Pruning Method: cos] Flop Reduction Rate: 0.279645/0.525000 [Pruned 4 filters from 45]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.610 | Loss= 0.38494

==>>[2022-08-27 21:33:52] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.42, Error=10.58]
[Pruning Method: l1norm] Flop Reduction Rate: 0.288439/0.525000 [Pruned 4 filters from 25]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.840, Acc@5=99.530 | Loss= 0.43950

==>>[2022-08-27 21:35:56] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.84, Error=12.16]
[Pruning Method: l1norm] Flop Reduction Rate: 0.295133/0.525000 [Pruned 1 filters from 53]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.780, Acc@5=99.600 | Loss= 0.41814

==>>[2022-08-27 21:38:00] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.78, Error=11.22]
[Pruning Method: cos] Flop Reduction Rate: 0.303928/0.525000 [Pruned 4 filters from 15]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.270, Acc@5=99.630 | Loss= 0.42334

==>>[2022-08-27 21:40:04] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.27, Error=11.73]
[Pruning Method: l1norm] Flop Reduction Rate: 0.313272/0.525000 [Pruned 17 filters from 123]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.810, Acc@5=99.650 | Loss= 0.35409

==>>[2022-08-27 21:42:06] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.81, Error=10.19]
[Pruning Method: l1norm] Flop Reduction Rate: 0.322067/0.525000 [Pruned 4 filters from 5]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.820, Acc@5=99.680 | Loss= 0.37990

==>>[2022-08-27 21:44:07] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.82, Error=11.18]
[Pruning Method: l1norm] Flop Reduction Rate: 0.328761/0.525000 [Pruned 1 filters from 86]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.400, Acc@5=99.500 | Loss= 0.42007

==>>[2022-08-27 21:46:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.40, Error=11.60]
[Pruning Method: cos] Flop Reduction Rate: 0.338325/0.525000 [Pruned 9 filters from 64]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.390 | Loss= 0.40728

==>>[2022-08-27 21:48:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.05, Error=10.95]
[Pruning Method: l1norm] Flop Reduction Rate: 0.347889/0.525000 [Pruned 9 filters from 94]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.040, Acc@5=99.580 | Loss= 0.38843

==>>[2022-08-27 21:50:08] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.04, Error=10.96]
[Pruning Method: l2norm] Flop Reduction Rate: 0.357234/0.525000 [Pruned 17 filters from 128]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.580, Acc@5=99.510 | Loss= 0.38475

==>>[2022-08-27 21:52:05] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.58, Error=11.42]
[Pruning Method: cos] Flop Reduction Rate: 0.366798/0.525000 [Pruned 9 filters from 59]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=85.730, Acc@5=99.510 | Loss= 0.51245

==>>[2022-08-27 21:54:03] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=85.73, Error=14.27]
[Pruning Method: eucl] Flop Reduction Rate: 0.376143/0.525000 [Pruned 17 filters from 143]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.650, Acc@5=99.550 | Loss= 0.40661

==>>[2022-08-27 21:56:00] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.65, Error=11.35]
[Pruning Method: l1norm] Flop Reduction Rate: 0.384937/0.525000 [Pruned 4 filters from 35]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.500, Acc@5=99.660 | Loss= 0.35702

==>>[2022-08-27 21:57:56] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.50, Error=10.50]
[Pruning Method: l1norm] Flop Reduction Rate: 0.394502/0.525000 [Pruned 9 filters from 79]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.820, Acc@5=99.750 | Loss= 0.39301

==>>[2022-08-27 21:59:53] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.82, Error=11.18]
[Pruning Method: l1norm] Flop Reduction Rate: 0.403296/0.525000 [Pruned 4 filters from 5]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.040, Acc@5=99.580 | Loss= 0.47081

==>>[2022-08-27 22:01:47] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.04, Error=12.96]
[Pruning Method: cos] Flop Reduction Rate: 0.412091/0.525000 [Pruned 4 filters from 35]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.920, Acc@5=99.530 | Loss= 0.42346

==>>[2022-08-27 22:03:41] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.92, Error=12.08]
[Pruning Method: cos] Flop Reduction Rate: 0.417466/0.525000 [Pruned 1 filters from 61]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.420, Acc@5=99.470 | Loss= 0.39445

==>>[2022-08-27 22:05:34] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.42, Error=11.58]
[Pruning Method: l1norm] Flop Reduction Rate: 0.426260/0.525000 [Pruned 4 filters from 45]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.630, Acc@5=99.740 | Loss= 0.37251

==>>[2022-08-27 22:07:28] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.63, Error=10.37]
[Pruning Method: l1norm] Flop Reduction Rate: 0.435055/0.525000 [Pruned 4 filters from 35]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.190, Acc@5=99.540 | Loss= 0.36186

==>>[2022-08-27 22:09:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.19, Error=10.81]
[Pruning Method: cos] Flop Reduction Rate: 0.443850/0.525000 [Pruned 4 filters from 40]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=86.950, Acc@5=99.460 | Loss= 0.48005

==>>[2022-08-27 22:11:12] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=86.95, Error=13.05]
[Pruning Method: cos] Flop Reduction Rate: 0.453194/0.525000 [Pruned 17 filters from 123]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.980, Acc@5=99.470 | Loss= 0.41567

==>>[2022-08-27 22:13:02] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.98, Error=12.02]
[Pruning Method: l2norm] Flop Reduction Rate: 0.461989/0.525000 [Pruned 4 filters from 25]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.890, Acc@5=99.580 | Loss= 0.39171

==>>[2022-08-27 22:14:51] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.89, Error=11.11]
[Pruning Method: l1norm] Flop Reduction Rate: 0.470784/0.525000 [Pruned 4 filters from 45]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.780, Acc@5=99.450 | Loss= 0.43729

==>>[2022-08-27 22:16:39] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.78, Error=12.22]
[Pruning Method: l1norm] Flop Reduction Rate: 0.480018/0.525000 [Pruned 9 filters from 59]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.610, Acc@5=99.610 | Loss= 0.37216

==>>[2022-08-27 22:18:25] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.61, Error=10.39]
[Pruning Method: l1norm] Flop Reduction Rate: 0.488813/0.525000 [Pruned 4 filters from 10]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.010, Acc@5=99.680 | Loss= 0.40926

==>>[2022-08-27 22:20:09] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.01, Error=10.99]
[Pruning Method: l2norm] Flop Reduction Rate: 0.497608/0.525000 [Pruned 4 filters from 10]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.500 | Loss= 0.40018

==>>[2022-08-27 22:21:53] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.90, Error=11.10]
[Pruning Method: l1norm] Flop Reduction Rate: 0.506952/0.525000 [Pruned 17 filters from 123]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.630 | Loss= 0.39192

==>>[2022-08-27 22:23:37] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.62, Error=11.38]
[Pruning Method: cos] Flop Reduction Rate: 0.516297/0.525000 [Pruned 17 filters from 118]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=88.970, Acc@5=99.660 | Loss= 0.37521

==>>[2022-08-27 22:25:21] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=88.97, Error=11.03]
[Pruning Method: eucl] Flop Reduction Rate: 0.525641/0.525000 [Pruned 17 filters from 128]
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=87.520, Acc@5=99.590 | Loss= 0.42571

==>>[2022-08-27 22:27:03] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=87.52, Error=12.48]
Prune Stats: {'l1norm': 168, 'l2norm': 64, 'eucl': 34, 'cos': 123}
Final Flop Reduction Rate: 0.5256
Conv Filters Before Pruning: {1: 16, 5: 16, 7: 16, 10: 16, 12: 16, 15: 16, 17: 16, 20: 16, 22: 16, 25: 16, 27: 16, 30: 16, 32: 16, 35: 16, 37: 16, 40: 16, 42: 16, 45: 16, 47: 16, 51: 32, 53: 32, 56: 32, 59: 32, 61: 32, 64: 32, 66: 32, 69: 32, 71: 32, 74: 32, 76: 32, 79: 32, 81: 32, 84: 32, 86: 32, 89: 32, 91: 32, 94: 32, 96: 32, 100: 64, 102: 64, 105: 64, 108: 64, 110: 64, 113: 64, 115: 64, 118: 64, 120: 64, 123: 64, 125: 64, 128: 64, 130: 64, 133: 64, 135: 64, 138: 64, 140: 64, 143: 64, 145: 64}
Conv Filters After Pruning: {1: 15, 5: 8, 7: 15, 10: 8, 12: 15, 15: 4, 17: 15, 20: 4, 22: 15, 25: 4, 27: 15, 30: 4, 32: 15, 35: 4, 37: 15, 40: 4, 42: 15, 45: 4, 47: 15, 51: 32, 53: 28, 56: 28, 59: 14, 61: 28, 64: 14, 66: 28, 69: 14, 71: 28, 74: 14, 76: 28, 79: 14, 81: 28, 84: 14, 86: 28, 89: 14, 91: 28, 94: 14, 96: 28, 100: 64, 102: 60, 105: 60, 108: 64, 110: 60, 113: 64, 115: 60, 118: 47, 120: 60, 123: 13, 125: 60, 128: 30, 130: 60, 133: 64, 135: 60, 138: 64, 140: 60, 143: 30, 145: 60}
Layerwise Pruning Rate: {1: 0.0625, 5: 0.5, 7: 0.0625, 10: 0.5, 12: 0.0625, 15: 0.75, 17: 0.0625, 20: 0.75, 22: 0.0625, 25: 0.75, 27: 0.0625, 30: 0.75, 32: 0.0625, 35: 0.75, 37: 0.0625, 40: 0.75, 42: 0.0625, 45: 0.75, 47: 0.0625, 51: 0.0, 53: 0.125, 56: 0.125, 59: 0.5625, 61: 0.125, 64: 0.5625, 66: 0.125, 69: 0.5625, 71: 0.125, 74: 0.5625, 76: 0.125, 79: 0.5625, 81: 0.125, 84: 0.5625, 86: 0.125, 89: 0.5625, 91: 0.125, 94: 0.5625, 96: 0.125, 100: 0.0, 102: 0.0625, 105: 0.0625, 108: 0.0, 110: 0.0625, 113: 0.0, 115: 0.0625, 118: 0.265625, 120: 0.0625, 123: 0.796875, 125: 0.0625, 128: 0.53125, 130: 0.0625, 133: 0.0, 135: 0.0625, 138: 0.0, 140: 0.0625, 143: 0.53125, 145: 0.0625}
=> Model [After Pruning]:
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(15, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(28, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 60, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(60, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(47, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(60, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(13, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(60, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(30, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(60, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(30, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=60, out_features=10, bias=True)
)
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=89.180, Acc@5=99.610 | Loss= 0.38574

==>>[2022-08-27 22:27:49] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=89.18, Error=10.82]
Epoch 61/200 [learning_rate=0.020000] Val [Acc@1=88.280, Acc@5=99.520 | Loss= 0.42410
Epoch 62/200 [learning_rate=0.020000] Val [Acc@1=88.210, Acc@5=99.280 | Loss= 0.43252
Epoch 63/200 [learning_rate=0.020000] Val [Acc@1=87.570, Acc@5=99.530 | Loss= 0.45431
Epoch 64/200 [learning_rate=0.020000] Val [Acc@1=88.550, Acc@5=99.590 | Loss= 0.38697
Epoch 65/200 [learning_rate=0.020000] Val [Acc@1=87.220, Acc@5=99.570 | Loss= 0.48271
Epoch 66/200 [learning_rate=0.020000] Val [Acc@1=86.230, Acc@5=99.170 | Loss= 0.50740
Epoch 67/200 [learning_rate=0.020000] Val [Acc@1=88.040, Acc@5=99.250 | Loss= 0.42150
Epoch 68/200 [learning_rate=0.020000] Val [Acc@1=88.420, Acc@5=99.580 | Loss= 0.41909
Epoch 69/200 [learning_rate=0.020000] Val [Acc@1=88.450, Acc@5=99.570 | Loss= 0.42710
Epoch 70/200 [learning_rate=0.020000] Val [Acc@1=87.640, Acc@5=99.360 | Loss= 0.45294
Epoch 71/200 [learning_rate=0.020000] Val [Acc@1=89.200, Acc@5=99.540 | Loss= 0.39833

==>>[2022-08-27 22:36:14] [Epoch=071/200] [Need: 01:38:48] [learning_rate=0.0200] [Best : Acc@1=89.20, Error=10.80]
Epoch 72/200 [learning_rate=0.020000] Val [Acc@1=88.000, Acc@5=99.470 | Loss= 0.44535
Epoch 73/200 [learning_rate=0.020000] Val [Acc@1=86.540, Acc@5=99.530 | Loss= 0.49823
Epoch 74/200 [learning_rate=0.020000] Val [Acc@1=88.680, Acc@5=99.550 | Loss= 0.42628
Epoch 75/200 [learning_rate=0.020000] Val [Acc@1=89.130, Acc@5=99.530 | Loss= 0.39188
Epoch 76/200 [learning_rate=0.020000] Val [Acc@1=89.690, Acc@5=99.540 | Loss= 0.37228

==>>[2022-08-27 22:40:04] [Epoch=076/200] [Need: 01:34:51] [learning_rate=0.0200] [Best : Acc@1=89.69, Error=10.31]
Epoch 77/200 [learning_rate=0.020000] Val [Acc@1=89.680, Acc@5=99.660 | Loss= 0.36581
Epoch 78/200 [learning_rate=0.020000] Val [Acc@1=88.110, Acc@5=99.640 | Loss= 0.43504
Epoch 79/200 [learning_rate=0.020000] Val [Acc@1=89.180, Acc@5=99.590 | Loss= 0.38033
Epoch 80/200 [learning_rate=0.020000] Val [Acc@1=88.370, Acc@5=99.530 | Loss= 0.42029
Epoch 81/200 [learning_rate=0.020000] Val [Acc@1=90.240, Acc@5=99.490 | Loss= 0.36240

==>>[2022-08-27 22:43:54] [Epoch=081/200] [Need: 01:31:03] [learning_rate=0.0200] [Best : Acc@1=90.24, Error=9.76]
Epoch 82/200 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.440 | Loss= 0.40299
Epoch 83/200 [learning_rate=0.020000] Val [Acc@1=87.550, Acc@5=99.510 | Loss= 0.46401
Epoch 84/200 [learning_rate=0.020000] Val [Acc@1=89.690, Acc@5=99.650 | Loss= 0.37347
Epoch 85/200 [learning_rate=0.020000] Val [Acc@1=85.880, Acc@5=99.480 | Loss= 0.55513
Epoch 86/200 [learning_rate=0.020000] Val [Acc@1=86.120, Acc@5=99.510 | Loss= 0.53208
Epoch 87/200 [learning_rate=0.020000] Val [Acc@1=89.610, Acc@5=99.590 | Loss= 0.37713
Epoch 88/200 [learning_rate=0.020000] Val [Acc@1=88.850, Acc@5=99.580 | Loss= 0.40628
Epoch 89/200 [learning_rate=0.020000] Val [Acc@1=89.980, Acc@5=99.600 | Loss= 0.35976
Epoch 90/200 [learning_rate=0.020000] Val [Acc@1=88.400, Acc@5=99.320 | Loss= 0.43815
Epoch 91/200 [learning_rate=0.020000] Val [Acc@1=89.510, Acc@5=99.660 | Loss= 0.37329
Epoch 92/200 [learning_rate=0.020000] Val [Acc@1=86.000, Acc@5=99.410 | Loss= 0.49969
Epoch 93/200 [learning_rate=0.020000] Val [Acc@1=89.390, Acc@5=99.600 | Loss= 0.38015
Epoch 94/200 [learning_rate=0.020000] Val [Acc@1=86.950, Acc@5=99.320 | Loss= 0.50290
Epoch 95/200 [learning_rate=0.020000] Val [Acc@1=88.380, Acc@5=99.520 | Loss= 0.43457
Epoch 96/200 [learning_rate=0.020000] Val [Acc@1=88.690, Acc@5=99.640 | Loss= 0.40584
Epoch 97/200 [learning_rate=0.020000] Val [Acc@1=88.350, Acc@5=99.680 | Loss= 0.42151
Epoch 98/200 [learning_rate=0.020000] Val [Acc@1=89.860, Acc@5=99.610 | Loss= 0.37366
Epoch 99/200 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.640 | Loss= 0.39472
Epoch 100/200 [learning_rate=0.020000] Val [Acc@1=90.330, Acc@5=99.550 | Loss= 0.35835

==>>[2022-08-27 22:58:28] [Epoch=100/200] [Need: 01:16:36] [learning_rate=0.0200] [Best : Acc@1=90.33, Error=9.67]
Epoch 101/200 [learning_rate=0.020000] Val [Acc@1=88.870, Acc@5=99.610 | Loss= 0.41178
Epoch 102/200 [learning_rate=0.020000] Val [Acc@1=88.170, Acc@5=99.480 | Loss= 0.43999
Epoch 103/200 [learning_rate=0.020000] Val [Acc@1=89.530, Acc@5=99.640 | Loss= 0.38112
Epoch 104/200 [learning_rate=0.020000] Val [Acc@1=88.840, Acc@5=99.550 | Loss= 0.40902
Epoch 105/200 [learning_rate=0.020000] Val [Acc@1=89.580, Acc@5=99.560 | Loss= 0.39183
Epoch 106/200 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.590 | Loss= 0.40333
Epoch 107/200 [learning_rate=0.020000] Val [Acc@1=88.610, Acc@5=99.580 | Loss= 0.44373
Epoch 108/200 [learning_rate=0.020000] Val [Acc@1=90.190, Acc@5=99.700 | Loss= 0.35190
Epoch 109/200 [learning_rate=0.020000] Val [Acc@1=89.400, Acc@5=99.650 | Loss= 0.38905
Epoch 110/200 [learning_rate=0.020000] Val [Acc@1=89.200, Acc@5=99.550 | Loss= 0.38677
Epoch 111/200 [learning_rate=0.020000] Val [Acc@1=88.210, Acc@5=99.610 | Loss= 0.43884
Epoch 112/200 [learning_rate=0.020000] Val [Acc@1=89.590, Acc@5=99.610 | Loss= 0.38704
Epoch 113/200 [learning_rate=0.020000] Val [Acc@1=88.640, Acc@5=99.560 | Loss= 0.41921
Epoch 114/200 [learning_rate=0.020000] Val [Acc@1=88.950, Acc@5=99.510 | Loss= 0.40517
Epoch 115/200 [learning_rate=0.020000] Val [Acc@1=88.120, Acc@5=99.640 | Loss= 0.45035
Epoch 116/200 [learning_rate=0.020000] Val [Acc@1=89.590, Acc@5=99.560 | Loss= 0.38023
Epoch 117/200 [learning_rate=0.020000] Val [Acc@1=89.560, Acc@5=99.630 | Loss= 0.38915
Epoch 118/200 [learning_rate=0.020000] Val [Acc@1=89.570, Acc@5=99.610 | Loss= 0.37040
Epoch 119/200 [learning_rate=0.020000] Val [Acc@1=88.220, Acc@5=99.570 | Loss= 0.43396
Epoch 120/200 [learning_rate=0.004000] Val [Acc@1=92.400, Acc@5=99.750 | Loss= 0.27926

==>>[2022-08-27 23:13:48] [Epoch=120/200] [Need: 01:01:19] [learning_rate=0.0040] [Best : Acc@1=92.40, Error=7.60]
Epoch 121/200 [learning_rate=0.004000] Val [Acc@1=92.520, Acc@5=99.760 | Loss= 0.27714

==>>[2022-08-27 23:14:34] [Epoch=121/200] [Need: 01:00:32] [learning_rate=0.0040] [Best : Acc@1=92.52, Error=7.48]
Epoch 122/200 [learning_rate=0.004000] Val [Acc@1=92.980, Acc@5=99.730 | Loss= 0.27573

==>>[2022-08-27 23:15:20] [Epoch=122/200] [Need: 00:59:46] [learning_rate=0.0040] [Best : Acc@1=92.98, Error=7.02]
Epoch 123/200 [learning_rate=0.004000] Val [Acc@1=92.900, Acc@5=99.770 | Loss= 0.26873
Epoch 124/200 [learning_rate=0.004000] Val [Acc@1=92.790, Acc@5=99.810 | Loss= 0.28196
Epoch 125/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.780 | Loss= 0.28023
Epoch 126/200 [learning_rate=0.004000] Val [Acc@1=92.550, Acc@5=99.770 | Loss= 0.28637
Epoch 127/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.760 | Loss= 0.28928
Epoch 128/200 [learning_rate=0.004000] Val [Acc@1=92.850, Acc@5=99.800 | Loss= 0.28412
Epoch 129/200 [learning_rate=0.004000] Val [Acc@1=93.000, Acc@5=99.770 | Loss= 0.28450

==>>[2022-08-27 23:20:42] [Epoch=129/200] [Need: 00:54:24] [learning_rate=0.0040] [Best : Acc@1=93.00, Error=7.00]
Epoch 130/200 [learning_rate=0.004000] Val [Acc@1=92.940, Acc@5=99.780 | Loss= 0.29168
Epoch 131/200 [learning_rate=0.004000] Val [Acc@1=92.890, Acc@5=99.760 | Loss= 0.29505
Epoch 132/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.770 | Loss= 0.28890
Epoch 133/200 [learning_rate=0.004000] Val [Acc@1=92.880, Acc@5=99.780 | Loss= 0.29551
Epoch 134/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.800 | Loss= 0.30228
Epoch 135/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.780 | Loss= 0.29721

==>>[2022-08-27 23:25:17] [Epoch=135/200] [Need: 00:49:48] [learning_rate=0.0040] [Best : Acc@1=93.17, Error=6.83]
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=93.200, Acc@5=99.730 | Loss= 0.29061

==>>[2022-08-27 23:26:03] [Epoch=136/200] [Need: 00:49:01] [learning_rate=0.0040] [Best : Acc@1=93.20, Error=6.80]
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.730 | Loss= 0.29371
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=93.010, Acc@5=99.780 | Loss= 0.29588
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=93.110, Acc@5=99.760 | Loss= 0.29900
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=93.090, Acc@5=99.730 | Loss= 0.29813
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.760 | Loss= 0.30422
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=93.190, Acc@5=99.800 | Loss= 0.30492
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=93.160, Acc@5=99.790 | Loss= 0.29229
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=93.110, Acc@5=99.800 | Loss= 0.29336
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=93.090, Acc@5=99.800 | Loss= 0.29557
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=92.780, Acc@5=99.740 | Loss= 0.30711
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=93.050, Acc@5=99.810 | Loss= 0.30246
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=93.010, Acc@5=99.780 | Loss= 0.29702
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.800 | Loss= 0.31637
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.790 | Loss= 0.31178
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=92.880, Acc@5=99.760 | Loss= 0.30796
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.750 | Loss= 0.31006
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=93.080, Acc@5=99.760 | Loss= 0.30346
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=92.780, Acc@5=99.780 | Loss= 0.32240
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=92.970, Acc@5=99.760 | Loss= 0.31373
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.730 | Loss= 0.31190
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=92.890, Acc@5=99.800 | Loss= 0.31320
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=92.990, Acc@5=99.800 | Loss= 0.30664
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=92.920, Acc@5=99.810 | Loss= 0.30934
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=93.020, Acc@5=99.820 | Loss= 0.30100
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=93.070, Acc@5=99.800 | Loss= 0.29999
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=93.150, Acc@5=99.790 | Loss= 0.30223
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=93.150, Acc@5=99.810 | Loss= 0.30267
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=93.220, Acc@5=99.810 | Loss= 0.30368

==>>[2022-08-27 23:47:26] [Epoch=164/200] [Need: 00:27:33] [learning_rate=0.0008] [Best : Acc@1=93.22, Error=6.78]
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=93.170, Acc@5=99.810 | Loss= 0.29889
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=93.420, Acc@5=99.800 | Loss= 0.29774

==>>[2022-08-27 23:48:58] [Epoch=166/200] [Need: 00:26:01] [learning_rate=0.0008] [Best : Acc@1=93.42, Error=6.58]
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=93.280, Acc@5=99.790 | Loss= 0.29904
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=93.200, Acc@5=99.840 | Loss= 0.29888
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=93.280, Acc@5=99.810 | Loss= 0.29775
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=93.320, Acc@5=99.790 | Loss= 0.29713
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=93.390, Acc@5=99.800 | Loss= 0.29742
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=93.290, Acc@5=99.780 | Loss= 0.29810
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=93.280, Acc@5=99.790 | Loss= 0.29471
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.350, Acc@5=99.780 | Loss= 0.29361
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=93.390, Acc@5=99.830 | Loss= 0.29739
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=93.240, Acc@5=99.810 | Loss= 0.29705
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=93.490, Acc@5=99.810 | Loss= 0.29863

==>>[2022-08-27 23:57:22] [Epoch=177/200] [Need: 00:17:36] [learning_rate=0.0008] [Best : Acc@1=93.49, Error=6.51]
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=93.200, Acc@5=99.790 | Loss= 0.29841
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.360, Acc@5=99.790 | Loss= 0.29716
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=93.240, Acc@5=99.760 | Loss= 0.29729
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.280, Acc@5=99.780 | Loss= 0.29843
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.300, Acc@5=99.820 | Loss= 0.29809
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=93.300, Acc@5=99.810 | Loss= 0.29834
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=93.220, Acc@5=99.810 | Loss= 0.29868
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=93.330, Acc@5=99.830 | Loss= 0.29962
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=93.470, Acc@5=99.840 | Loss= 0.29974
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=93.260, Acc@5=99.790 | Loss= 0.29885
Epoch 188/200 [learning_rate=0.000800] Val [Acc@1=93.300, Acc@5=99.780 | Loss= 0.29765
Epoch 189/200 [learning_rate=0.000800] Val [Acc@1=93.120, Acc@5=99.780 | Loss= 0.30000
Epoch 190/200 [learning_rate=0.000160] Val [Acc@1=93.130, Acc@5=99.800 | Loss= 0.29853
Epoch 191/200 [learning_rate=0.000160] Val [Acc@1=93.320, Acc@5=99.800 | Loss= 0.29927
Epoch 192/200 [learning_rate=0.000160] Val [Acc@1=93.320, Acc@5=99.810 | Loss= 0.29740
Epoch 193/200 [learning_rate=0.000160] Val [Acc@1=93.290, Acc@5=99.770 | Loss= 0.29762
Epoch 194/200 [learning_rate=0.000160] Val [Acc@1=93.310, Acc@5=99.810 | Loss= 0.29772
Epoch 195/200 [learning_rate=0.000160] Val [Acc@1=93.160, Acc@5=99.810 | Loss= 0.29827
Epoch 196/200 [learning_rate=0.000160] Val [Acc@1=93.410, Acc@5=99.820 | Loss= 0.29513
Epoch 197/200 [learning_rate=0.000160] Val [Acc@1=93.350, Acc@5=99.810 | Loss= 0.29698
Epoch 198/200 [learning_rate=0.000160] Val [Acc@1=93.250, Acc@5=99.800 | Loss= 0.29848
Epoch 199/200 [learning_rate=0.000160] Val [Acc@1=93.200, Acc@5=99.790 | Loss= 0.29675
