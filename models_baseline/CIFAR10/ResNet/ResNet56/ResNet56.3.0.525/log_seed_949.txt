save path : Baseline_CIFAR10/ResNet56/ResNet56.3.0.525
{'data_path': './data/cifar.python', 'pretrain_path': 'Baseline_CIFAR10/ResNet56/ResNet56.3.0.525/resnet56.epoch.60.pth.tar', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet56', 'save_path': 'Baseline_CIFAR10/ResNet56/ResNet56.3.0.525', 'mode': 'train', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'start_epoch': 60, 'prune_epoch': 30, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.342, 'recover_flop': 0.0, 'manualSeed': 949, 'cuda': True, 'use_cuda': True}
Random Seed: 949
python version : 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.12.0
cudnn  version : 8302
Pretrain path: Baseline_CIFAR10/ResNet56/ResNet56.3.0.525/resnet56.epoch.60.pth.tar
Pruned path: ./
=> creating model 'resnet56'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='Baseline_CIFAR10/ResNet56/ResNet56.3.0.525/resnet56.epoch.60.pth.tar', pruned_path='./', dataset='cifar10', arch='resnet56', save_path='Baseline_CIFAR10/ResNet56/ResNet56.3.0.525', mode='train', batch_size=256, verbose=False, total_epoches=200, start_epoch=60, prune_epoch=30, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.342, recover_flop=0.0, manualSeed=949, cuda=True, use_cuda=True)
=> train network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=90.980, Acc@5=99.810 | Loss= 0.27523

==>>[2022-08-28 14:31:29] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=90.98, Error=9.02]
Epoch 61/200 [learning_rate=0.020000] Val [Acc@1=91.760, Acc@5=99.820 | Loss= 0.26165

==>>[2022-08-28 14:32:19] [Epoch=061/200] [Need: 02:00:06] [learning_rate=0.0200] [Best : Acc@1=91.76, Error=8.24]
Epoch 62/200 [learning_rate=0.020000] Val [Acc@1=91.410, Acc@5=99.840 | Loss= 0.26834
Epoch 63/200 [learning_rate=0.020000] Val [Acc@1=91.700, Acc@5=99.790 | Loss= 0.26475
Epoch 64/200 [learning_rate=0.020000] Val [Acc@1=92.230, Acc@5=99.810 | Loss= 0.25943

==>>[2022-08-28 14:34:40] [Epoch=064/200] [Need: 01:49:06] [learning_rate=0.0200] [Best : Acc@1=92.23, Error=7.77]
Epoch 65/200 [learning_rate=0.020000] Val [Acc@1=91.450, Acc@5=99.790 | Loss= 0.28667
Epoch 66/200 [learning_rate=0.020000] Val [Acc@1=90.940, Acc@5=99.770 | Loss= 0.30607
Epoch 67/200 [learning_rate=0.020000] Val [Acc@1=91.630, Acc@5=99.780 | Loss= 0.28934
Epoch 68/200 [learning_rate=0.020000] Val [Acc@1=91.830, Acc@5=99.810 | Loss= 0.27806
Epoch 69/200 [learning_rate=0.020000] Val [Acc@1=91.300, Acc@5=99.790 | Loss= 0.31449
Epoch 70/200 [learning_rate=0.020000] Val [Acc@1=90.910, Acc@5=99.720 | Loss= 0.32773
Epoch 71/200 [learning_rate=0.020000] Val [Acc@1=91.590, Acc@5=99.790 | Loss= 0.29212
Epoch 72/200 [learning_rate=0.020000] Val [Acc@1=90.910, Acc@5=99.760 | Loss= 0.31850
Epoch 73/200 [learning_rate=0.020000] Val [Acc@1=90.950, Acc@5=99.690 | Loss= 0.32383
Epoch 74/200 [learning_rate=0.020000] Val [Acc@1=90.370, Acc@5=99.730 | Loss= 0.33622
Epoch 75/200 [learning_rate=0.020000] Val [Acc@1=91.080, Acc@5=99.640 | Loss= 0.31669
Epoch 76/200 [learning_rate=0.020000] Val [Acc@1=90.540, Acc@5=99.700 | Loss= 0.33339
Epoch 77/200 [learning_rate=0.020000] Val [Acc@1=90.600, Acc@5=99.760 | Loss= 0.33698
Epoch 78/200 [learning_rate=0.020000] Val [Acc@1=90.240, Acc@5=99.700 | Loss= 0.35223
Epoch 79/200 [learning_rate=0.020000] Val [Acc@1=90.440, Acc@5=99.720 | Loss= 0.34315
Epoch 80/200 [learning_rate=0.020000] Val [Acc@1=90.400, Acc@5=99.550 | Loss= 0.36032
Epoch 81/200 [learning_rate=0.020000] Val [Acc@1=90.620, Acc@5=99.660 | Loss= 0.33015
Epoch 82/200 [learning_rate=0.020000] Val [Acc@1=89.200, Acc@5=99.580 | Loss= 0.39926
Epoch 83/200 [learning_rate=0.020000] Val [Acc@1=89.240, Acc@5=99.600 | Loss= 0.40514
Epoch 84/200 [learning_rate=0.020000] Val [Acc@1=89.300, Acc@5=99.690 | Loss= 0.41646
Epoch 85/200 [learning_rate=0.020000] Val [Acc@1=89.720, Acc@5=99.670 | Loss= 0.37068
Epoch 86/200 [learning_rate=0.020000] Val [Acc@1=90.390, Acc@5=99.750 | Loss= 0.34445
Epoch 87/200 [learning_rate=0.020000] Val [Acc@1=90.310, Acc@5=99.650 | Loss= 0.35002
Epoch 88/200 [learning_rate=0.020000] Val [Acc@1=88.560, Acc@5=99.540 | Loss= 0.40908
Epoch 89/200 [learning_rate=0.020000] Val [Acc@1=90.320, Acc@5=99.610 | Loss= 0.34751
Epoch 90/200 [learning_rate=0.020000] Val [Acc@1=90.610, Acc@5=99.720 | Loss= 0.33594
Epoch 91/200 [learning_rate=0.020000] Val [Acc@1=88.390, Acc@5=99.590 | Loss= 0.44385
Epoch 92/200 [learning_rate=0.020000] Val [Acc@1=88.160, Acc@5=99.520 | Loss= 0.45062
Epoch 93/200 [learning_rate=0.020000] Val [Acc@1=88.680, Acc@5=99.570 | Loss= 0.42150
Epoch 94/200 [learning_rate=0.020000] Val [Acc@1=90.900, Acc@5=99.710 | Loss= 0.33280
Epoch 95/200 [learning_rate=0.020000] Val [Acc@1=89.430, Acc@5=99.520 | Loss= 0.38414
Epoch 96/200 [learning_rate=0.020000] Val [Acc@1=89.330, Acc@5=99.520 | Loss= 0.40433
Epoch 97/200 [learning_rate=0.020000] Val [Acc@1=89.680, Acc@5=99.500 | Loss= 0.38691
Epoch 98/200 [learning_rate=0.020000] Val [Acc@1=88.430, Acc@5=99.460 | Loss= 0.42274
Epoch 99/200 [learning_rate=0.020000] Val [Acc@1=88.560, Acc@5=99.430 | Loss= 0.43535
Epoch 100/200 [learning_rate=0.020000] Val [Acc@1=88.980, Acc@5=99.540 | Loss= 0.39940
Epoch 101/200 [learning_rate=0.020000] Val [Acc@1=90.230, Acc@5=99.690 | Loss= 0.34212
Epoch 102/200 [learning_rate=0.020000] Val [Acc@1=88.810, Acc@5=99.600 | Loss= 0.41416
Epoch 103/200 [learning_rate=0.020000] Val [Acc@1=90.010, Acc@5=99.600 | Loss= 0.36652
Epoch 104/200 [learning_rate=0.020000] Val [Acc@1=90.170, Acc@5=99.650 | Loss= 0.35656
Epoch 105/200 [learning_rate=0.020000] Val [Acc@1=88.990, Acc@5=99.620 | Loss= 0.40056
Epoch 106/200 [learning_rate=0.020000] Val [Acc@1=86.870, Acc@5=99.380 | Loss= 0.50919
Epoch 107/200 [learning_rate=0.020000] Val [Acc@1=89.070, Acc@5=99.600 | Loss= 0.40311
Epoch 108/200 [learning_rate=0.020000] Val [Acc@1=88.720, Acc@5=99.660 | Loss= 0.40875
Epoch 109/200 [learning_rate=0.020000] Val [Acc@1=88.970, Acc@5=99.510 | Loss= 0.43409
Epoch 110/200 [learning_rate=0.020000] Val [Acc@1=87.870, Acc@5=99.580 | Loss= 0.47612
Epoch 111/200 [learning_rate=0.020000] Val [Acc@1=88.760, Acc@5=99.600 | Loss= 0.42987
Epoch 112/200 [learning_rate=0.020000] Val [Acc@1=86.950, Acc@5=99.570 | Loss= 0.50163
Epoch 113/200 [learning_rate=0.020000] Val [Acc@1=89.060, Acc@5=99.680 | Loss= 0.37127
Epoch 114/200 [learning_rate=0.020000] Val [Acc@1=90.440, Acc@5=99.650 | Loss= 0.35694
Epoch 115/200 [learning_rate=0.020000] Val [Acc@1=88.910, Acc@5=99.620 | Loss= 0.40772
Epoch 116/200 [learning_rate=0.020000] Val [Acc@1=90.700, Acc@5=99.690 | Loss= 0.32306
Epoch 117/200 [learning_rate=0.020000] Val [Acc@1=89.780, Acc@5=99.680 | Loss= 0.36525
Epoch 118/200 [learning_rate=0.020000] Val [Acc@1=89.840, Acc@5=99.570 | Loss= 0.38904
Epoch 119/200 [learning_rate=0.020000] Val [Acc@1=88.080, Acc@5=99.460 | Loss= 0.44349
Epoch 120/200 [learning_rate=0.004000] Val [Acc@1=92.570, Acc@5=99.790 | Loss= 0.26486

==>>[2022-08-28 15:18:15] [Epoch=120/200] [Need: 01:02:24] [learning_rate=0.0040] [Best : Acc@1=92.57, Error=7.43]
Epoch 121/200 [learning_rate=0.004000] Val [Acc@1=92.920, Acc@5=99.770 | Loss= 0.26313

==>>[2022-08-28 15:19:02] [Epoch=121/200] [Need: 01:01:37] [learning_rate=0.0040] [Best : Acc@1=92.92, Error=7.08]
Epoch 122/200 [learning_rate=0.004000] Val [Acc@1=92.870, Acc@5=99.770 | Loss= 0.27524
Epoch 123/200 [learning_rate=0.004000] Val [Acc@1=93.110, Acc@5=99.790 | Loss= 0.27036

==>>[2022-08-28 15:20:35] [Epoch=123/200] [Need: 01:00:03] [learning_rate=0.0040] [Best : Acc@1=93.11, Error=6.89]
Epoch 124/200 [learning_rate=0.004000] Val [Acc@1=93.020, Acc@5=99.770 | Loss= 0.27933
Epoch 125/200 [learning_rate=0.004000] Val [Acc@1=93.010, Acc@5=99.760 | Loss= 0.27979
Epoch 126/200 [learning_rate=0.004000] Val [Acc@1=92.940, Acc@5=99.720 | Loss= 0.28954
Epoch 127/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.750 | Loss= 0.28174

==>>[2022-08-28 15:23:41] [Epoch=127/200] [Need: 00:56:55] [learning_rate=0.0040] [Best : Acc@1=93.17, Error=6.83]
Epoch 128/200 [learning_rate=0.004000] Val [Acc@1=93.070, Acc@5=99.750 | Loss= 0.28076
Epoch 129/200 [learning_rate=0.004000] Val [Acc@1=93.130, Acc@5=99.720 | Loss= 0.27752
Epoch 130/200 [learning_rate=0.004000] Val [Acc@1=93.140, Acc@5=99.730 | Loss= 0.28586
Epoch 131/200 [learning_rate=0.004000] Val [Acc@1=93.180, Acc@5=99.750 | Loss= 0.28465

==>>[2022-08-28 15:26:48] [Epoch=131/200] [Need: 00:53:47] [learning_rate=0.0040] [Best : Acc@1=93.18, Error=6.82]
Epoch 132/200 [learning_rate=0.004000] Val [Acc@1=93.140, Acc@5=99.780 | Loss= 0.29172
Epoch 133/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.740 | Loss= 0.28913
Epoch 134/200 [learning_rate=0.004000] Val [Acc@1=93.140, Acc@5=99.740 | Loss= 0.29689
Epoch 135/200 [learning_rate=0.004000] Val [Acc@1=93.330, Acc@5=99.760 | Loss= 0.29514

==>>[2022-08-28 15:29:54] [Epoch=135/200] [Need: 00:50:39] [learning_rate=0.0040] [Best : Acc@1=93.33, Error=6.67]
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=93.240, Acc@5=99.770 | Loss= 0.30304
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=93.370, Acc@5=99.750 | Loss= 0.29852

==>>[2022-08-28 15:31:28] [Epoch=137/200] [Need: 00:49:06] [learning_rate=0.0040] [Best : Acc@1=93.37, Error=6.63]
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=93.160, Acc@5=99.720 | Loss= 0.29822
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=93.340, Acc@5=99.750 | Loss= 0.29400
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=93.160, Acc@5=99.770 | Loss= 0.30151
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=93.440, Acc@5=99.760 | Loss= 0.29608

==>>[2022-08-28 15:34:35] [Epoch=141/200] [Need: 00:45:59] [learning_rate=0.0040] [Best : Acc@1=93.44, Error=6.56]
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=93.250, Acc@5=99.760 | Loss= 0.30248
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=93.380, Acc@5=99.740 | Loss= 0.29696
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=93.280, Acc@5=99.780 | Loss= 0.30429
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=93.390, Acc@5=99.730 | Loss= 0.30745
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=93.060, Acc@5=99.710 | Loss= 0.31166
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=92.980, Acc@5=99.730 | Loss= 0.31397
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=93.250, Acc@5=99.740 | Loss= 0.30085
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=93.400, Acc@5=99.750 | Loss= 0.30306
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=93.310, Acc@5=99.770 | Loss= 0.30117
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=93.160, Acc@5=99.740 | Loss= 0.31104
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=93.200, Acc@5=99.740 | Loss= 0.30288
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=93.300, Acc@5=99.730 | Loss= 0.30055
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=93.170, Acc@5=99.740 | Loss= 0.29970
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=93.180, Acc@5=99.740 | Loss= 0.31113
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=93.300, Acc@5=99.740 | Loss= 0.30705
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=93.200, Acc@5=99.730 | Loss= 0.31035
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=93.340, Acc@5=99.770 | Loss= 0.29784
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=93.310, Acc@5=99.730 | Loss= 0.31180
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=93.450, Acc@5=99.750 | Loss= 0.30175

==>>[2022-08-28 15:49:27] [Epoch=160/200] [Need: 00:31:11] [learning_rate=0.0008] [Best : Acc@1=93.45, Error=6.55]
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=93.490, Acc@5=99.760 | Loss= 0.30175

==>>[2022-08-28 15:50:13] [Epoch=161/200] [Need: 00:30:25] [learning_rate=0.0008] [Best : Acc@1=93.49, Error=6.51]
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=93.450, Acc@5=99.740 | Loss= 0.30470
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=93.460, Acc@5=99.730 | Loss= 0.30407
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=93.450, Acc@5=99.770 | Loss= 0.30122
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=93.380, Acc@5=99.770 | Loss= 0.30493
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=93.400, Acc@5=99.780 | Loss= 0.30231
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=93.330, Acc@5=99.740 | Loss= 0.30344
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=93.510, Acc@5=99.770 | Loss= 0.30393

==>>[2022-08-28 15:55:40] [Epoch=168/200] [Need: 00:24:57] [learning_rate=0.0008] [Best : Acc@1=93.51, Error=6.49]
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=93.480, Acc@5=99.790 | Loss= 0.30489
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=93.520, Acc@5=99.780 | Loss= 0.30059

==>>[2022-08-28 15:57:13] [Epoch=170/200] [Need: 00:23:23] [learning_rate=0.0008] [Best : Acc@1=93.52, Error=6.48]
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=93.510, Acc@5=99.780 | Loss= 0.30325
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=93.380, Acc@5=99.730 | Loss= 0.30292
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=93.530, Acc@5=99.750 | Loss= 0.29990

==>>[2022-08-28 15:59:34] [Epoch=173/200] [Need: 00:21:03] [learning_rate=0.0008] [Best : Acc@1=93.53, Error=6.47]
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.550, Acc@5=99.790 | Loss= 0.29822

==>>[2022-08-28 16:00:20] [Epoch=174/200] [Need: 00:20:16] [learning_rate=0.0008] [Best : Acc@1=93.55, Error=6.45]
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=93.560, Acc@5=99.770 | Loss= 0.30224

==>>[2022-08-28 16:01:07] [Epoch=175/200] [Need: 00:19:29] [learning_rate=0.0008] [Best : Acc@1=93.56, Error=6.44]
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=93.620, Acc@5=99.780 | Loss= 0.30054

==>>[2022-08-28 16:01:54] [Epoch=176/200] [Need: 00:18:42] [learning_rate=0.0008] [Best : Acc@1=93.62, Error=6.38]
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=93.610, Acc@5=99.760 | Loss= 0.29943
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=93.620, Acc@5=99.760 | Loss= 0.29939
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.620, Acc@5=99.740 | Loss= 0.29865
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=93.610, Acc@5=99.770 | Loss= 0.29863
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.610, Acc@5=99.750 | Loss= 0.30131
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.560, Acc@5=99.750 | Loss= 0.30146
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=93.650, Acc@5=99.780 | Loss= 0.29844

==>>[2022-08-28 16:07:21] [Epoch=183/200] [Need: 00:13:15] [learning_rate=0.0008] [Best : Acc@1=93.65, Error=6.35]
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=93.590, Acc@5=99.760 | Loss= 0.30091
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=93.560, Acc@5=99.760 | Loss= 0.30108
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=93.530, Acc@5=99.760 | Loss= 0.30068
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=93.530, Acc@5=99.780 | Loss= 0.29946
Epoch 188/200 [learning_rate=0.000800] Val [Acc@1=93.520, Acc@5=99.760 | Loss= 0.30239
Epoch 189/200 [learning_rate=0.000800] Val [Acc@1=93.530, Acc@5=99.740 | Loss= 0.30265
Epoch 190/200 [learning_rate=0.000160] Val [Acc@1=93.550, Acc@5=99.770 | Loss= 0.30144
Epoch 191/200 [learning_rate=0.000160] Val [Acc@1=93.390, Acc@5=99.760 | Loss= 0.29996
Epoch 192/200 [learning_rate=0.000160] Val [Acc@1=93.510, Acc@5=99.750 | Loss= 0.30119
Epoch 193/200 [learning_rate=0.000160] Val [Acc@1=93.610, Acc@5=99.780 | Loss= 0.30047
Epoch 194/200 [learning_rate=0.000160] Val [Acc@1=93.520, Acc@5=99.760 | Loss= 0.30076
Epoch 195/200 [learning_rate=0.000160] Val [Acc@1=93.500, Acc@5=99.750 | Loss= 0.29901
Epoch 196/200 [learning_rate=0.000160] Val [Acc@1=93.540, Acc@5=99.720 | Loss= 0.30055
Epoch 197/200 [learning_rate=0.000160] Val [Acc@1=93.460, Acc@5=99.760 | Loss= 0.30088
Epoch 198/200 [learning_rate=0.000160] Val [Acc@1=93.540, Acc@5=99.770 | Loss= 0.30030
Epoch 199/200 [learning_rate=0.000160] Val [Acc@1=93.510, Acc@5=99.770 | Loss= 0.30117
