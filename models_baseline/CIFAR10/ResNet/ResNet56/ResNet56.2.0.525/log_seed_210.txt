save path : Baseline_CIFAR10/ResNet56/ResNet56.2.0.525
{'data_path': './data/cifar.python', 'pretrain_path': 'Baseline_CIFAR10/ResNet56/ResNet56.2.0.525/resnet56.epoch.60.pth.tar', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet56', 'save_path': 'Baseline_CIFAR10/ResNet56/ResNet56.2.0.525', 'mode': 'train', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'start_epoch': 60, 'prune_epoch': 30, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.342, 'recover_flop': 0.0, 'manualSeed': 210, 'cuda': True, 'use_cuda': True}
Random Seed: 210
python version : 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.12.0
cudnn  version : 8302
Pretrain path: Baseline_CIFAR10/ResNet56/ResNet56.2.0.525/resnet56.epoch.60.pth.tar
Pruned path: ./
=> creating model 'resnet56'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='Baseline_CIFAR10/ResNet56/ResNet56.2.0.525/resnet56.epoch.60.pth.tar', pruned_path='./', dataset='cifar10', arch='resnet56', save_path='Baseline_CIFAR10/ResNet56/ResNet56.2.0.525', mode='train', batch_size=256, verbose=False, total_epoches=200, start_epoch=60, prune_epoch=30, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.342, recover_flop=0.0, manualSeed=210, cuda=True, use_cuda=True)
=> train network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.320, Acc@5=99.810 | Loss= 0.27045

==>>[2022-08-28 09:25:05] [Epoch=060/200] [Need: 00:00:00] [learning_rate=0.0200] [Best : Acc@1=91.32, Error=8.68]
Epoch 61/200 [learning_rate=0.020000] Val [Acc@1=91.810, Acc@5=99.710 | Loss= 0.25973

==>>[2022-08-28 09:25:55] [Epoch=061/200] [Need: 01:59:24] [learning_rate=0.0200] [Best : Acc@1=91.81, Error=8.19]
Epoch 62/200 [learning_rate=0.020000] Val [Acc@1=91.540, Acc@5=99.800 | Loss= 0.27027
Epoch 63/200 [learning_rate=0.020000] Val [Acc@1=91.840, Acc@5=99.800 | Loss= 0.27224

==>>[2022-08-28 09:27:28] [Epoch=063/200] [Need: 01:51:16] [learning_rate=0.0200] [Best : Acc@1=91.84, Error=8.16]
Epoch 64/200 [learning_rate=0.020000] Val [Acc@1=90.980, Acc@5=99.740 | Loss= 0.30718
Epoch 65/200 [learning_rate=0.020000] Val [Acc@1=91.430, Acc@5=99.770 | Loss= 0.29281
Epoch 66/200 [learning_rate=0.020000] Val [Acc@1=90.620, Acc@5=99.620 | Loss= 0.32625
Epoch 67/200 [learning_rate=0.020000] Val [Acc@1=91.420, Acc@5=99.730 | Loss= 0.29263
Epoch 68/200 [learning_rate=0.020000] Val [Acc@1=91.050, Acc@5=99.720 | Loss= 0.31943
Epoch 69/200 [learning_rate=0.020000] Val [Acc@1=90.780, Acc@5=99.780 | Loss= 0.31998
Epoch 70/200 [learning_rate=0.020000] Val [Acc@1=91.290, Acc@5=99.780 | Loss= 0.31106
Epoch 71/200 [learning_rate=0.020000] Val [Acc@1=90.570, Acc@5=99.760 | Loss= 0.33558
Epoch 72/200 [learning_rate=0.020000] Val [Acc@1=91.030, Acc@5=99.730 | Loss= 0.31959
Epoch 73/200 [learning_rate=0.020000] Val [Acc@1=90.850, Acc@5=99.750 | Loss= 0.32654
Epoch 74/200 [learning_rate=0.020000] Val [Acc@1=89.920, Acc@5=99.710 | Loss= 0.36602
Epoch 75/200 [learning_rate=0.020000] Val [Acc@1=90.390, Acc@5=99.730 | Loss= 0.35677
Epoch 76/200 [learning_rate=0.020000] Val [Acc@1=89.340, Acc@5=99.580 | Loss= 0.39570
Epoch 77/200 [learning_rate=0.020000] Val [Acc@1=90.880, Acc@5=99.740 | Loss= 0.34178
Epoch 78/200 [learning_rate=0.020000] Val [Acc@1=90.430, Acc@5=99.680 | Loss= 0.33770
Epoch 79/200 [learning_rate=0.020000] Val [Acc@1=90.790, Acc@5=99.760 | Loss= 0.32887
Epoch 80/200 [learning_rate=0.020000] Val [Acc@1=91.030, Acc@5=99.640 | Loss= 0.32291
Epoch 81/200 [learning_rate=0.020000] Val [Acc@1=90.450, Acc@5=99.610 | Loss= 0.34532
Epoch 82/200 [learning_rate=0.020000] Val [Acc@1=90.410, Acc@5=99.610 | Loss= 0.33775
Epoch 83/200 [learning_rate=0.020000] Val [Acc@1=89.960, Acc@5=99.510 | Loss= 0.35374
Epoch 84/200 [learning_rate=0.020000] Val [Acc@1=89.450, Acc@5=99.720 | Loss= 0.38797
Epoch 85/200 [learning_rate=0.020000] Val [Acc@1=89.770, Acc@5=99.560 | Loss= 0.39645
Epoch 86/200 [learning_rate=0.020000] Val [Acc@1=89.880, Acc@5=99.570 | Loss= 0.38187
Epoch 87/200 [learning_rate=0.020000] Val [Acc@1=90.500, Acc@5=99.560 | Loss= 0.33720
Epoch 88/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.650 | Loss= 0.39469
Epoch 89/200 [learning_rate=0.020000] Val [Acc@1=89.900, Acc@5=99.610 | Loss= 0.37695
Epoch 90/200 [learning_rate=0.020000] Val [Acc@1=88.330, Acc@5=99.300 | Loss= 0.42734
Epoch 91/200 [learning_rate=0.020000] Val [Acc@1=90.560, Acc@5=99.670 | Loss= 0.33905
Epoch 92/200 [learning_rate=0.020000] Val [Acc@1=90.620, Acc@5=99.750 | Loss= 0.34866
Epoch 93/200 [learning_rate=0.020000] Val [Acc@1=88.810, Acc@5=99.690 | Loss= 0.42124
Epoch 94/200 [learning_rate=0.020000] Val [Acc@1=89.420, Acc@5=99.620 | Loss= 0.39937
Epoch 95/200 [learning_rate=0.020000] Val [Acc@1=89.340, Acc@5=99.480 | Loss= 0.39970
Epoch 96/200 [learning_rate=0.020000] Val [Acc@1=88.570, Acc@5=99.600 | Loss= 0.42162
Epoch 97/200 [learning_rate=0.020000] Val [Acc@1=89.880, Acc@5=99.680 | Loss= 0.36776
Epoch 98/200 [learning_rate=0.020000] Val [Acc@1=88.790, Acc@5=99.520 | Loss= 0.41912
Epoch 99/200 [learning_rate=0.020000] Val [Acc@1=89.110, Acc@5=99.510 | Loss= 0.40781
Epoch 100/200 [learning_rate=0.020000] Val [Acc@1=88.610, Acc@5=99.580 | Loss= 0.40391
Epoch 101/200 [learning_rate=0.020000] Val [Acc@1=89.750, Acc@5=99.700 | Loss= 0.38855
Epoch 102/200 [learning_rate=0.020000] Val [Acc@1=89.350, Acc@5=99.670 | Loss= 0.37166
Epoch 103/200 [learning_rate=0.020000] Val [Acc@1=87.610, Acc@5=99.720 | Loss= 0.45421
Epoch 104/200 [learning_rate=0.020000] Val [Acc@1=88.440, Acc@5=99.650 | Loss= 0.44269
Epoch 105/200 [learning_rate=0.020000] Val [Acc@1=88.470, Acc@5=99.380 | Loss= 0.44742
Epoch 106/200 [learning_rate=0.020000] Val [Acc@1=89.720, Acc@5=99.710 | Loss= 0.37595
Epoch 107/200 [learning_rate=0.020000] Val [Acc@1=88.620, Acc@5=99.430 | Loss= 0.41873
Epoch 108/200 [learning_rate=0.020000] Val [Acc@1=87.320, Acc@5=99.430 | Loss= 0.48839
Epoch 109/200 [learning_rate=0.020000] Val [Acc@1=88.810, Acc@5=99.320 | Loss= 0.41946
Epoch 110/200 [learning_rate=0.020000] Val [Acc@1=89.580, Acc@5=99.640 | Loss= 0.38363
Epoch 111/200 [learning_rate=0.020000] Val [Acc@1=90.050, Acc@5=99.690 | Loss= 0.34473
Epoch 112/200 [learning_rate=0.020000] Val [Acc@1=87.340, Acc@5=99.350 | Loss= 0.48315
Epoch 113/200 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.510 | Loss= 0.39009
Epoch 114/200 [learning_rate=0.020000] Val [Acc@1=88.850, Acc@5=99.550 | Loss= 0.39811
Epoch 115/200 [learning_rate=0.020000] Val [Acc@1=85.970, Acc@5=99.490 | Loss= 0.58075
Epoch 116/200 [learning_rate=0.020000] Val [Acc@1=88.390, Acc@5=99.520 | Loss= 0.42377
Epoch 117/200 [learning_rate=0.020000] Val [Acc@1=89.050, Acc@5=99.670 | Loss= 0.39448
Epoch 118/200 [learning_rate=0.020000] Val [Acc@1=88.830, Acc@5=99.510 | Loss= 0.43937
Epoch 119/200 [learning_rate=0.020000] Val [Acc@1=89.440, Acc@5=99.530 | Loss= 0.39036
Epoch 120/200 [learning_rate=0.004000] Val [Acc@1=92.460, Acc@5=99.770 | Loss= 0.27110

==>>[2022-08-28 10:12:00] [Epoch=120/200] [Need: 01:02:36] [learning_rate=0.0040] [Best : Acc@1=92.46, Error=7.54]
Epoch 121/200 [learning_rate=0.004000] Val [Acc@1=92.820, Acc@5=99.780 | Loss= 0.27505

==>>[2022-08-28 10:12:47] [Epoch=121/200] [Need: 01:01:49] [learning_rate=0.0040] [Best : Acc@1=92.82, Error=7.18]
Epoch 122/200 [learning_rate=0.004000] Val [Acc@1=92.840, Acc@5=99.800 | Loss= 0.27767

==>>[2022-08-28 10:13:34] [Epoch=122/200] [Need: 01:01:02] [learning_rate=0.0040] [Best : Acc@1=92.84, Error=7.16]
Epoch 123/200 [learning_rate=0.004000] Val [Acc@1=92.940, Acc@5=99.780 | Loss= 0.28593

==>>[2022-08-28 10:14:21] [Epoch=123/200] [Need: 01:00:16] [learning_rate=0.0040] [Best : Acc@1=92.94, Error=7.06]
Epoch 124/200 [learning_rate=0.004000] Val [Acc@1=92.680, Acc@5=99.770 | Loss= 0.28730
Epoch 125/200 [learning_rate=0.004000] Val [Acc@1=92.770, Acc@5=99.780 | Loss= 0.29537
Epoch 126/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.750 | Loss= 0.29641
Epoch 127/200 [learning_rate=0.004000] Val [Acc@1=92.840, Acc@5=99.730 | Loss= 0.29667
Epoch 128/200 [learning_rate=0.004000] Val [Acc@1=92.970, Acc@5=99.760 | Loss= 0.29657

==>>[2022-08-28 10:18:15] [Epoch=128/200] [Need: 00:56:21] [learning_rate=0.0040] [Best : Acc@1=92.97, Error=7.03]
Epoch 129/200 [learning_rate=0.004000] Val [Acc@1=92.970, Acc@5=99.780 | Loss= 0.29852
Epoch 130/200 [learning_rate=0.004000] Val [Acc@1=92.840, Acc@5=99.780 | Loss= 0.29752
Epoch 131/200 [learning_rate=0.004000] Val [Acc@1=93.080, Acc@5=99.710 | Loss= 0.30193

==>>[2022-08-28 10:20:36] [Epoch=131/200] [Need: 00:54:00] [learning_rate=0.0040] [Best : Acc@1=93.08, Error=6.92]
Epoch 132/200 [learning_rate=0.004000] Val [Acc@1=93.030, Acc@5=99.710 | Loss= 0.29996
Epoch 133/200 [learning_rate=0.004000] Val [Acc@1=93.050, Acc@5=99.750 | Loss= 0.30507
Epoch 134/200 [learning_rate=0.004000] Val [Acc@1=93.020, Acc@5=99.740 | Loss= 0.30273
Epoch 135/200 [learning_rate=0.004000] Val [Acc@1=92.920, Acc@5=99.700 | Loss= 0.31072
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=93.130, Acc@5=99.750 | Loss= 0.30695

==>>[2022-08-28 10:24:31] [Epoch=136/200] [Need: 00:50:05] [learning_rate=0.0040] [Best : Acc@1=93.13, Error=6.87]
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=93.080, Acc@5=99.740 | Loss= 0.31039
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=93.050, Acc@5=99.770 | Loss= 0.31456
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=92.950, Acc@5=99.750 | Loss= 0.31449
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=92.990, Acc@5=99.730 | Loss= 0.31524
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=93.010, Acc@5=99.780 | Loss= 0.31852
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=93.090, Acc@5=99.710 | Loss= 0.30949
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.760 | Loss= 0.31935
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.690 | Loss= 0.31985
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=93.150, Acc@5=99.730 | Loss= 0.31138

==>>[2022-08-28 10:31:33] [Epoch=145/200] [Need: 00:43:02] [learning_rate=0.0040] [Best : Acc@1=93.15, Error=6.85]
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=93.090, Acc@5=99.700 | Loss= 0.31892
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=93.040, Acc@5=99.700 | Loss= 0.31894
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=93.020, Acc@5=99.740 | Loss= 0.31614
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=92.950, Acc@5=99.720 | Loss= 0.32050
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=93.070, Acc@5=99.730 | Loss= 0.31809
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=93.140, Acc@5=99.720 | Loss= 0.32038
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=93.060, Acc@5=99.730 | Loss= 0.31077
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=93.320, Acc@5=99.750 | Loss= 0.31210

==>>[2022-08-28 10:37:48] [Epoch=153/200] [Need: 00:36:46] [learning_rate=0.0040] [Best : Acc@1=93.32, Error=6.68]
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=93.090, Acc@5=99.750 | Loss= 0.32279
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=93.190, Acc@5=99.720 | Loss= 0.30922
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=93.150, Acc@5=99.760 | Loss= 0.31766
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=93.200, Acc@5=99.740 | Loss= 0.32001
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=93.250, Acc@5=99.680 | Loss= 0.31725
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=93.160, Acc@5=99.750 | Loss= 0.32608
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=93.370, Acc@5=99.710 | Loss= 0.31236

==>>[2022-08-28 10:43:15] [Epoch=160/200] [Need: 00:31:17] [learning_rate=0.0008] [Best : Acc@1=93.37, Error=6.63]
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=93.370, Acc@5=99.720 | Loss= 0.31190
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=93.250, Acc@5=99.730 | Loss= 0.31275
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=93.230, Acc@5=99.700 | Loss= 0.31347
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=93.300, Acc@5=99.690 | Loss= 0.31055
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=93.260, Acc@5=99.680 | Loss= 0.31449
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=93.270, Acc@5=99.700 | Loss= 0.31623
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=93.370, Acc@5=99.710 | Loss= 0.31390
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=93.250, Acc@5=99.720 | Loss= 0.31352
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=93.290, Acc@5=99.730 | Loss= 0.31213
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=93.320, Acc@5=99.700 | Loss= 0.31213
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=93.260, Acc@5=99.720 | Loss= 0.31316
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=93.340, Acc@5=99.740 | Loss= 0.30960
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=93.310, Acc@5=99.710 | Loss= 0.31115
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.330, Acc@5=99.720 | Loss= 0.31108
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=93.310, Acc@5=99.750 | Loss= 0.30930
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=93.380, Acc@5=99.730 | Loss= 0.31214

==>>[2022-08-28 10:55:43] [Epoch=176/200] [Need: 00:18:45] [learning_rate=0.0008] [Best : Acc@1=93.38, Error=6.62]
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=93.410, Acc@5=99.720 | Loss= 0.30886

==>>[2022-08-28 10:56:30] [Epoch=177/200] [Need: 00:17:58] [learning_rate=0.0008] [Best : Acc@1=93.41, Error=6.59]
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=93.360, Acc@5=99.720 | Loss= 0.31424
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.410, Acc@5=99.720 | Loss= 0.31339
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=93.330, Acc@5=99.710 | Loss= 0.31003
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.430, Acc@5=99.720 | Loss= 0.31365

==>>[2022-08-28 10:59:36] [Epoch=181/200] [Need: 00:14:51] [learning_rate=0.0008] [Best : Acc@1=93.43, Error=6.57]
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.430, Acc@5=99.730 | Loss= 0.31187
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=93.420, Acc@5=99.720 | Loss= 0.31171
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=93.360, Acc@5=99.720 | Loss= 0.31445
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=93.290, Acc@5=99.720 | Loss= 0.31131
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=93.330, Acc@5=99.700 | Loss= 0.31417
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=93.320, Acc@5=99.720 | Loss= 0.31190
