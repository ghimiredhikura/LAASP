save path : Baseline_CIFAR10/ResNet32/ResNet32.3.0.53
{'data_path': './data/cifar.python', 'pretrain_path': 'Baseline_CIFAR10/ResNet32/ResNet32.3.0.53/resnet32.epoch.50.pth.tar', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet32', 'save_path': 'Baseline_CIFAR10/ResNet32/ResNet32.3.0.53', 'mode': 'train', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'start_epoch': 50, 'prune_epoch': 30, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.342, 'recover_flop': 0.0, 'manualSeed': 1534, 'cuda': True, 'use_cuda': True}
Random Seed: 1534
python version : 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.12.0
cudnn  version : 8302
Pretrain path: Baseline_CIFAR10/ResNet32/ResNet32.3.0.53/resnet32.epoch.50.pth.tar
Pruned path: ./
=> creating model 'resnet32'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='Baseline_CIFAR10/ResNet32/ResNet32.3.0.53/resnet32.epoch.50.pth.tar', pruned_path='./', dataset='cifar10', arch='resnet32', save_path='Baseline_CIFAR10/ResNet32/ResNet32.3.0.53', mode='train', batch_size=256, verbose=False, total_epoches=200, start_epoch=50, prune_epoch=30, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.342, recover_flop=0.0, manualSeed=1534, cuda=True, use_cuda=True)
=> train network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 50/200 [learning_rate=0.100000] Val [Acc@1=80.230, Acc@5=98.580 | Loss= 0.65682

==>>[2022-08-27 15:21:38] [Epoch=050/200] [Need: 00:00:00] [learning_rate=0.1000] [Best : Acc@1=80.23, Error=19.77]
Epoch 51/200 [learning_rate=0.100000] Val [Acc@1=78.360, Acc@5=98.420 | Loss= 0.72088
Epoch 52/200 [learning_rate=0.100000] Val [Acc@1=82.460, Acc@5=98.880 | Loss= 0.52698

==>>[2022-08-27 15:23:05] [Epoch=052/200] [Need: 01:51:58] [learning_rate=0.1000] [Best : Acc@1=82.46, Error=17.54]
Epoch 53/200 [learning_rate=0.100000] Val [Acc@1=81.610, Acc@5=99.330 | Loss= 0.56886
Epoch 54/200 [learning_rate=0.100000] Val [Acc@1=76.800, Acc@5=97.730 | Loss= 0.77034
Epoch 55/200 [learning_rate=0.100000] Val [Acc@1=83.680, Acc@5=99.120 | Loss= 0.50469

==>>[2022-08-27 15:25:14] [Epoch=055/200] [Need: 01:45:49] [learning_rate=0.1000] [Best : Acc@1=83.68, Error=16.32]
Epoch 56/200 [learning_rate=0.100000] Val [Acc@1=80.720, Acc@5=98.980 | Loss= 0.62601
Epoch 57/200 [learning_rate=0.100000] Val [Acc@1=73.640, Acc@5=98.500 | Loss= 0.96323
Epoch 58/200 [learning_rate=0.100000] Val [Acc@1=81.760, Acc@5=99.180 | Loss= 0.56722
Epoch 59/200 [learning_rate=0.100000] Val [Acc@1=82.180, Acc@5=99.240 | Loss= 0.55518
Epoch 60/200 [learning_rate=0.020000] Val [Acc@1=91.210, Acc@5=99.780 | Loss= 0.26542

==>>[2022-08-27 15:28:47] [Epoch=060/200] [Need: 01:40:54] [learning_rate=0.0200] [Best : Acc@1=91.21, Error=8.79]
Epoch 61/200 [learning_rate=0.020000] Val [Acc@1=91.100, Acc@5=99.790 | Loss= 0.27307
Epoch 62/200 [learning_rate=0.020000] Val [Acc@1=91.640, Acc@5=99.830 | Loss= 0.25987

==>>[2022-08-27 15:30:13] [Epoch=062/200] [Need: 01:39:17] [learning_rate=0.0200] [Best : Acc@1=91.64, Error=8.36]
Epoch 63/200 [learning_rate=0.020000] Val [Acc@1=91.550, Acc@5=99.730 | Loss= 0.26923
Epoch 64/200 [learning_rate=0.020000] Val [Acc@1=91.310, Acc@5=99.670 | Loss= 0.28107
Epoch 65/200 [learning_rate=0.020000] Val [Acc@1=91.170, Acc@5=99.670 | Loss= 0.29402
Epoch 66/200 [learning_rate=0.020000] Val [Acc@1=91.860, Acc@5=99.780 | Loss= 0.26366

==>>[2022-08-27 15:33:04] [Epoch=066/200] [Need: 01:36:11] [learning_rate=0.0200] [Best : Acc@1=91.86, Error=8.14]
Epoch 67/200 [learning_rate=0.020000] Val [Acc@1=91.400, Acc@5=99.780 | Loss= 0.27291
Epoch 68/200 [learning_rate=0.020000] Val [Acc@1=90.510, Acc@5=99.690 | Loss= 0.34362
Epoch 69/200 [learning_rate=0.020000] Val [Acc@1=90.420, Acc@5=99.700 | Loss= 0.31718
Epoch 70/200 [learning_rate=0.020000] Val [Acc@1=90.280, Acc@5=99.660 | Loss= 0.32217
Epoch 71/200 [learning_rate=0.020000] Val [Acc@1=90.160, Acc@5=99.580 | Loss= 0.33648
Epoch 72/200 [learning_rate=0.020000] Val [Acc@1=89.910, Acc@5=99.670 | Loss= 0.35420
Epoch 73/200 [learning_rate=0.020000] Val [Acc@1=90.880, Acc@5=99.690 | Loss= 0.31176
Epoch 74/200 [learning_rate=0.020000] Val [Acc@1=89.060, Acc@5=99.580 | Loss= 0.40353
Epoch 75/200 [learning_rate=0.020000] Val [Acc@1=89.850, Acc@5=99.690 | Loss= 0.35348
Epoch 76/200 [learning_rate=0.020000] Val [Acc@1=90.490, Acc@5=99.740 | Loss= 0.32756
Epoch 77/200 [learning_rate=0.020000] Val [Acc@1=89.560, Acc@5=99.720 | Loss= 0.35023
Epoch 78/200 [learning_rate=0.020000] Val [Acc@1=90.060, Acc@5=99.680 | Loss= 0.34165
Epoch 79/200 [learning_rate=0.020000] Val [Acc@1=89.590, Acc@5=99.670 | Loss= 0.36743
Epoch 80/200 [learning_rate=0.020000] Val [Acc@1=89.030, Acc@5=99.500 | Loss= 0.40032
Epoch 81/200 [learning_rate=0.020000] Val [Acc@1=90.210, Acc@5=99.680 | Loss= 0.34050
Epoch 82/200 [learning_rate=0.020000] Val [Acc@1=90.030, Acc@5=99.680 | Loss= 0.35289
Epoch 83/200 [learning_rate=0.020000] Val [Acc@1=87.870, Acc@5=99.700 | Loss= 0.44481
Epoch 84/200 [learning_rate=0.020000] Val [Acc@1=88.990, Acc@5=99.680 | Loss= 0.37723
Epoch 85/200 [learning_rate=0.020000] Val [Acc@1=89.410, Acc@5=99.560 | Loss= 0.36689
Epoch 86/200 [learning_rate=0.020000] Val [Acc@1=87.960, Acc@5=99.590 | Loss= 0.41767
Epoch 87/200 [learning_rate=0.020000] Val [Acc@1=89.740, Acc@5=99.600 | Loss= 0.36167
Epoch 88/200 [learning_rate=0.020000] Val [Acc@1=89.560, Acc@5=99.720 | Loss= 0.36704
Epoch 89/200 [learning_rate=0.020000] Val [Acc@1=89.520, Acc@5=99.670 | Loss= 0.36086
Epoch 90/200 [learning_rate=0.020000] Val [Acc@1=90.290, Acc@5=99.700 | Loss= 0.32980
Epoch 91/200 [learning_rate=0.020000] Val [Acc@1=90.040, Acc@5=99.690 | Loss= 0.34624
Epoch 92/200 [learning_rate=0.020000] Val [Acc@1=90.370, Acc@5=99.660 | Loss= 0.33344
Epoch 93/200 [learning_rate=0.020000] Val [Acc@1=88.050, Acc@5=99.380 | Loss= 0.44823
Epoch 94/200 [learning_rate=0.020000] Val [Acc@1=87.880, Acc@5=99.610 | Loss= 0.46398
Epoch 95/200 [learning_rate=0.020000] Val [Acc@1=90.020, Acc@5=99.690 | Loss= 0.33930
Epoch 96/200 [learning_rate=0.020000] Val [Acc@1=90.370, Acc@5=99.700 | Loss= 0.33917
Epoch 97/200 [learning_rate=0.020000] Val [Acc@1=88.430, Acc@5=99.630 | Loss= 0.39310
Epoch 98/200 [learning_rate=0.020000] Val [Acc@1=88.890, Acc@5=99.660 | Loss= 0.38378
Epoch 99/200 [learning_rate=0.020000] Val [Acc@1=90.630, Acc@5=99.670 | Loss= 0.32695
Epoch 100/200 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.640 | Loss= 0.35838
Epoch 101/200 [learning_rate=0.020000] Val [Acc@1=88.220, Acc@5=99.460 | Loss= 0.42432
Epoch 102/200 [learning_rate=0.020000] Val [Acc@1=88.520, Acc@5=99.620 | Loss= 0.41095
Epoch 103/200 [learning_rate=0.020000] Val [Acc@1=89.640, Acc@5=99.540 | Loss= 0.36022
Epoch 104/200 [learning_rate=0.020000] Val [Acc@1=89.370, Acc@5=99.570 | Loss= 0.37061
Epoch 105/200 [learning_rate=0.020000] Val [Acc@1=88.060, Acc@5=99.510 | Loss= 0.44692
Epoch 106/200 [learning_rate=0.020000] Val [Acc@1=89.550, Acc@5=99.630 | Loss= 0.35933
Epoch 107/200 [learning_rate=0.020000] Val [Acc@1=89.830, Acc@5=99.620 | Loss= 0.35758
Epoch 108/200 [learning_rate=0.020000] Val [Acc@1=88.130, Acc@5=99.620 | Loss= 0.44081
Epoch 109/200 [learning_rate=0.020000] Val [Acc@1=89.930, Acc@5=99.530 | Loss= 0.34825
Epoch 110/200 [learning_rate=0.020000] Val [Acc@1=88.900, Acc@5=99.690 | Loss= 0.40104
Epoch 111/200 [learning_rate=0.020000] Val [Acc@1=89.650, Acc@5=99.550 | Loss= 0.37955
Epoch 112/200 [learning_rate=0.020000] Val [Acc@1=90.000, Acc@5=99.630 | Loss= 0.34345
Epoch 113/200 [learning_rate=0.020000] Val [Acc@1=89.690, Acc@5=99.660 | Loss= 0.36398
Epoch 114/200 [learning_rate=0.020000] Val [Acc@1=87.180, Acc@5=99.530 | Loss= 0.47109
Epoch 115/200 [learning_rate=0.020000] Val [Acc@1=90.200, Acc@5=99.560 | Loss= 0.33046
Epoch 116/200 [learning_rate=0.020000] Val [Acc@1=89.180, Acc@5=99.580 | Loss= 0.37390
Epoch 117/200 [learning_rate=0.020000] Val [Acc@1=86.520, Acc@5=99.470 | Loss= 0.49641
Epoch 118/200 [learning_rate=0.020000] Val [Acc@1=90.070, Acc@5=99.650 | Loss= 0.35263
Epoch 119/200 [learning_rate=0.020000] Val [Acc@1=90.260, Acc@5=99.650 | Loss= 0.34141
Epoch 120/200 [learning_rate=0.004000] Val [Acc@1=92.390, Acc@5=99.780 | Loss= 0.25949

==>>[2022-08-27 16:11:27] [Epoch=120/200] [Need: 00:56:59] [learning_rate=0.0040] [Best : Acc@1=92.39, Error=7.61]
Epoch 121/200 [learning_rate=0.004000] Val [Acc@1=92.620, Acc@5=99.770 | Loss= 0.25795

==>>[2022-08-27 16:12:10] [Epoch=121/200] [Need: 00:56:17] [learning_rate=0.0040] [Best : Acc@1=92.62, Error=7.38]
Epoch 122/200 [learning_rate=0.004000] Val [Acc@1=92.430, Acc@5=99.820 | Loss= 0.25555
Epoch 123/200 [learning_rate=0.004000] Val [Acc@1=92.740, Acc@5=99.780 | Loss= 0.25859

==>>[2022-08-27 16:13:35] [Epoch=123/200] [Need: 00:54:51] [learning_rate=0.0040] [Best : Acc@1=92.74, Error=7.26]
Epoch 124/200 [learning_rate=0.004000] Val [Acc@1=92.760, Acc@5=99.780 | Loss= 0.25896

==>>[2022-08-27 16:14:18] [Epoch=124/200] [Need: 00:54:08] [learning_rate=0.0040] [Best : Acc@1=92.76, Error=7.24]
Epoch 125/200 [learning_rate=0.004000] Val [Acc@1=92.860, Acc@5=99.770 | Loss= 0.26498

==>>[2022-08-27 16:15:01] [Epoch=125/200] [Need: 00:53:26] [learning_rate=0.0040] [Best : Acc@1=92.86, Error=7.14]
Epoch 126/200 [learning_rate=0.004000] Val [Acc@1=92.740, Acc@5=99.770 | Loss= 0.27434
Epoch 127/200 [learning_rate=0.004000] Val [Acc@1=92.950, Acc@5=99.790 | Loss= 0.27296

==>>[2022-08-27 16:16:27] [Epoch=127/200] [Need: 00:52:01] [learning_rate=0.0040] [Best : Acc@1=92.95, Error=7.05]
Epoch 128/200 [learning_rate=0.004000] Val [Acc@1=92.860, Acc@5=99.750 | Loss= 0.26991
Epoch 129/200 [learning_rate=0.004000] Val [Acc@1=93.010, Acc@5=99.790 | Loss= 0.27166

==>>[2022-08-27 16:17:53] [Epoch=129/200] [Need: 00:50:36] [learning_rate=0.0040] [Best : Acc@1=93.01, Error=6.99]
Epoch 130/200 [learning_rate=0.004000] Val [Acc@1=92.780, Acc@5=99.740 | Loss= 0.27903
Epoch 131/200 [learning_rate=0.004000] Val [Acc@1=92.850, Acc@5=99.830 | Loss= 0.28296
Epoch 132/200 [learning_rate=0.004000] Val [Acc@1=92.790, Acc@5=99.800 | Loss= 0.27868
Epoch 133/200 [learning_rate=0.004000] Val [Acc@1=92.690, Acc@5=99.810 | Loss= 0.27895
Epoch 134/200 [learning_rate=0.004000] Val [Acc@1=92.770, Acc@5=99.800 | Loss= 0.28205
Epoch 135/200 [learning_rate=0.004000] Val [Acc@1=92.870, Acc@5=99.760 | Loss= 0.28242
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=92.820, Acc@5=99.780 | Loss= 0.28186
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=92.900, Acc@5=99.810 | Loss= 0.28752
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=92.810, Acc@5=99.740 | Loss= 0.28581
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=92.840, Acc@5=99.790 | Loss= 0.29140
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=92.690, Acc@5=99.780 | Loss= 0.29282
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=92.820, Acc@5=99.750 | Loss= 0.29180
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=92.700, Acc@5=99.770 | Loss= 0.30140
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=92.940, Acc@5=99.750 | Loss= 0.29204
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=92.910, Acc@5=99.760 | Loss= 0.29367
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.760 | Loss= 0.29257
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.750 | Loss= 0.29188
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=92.930, Acc@5=99.770 | Loss= 0.29868
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=92.980, Acc@5=99.760 | Loss= 0.29262
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.760 | Loss= 0.30468
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=92.760, Acc@5=99.770 | Loss= 0.29594
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=92.760, Acc@5=99.730 | Loss= 0.29750
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=92.870, Acc@5=99.760 | Loss= 0.30052
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=92.690, Acc@5=99.720 | Loss= 0.30391
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.720 | Loss= 0.31337
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=92.760, Acc@5=99.760 | Loss= 0.30920
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=92.750, Acc@5=99.760 | Loss= 0.30598
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=92.670, Acc@5=99.750 | Loss= 0.31018
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=92.760, Acc@5=99.720 | Loss= 0.31314
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=92.730, Acc@5=99.720 | Loss= 0.30254
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=92.830, Acc@5=99.720 | Loss= 0.29909
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=92.810, Acc@5=99.710 | Loss= 0.29877
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=92.910, Acc@5=99.730 | Loss= 0.29878
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.730 | Loss= 0.29617
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=92.880, Acc@5=99.730 | Loss= 0.29564
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=92.910, Acc@5=99.740 | Loss= 0.29514
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=92.910, Acc@5=99.750 | Loss= 0.29691
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=92.980, Acc@5=99.720 | Loss= 0.29792
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=92.880, Acc@5=99.720 | Loss= 0.29575
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=92.930, Acc@5=99.680 | Loss= 0.29471
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=92.920, Acc@5=99.710 | Loss= 0.29565
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=92.960, Acc@5=99.730 | Loss= 0.29258
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=92.950, Acc@5=99.740 | Loss= 0.29547
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=92.940, Acc@5=99.730 | Loss= 0.29296
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.710 | Loss= 0.29579
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=92.930, Acc@5=99.710 | Loss= 0.29523
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=92.980, Acc@5=99.730 | Loss= 0.29393
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=92.940, Acc@5=99.680 | Loss= 0.29366
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=92.920, Acc@5=99.710 | Loss= 0.29427
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.740 | Loss= 0.29484
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=92.990, Acc@5=99.700 | Loss= 0.29626
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.050, Acc@5=99.730 | Loss= 0.29661

==>>[2022-08-27 16:54:59] [Epoch=181/200] [Need: 00:13:32] [learning_rate=0.0008] [Best : Acc@1=93.05, Error=6.95]
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.740 | Loss= 0.29788
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.740 | Loss= 0.29390
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=92.910, Acc@5=99.720 | Loss= 0.29427
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.720 | Loss= 0.29426
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=92.990, Acc@5=99.740 | Loss= 0.29703
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=93.080, Acc@5=99.720 | Loss= 0.29346

==>>[2022-08-27 16:59:15] [Epoch=187/200] [Need: 00:09:16] [learning_rate=0.0008] [Best : Acc@1=93.08, Error=6.92]
Epoch 188/200 [learning_rate=0.000800] Val [Acc@1=93.030, Acc@5=99.740 | Loss= 0.29279
Epoch 189/200 [learning_rate=0.000800] Val [Acc@1=92.920, Acc@5=99.690 | Loss= 0.29368
Epoch 190/200 [learning_rate=0.000160] Val [Acc@1=93.010, Acc@5=99.700 | Loss= 0.29478
Epoch 191/200 [learning_rate=0.000160] Val [Acc@1=92.940, Acc@5=99.740 | Loss= 0.29564
Epoch 192/200 [learning_rate=0.000160] Val [Acc@1=92.940, Acc@5=99.740 | Loss= 0.29759
Epoch 193/200 [learning_rate=0.000160] Val [Acc@1=93.000, Acc@5=99.720 | Loss= 0.29455
Epoch 194/200 [learning_rate=0.000160] Val [Acc@1=93.020, Acc@5=99.710 | Loss= 0.29552
Epoch 195/200 [learning_rate=0.000160] Val [Acc@1=93.050, Acc@5=99.750 | Loss= 0.29666
Epoch 196/200 [learning_rate=0.000160] Val [Acc@1=93.020, Acc@5=99.740 | Loss= 0.29473
Epoch 197/200 [learning_rate=0.000160] Val [Acc@1=92.960, Acc@5=99.740 | Loss= 0.29899
Epoch 198/200 [learning_rate=0.000160] Val [Acc@1=93.020, Acc@5=99.720 | Loss= 0.29520
Epoch 199/200 [learning_rate=0.000160] Val [Acc@1=93.030, Acc@5=99.750 | Loss= 0.29524
