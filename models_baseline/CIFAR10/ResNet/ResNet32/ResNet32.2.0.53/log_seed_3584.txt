save path : Baseline_CIFAR10/ResNet32/ResNet32.2.0.53
{'data_path': './data/cifar.python', 'pretrain_path': 'Baseline_CIFAR10/ResNet32/ResNet32.2.0.53/resnet32.checkpoint.pth.tar', 'pruned_path': './', 'dataset': 'cifar10', 'arch': 'resnet32', 'save_path': 'Baseline_CIFAR10/ResNet32/ResNet32.2.0.53', 'mode': 'train', 'batch_size': 256, 'verbose': False, 'total_epoches': 200, 'start_epoch': 136, 'prune_epoch': 30, 'recover_epoch': 1, 'lr': 0.1, 'momentum': 0.9, 'decay': 0.0005, 'schedule': [60, 120, 160, 190], 'gammas': [0.2, 0.2, 0.2, 0.2], 'seed': 1, 'no_cuda': False, 'ngpu': 1, 'workers': 8, 'rate_flop': 0.342, 'recover_flop': 0.0, 'manualSeed': 3584, 'cuda': True, 'use_cuda': True}
Random Seed: 3584
python version : 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]
torch  version : 1.12.0
cudnn  version : 8302
Pretrain path: Baseline_CIFAR10/ResNet32/ResNet32.2.0.53/resnet32.checkpoint.pth.tar
Pruned path: ./
=> creating model 'resnet32'
=> Model : CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> parameter : Namespace(data_path='./data/cifar.python', pretrain_path='Baseline_CIFAR10/ResNet32/ResNet32.2.0.53/resnet32.checkpoint.pth.tar', pruned_path='./', dataset='cifar10', arch='resnet32', save_path='Baseline_CIFAR10/ResNet32/ResNet32.2.0.53', mode='train', batch_size=256, verbose=False, total_epoches=200, start_epoch=136, prune_epoch=30, recover_epoch=1, lr=0.1, momentum=0.9, decay=0.0005, schedule=[60, 120, 160, 190], gammas=[0.2, 0.2, 0.2, 0.2], seed=1, no_cuda=False, ngpu=1, workers=8, rate_flop=0.342, recover_flop=0.0, manualSeed=3584, cuda=True, use_cuda=True)
=> train network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
Epoch 136/200 [learning_rate=0.004000] Val [Acc@1=92.840, Acc@5=99.720 | Loss= 0.30468

==>>[2022-08-27 14:35:52] [Epoch=136/200] [Need: 00:00:00] [learning_rate=0.0040] [Best : Acc@1=92.84, Error=7.16]
Epoch 137/200 [learning_rate=0.004000] Val [Acc@1=92.390, Acc@5=99.740 | Loss= 0.31471
Epoch 138/200 [learning_rate=0.004000] Val [Acc@1=92.530, Acc@5=99.740 | Loss= 0.31240
Epoch 139/200 [learning_rate=0.004000] Val [Acc@1=92.750, Acc@5=99.730 | Loss= 0.31745
Epoch 140/200 [learning_rate=0.004000] Val [Acc@1=92.600, Acc@5=99.740 | Loss= 0.31378
Epoch 141/200 [learning_rate=0.004000] Val [Acc@1=92.510, Acc@5=99.720 | Loss= 0.31534
Epoch 142/200 [learning_rate=0.004000] Val [Acc@1=92.580, Acc@5=99.700 | Loss= 0.31603
Epoch 143/200 [learning_rate=0.004000] Val [Acc@1=92.630, Acc@5=99.740 | Loss= 0.32082
Epoch 144/200 [learning_rate=0.004000] Val [Acc@1=92.680, Acc@5=99.710 | Loss= 0.31211
Epoch 145/200 [learning_rate=0.004000] Val [Acc@1=92.660, Acc@5=99.710 | Loss= 0.31798
Epoch 146/200 [learning_rate=0.004000] Val [Acc@1=92.710, Acc@5=99.700 | Loss= 0.32608
Epoch 147/200 [learning_rate=0.004000] Val [Acc@1=92.660, Acc@5=99.640 | Loss= 0.32243
Epoch 148/200 [learning_rate=0.004000] Val [Acc@1=92.790, Acc@5=99.690 | Loss= 0.32256
Epoch 149/200 [learning_rate=0.004000] Val [Acc@1=92.700, Acc@5=99.690 | Loss= 0.32371
Epoch 150/200 [learning_rate=0.004000] Val [Acc@1=92.550, Acc@5=99.720 | Loss= 0.32696
Epoch 151/200 [learning_rate=0.004000] Val [Acc@1=92.480, Acc@5=99.720 | Loss= 0.33388
Epoch 152/200 [learning_rate=0.004000] Val [Acc@1=92.530, Acc@5=99.680 | Loss= 0.33194
Epoch 153/200 [learning_rate=0.004000] Val [Acc@1=92.730, Acc@5=99.680 | Loss= 0.32191
Epoch 154/200 [learning_rate=0.004000] Val [Acc@1=92.440, Acc@5=99.720 | Loss= 0.32940
Epoch 155/200 [learning_rate=0.004000] Val [Acc@1=92.680, Acc@5=99.720 | Loss= 0.32591
Epoch 156/200 [learning_rate=0.004000] Val [Acc@1=92.740, Acc@5=99.740 | Loss= 0.32108
Epoch 157/200 [learning_rate=0.004000] Val [Acc@1=92.580, Acc@5=99.740 | Loss= 0.32105
Epoch 158/200 [learning_rate=0.004000] Val [Acc@1=92.540, Acc@5=99.720 | Loss= 0.32454
Epoch 159/200 [learning_rate=0.004000] Val [Acc@1=92.720, Acc@5=99.670 | Loss= 0.32534
Epoch 160/200 [learning_rate=0.000800] Val [Acc@1=92.970, Acc@5=99.690 | Loss= 0.31492

==>>[2022-08-27 14:52:58] [Epoch=160/200] [Need: 00:28:35] [learning_rate=0.0008] [Best : Acc@1=92.97, Error=7.03]
Epoch 161/200 [learning_rate=0.000800] Val [Acc@1=93.090, Acc@5=99.670 | Loss= 0.31397

==>>[2022-08-27 14:53:41] [Epoch=161/200] [Need: 00:27:52] [learning_rate=0.0008] [Best : Acc@1=93.09, Error=6.91]
Epoch 162/200 [learning_rate=0.000800] Val [Acc@1=92.860, Acc@5=99.710 | Loss= 0.31421
Epoch 163/200 [learning_rate=0.000800] Val [Acc@1=92.950, Acc@5=99.700 | Loss= 0.31416
Epoch 164/200 [learning_rate=0.000800] Val [Acc@1=92.970, Acc@5=99.710 | Loss= 0.31389
Epoch 165/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.710 | Loss= 0.31097
Epoch 166/200 [learning_rate=0.000800] Val [Acc@1=92.880, Acc@5=99.730 | Loss= 0.31465
Epoch 167/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.700 | Loss= 0.31710
Epoch 168/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.730 | Loss= 0.31400
Epoch 169/200 [learning_rate=0.000800] Val [Acc@1=92.890, Acc@5=99.710 | Loss= 0.31698
Epoch 170/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.710 | Loss= 0.31579
Epoch 171/200 [learning_rate=0.000800] Val [Acc@1=93.030, Acc@5=99.710 | Loss= 0.31122
Epoch 172/200 [learning_rate=0.000800] Val [Acc@1=93.030, Acc@5=99.710 | Loss= 0.31532
Epoch 173/200 [learning_rate=0.000800] Val [Acc@1=93.030, Acc@5=99.690 | Loss= 0.31229
Epoch 174/200 [learning_rate=0.000800] Val [Acc@1=93.010, Acc@5=99.720 | Loss= 0.31768
Epoch 175/200 [learning_rate=0.000800] Val [Acc@1=93.070, Acc@5=99.710 | Loss= 0.31364
Epoch 176/200 [learning_rate=0.000800] Val [Acc@1=93.020, Acc@5=99.690 | Loss= 0.31581
Epoch 177/200 [learning_rate=0.000800] Val [Acc@1=93.060, Acc@5=99.710 | Loss= 0.31638
Epoch 178/200 [learning_rate=0.000800] Val [Acc@1=93.090, Acc@5=99.730 | Loss= 0.31388
Epoch 179/200 [learning_rate=0.000800] Val [Acc@1=93.060, Acc@5=99.700 | Loss= 0.31722
Epoch 180/200 [learning_rate=0.000800] Val [Acc@1=92.920, Acc@5=99.680 | Loss= 0.31972
Epoch 181/200 [learning_rate=0.000800] Val [Acc@1=93.060, Acc@5=99.670 | Loss= 0.31711
Epoch 182/200 [learning_rate=0.000800] Val [Acc@1=93.040, Acc@5=99.710 | Loss= 0.31688
Epoch 183/200 [learning_rate=0.000800] Val [Acc@1=92.970, Acc@5=99.700 | Loss= 0.31623
Epoch 184/200 [learning_rate=0.000800] Val [Acc@1=92.870, Acc@5=99.730 | Loss= 0.31791
Epoch 185/200 [learning_rate=0.000800] Val [Acc@1=92.930, Acc@5=99.720 | Loss= 0.31695
Epoch 186/200 [learning_rate=0.000800] Val [Acc@1=93.000, Acc@5=99.700 | Loss= 0.31663
Epoch 187/200 [learning_rate=0.000800] Val [Acc@1=92.980, Acc@5=99.740 | Loss= 0.31556
Epoch 188/200 [learning_rate=0.000800] Val [Acc@1=93.020, Acc@5=99.710 | Loss= 0.31727
Epoch 189/200 [learning_rate=0.000800] Val [Acc@1=92.990, Acc@5=99.730 | Loss= 0.31471
Epoch 190/200 [learning_rate=0.000160] Val [Acc@1=93.060, Acc@5=99.720 | Loss= 0.31624
Epoch 191/200 [learning_rate=0.000160] Val [Acc@1=93.110, Acc@5=99.740 | Loss= 0.31573

==>>[2022-08-27 15:15:03] [Epoch=191/200] [Need: 00:06:25] [learning_rate=0.0002] [Best : Acc@1=93.11, Error=6.89]
Epoch 192/200 [learning_rate=0.000160] Val [Acc@1=93.040, Acc@5=99.710 | Loss= 0.31519
Epoch 193/200 [learning_rate=0.000160] Val [Acc@1=93.050, Acc@5=99.720 | Loss= 0.31525
Epoch 194/200 [learning_rate=0.000160] Val [Acc@1=93.010, Acc@5=99.680 | Loss= 0.31614
Epoch 195/200 [learning_rate=0.000160] Val [Acc@1=92.970, Acc@5=99.710 | Loss= 0.31616
Epoch 196/200 [learning_rate=0.000160] Val [Acc@1=93.060, Acc@5=99.720 | Loss= 0.31555
Epoch 197/200 [learning_rate=0.000160] Val [Acc@1=93.060, Acc@5=99.740 | Loss= 0.31754
Epoch 198/200 [learning_rate=0.000160] Val [Acc@1=93.050, Acc@5=99.720 | Loss= 0.31624
Epoch 199/200 [learning_rate=0.000160] Val [Acc@1=92.980, Acc@5=99.700 | Loss= 0.31574
